# 実験考察レポート: goemotions_annoyance_1_4o-mini_word

## 個別実験の詳細考察

以下は与えられた実験結果（LLM出力が記録されておらず、評価指標がともに0になっている状況）を前提にした詳細な考察です。特に「単語レベルの特徴分析」を重視し、具体例を交えて原因推定と改善案を示します。

1. 単語レベルでの特徴分析
- 手法／目的
  - 与えられた代表サンプル群（A: 発火群、B: 非発火群）を直接観察し、頻出語・語彙的特徴を抽出・比較して，Aに特徴的な単語・表現を特定する。

- A群（発火群）に特徴的な語／表現（代表例と文脈）
  - 感情表現・罵倒語・強い否定語: "fuck"（"Calm the fuck down."）、"damn"（"[NAME] damn it I'm so tired..."）、"hate"（"I hate that this is true."）  
    文脈: 否定的・攻撃的な感情を直接表出。相手への指摘や苛立ちの表明に使われる。
  - 命令形・助言的二人称: "stop"（"For the love of [NAME] stop making him relevant."）、"You should"（"You should be required to be 25+..."）  
    文脈: 他者の行動を咎める／規範を主張するトーン。対象を具体的に指し示す二人称・命令が多い。
  - 軽蔑・皮肉的表現: "karma whores"、"We live in a society"（皮肉的な批判）  
    文脈: 他者の動機や行動を低く評価し、社会批判や嘲笑を含む。
  - 疲労・煩わしさを示す語: "tired"、"bugging me"、"so tired"  
    文脈: 同様に不快・煩わしさの強調。
  - SNS／外見に関する語: "Instagram"、"followers"、"posting her ass"  
    文脈: インフルエンサー文化や外見誇示に対する軽蔑的コメント。
  - ネガティブな語彙全般: "sketchy"、"horrible"、"stupid"、"annoy" など
    文脈: 不信・否定・不満の表出。

- B群（非発火群）に特徴的な語／表現（代表例と文脈）
  - 支援的・共感的語彙: "therapy really is"、"I apologize"、"Cheers!"、"Hope for the best"  
    文脈: 励まし、助言、友好的な反応や中立的な情報提供。
  - 事実的・情報的表現: "moving to the UK"、"Did he say..."、"This terrifies me."（ただしネガティブだが個人の不安表現）  
    文脈: 個人的な状況の共有や質問・雑談。
  - ポジティブな感嘆・称賛: "Oh, awesome!"、"Wow, great news bro!"、"cutest vid"  
    文脈: 肯定的・軽い感動。
  - 中立〜やや否定だが攻撃性が低い表現: "Super weird question."、"not really commenting"  
    文脈: 興味／戸惑いの表明で、A群のような攻撃性・命令調は少ない。

- 単語の意味的・感情的ニュアンスのまとめ
  - A群: 怒り・苛立ち・軽蔑（高い負の情動強度）、相手指向（二人称・命令）、皮肉・嘲笑、SNS/外見批判。語彙は直接的で攻撃的。
  - B群: 中立〜支援的・情報共有・感嘆。負の情動はあるが内向き（自身の不安など）で、他者攻撃や命令は少ない。

2. 文脈・意味的ニュアンスの考察
- A群に共通する文脈的特徴
  - 他者批判・規範主張: 「～すべき」「～すればいいのに」といった規範的な述べ方や他者の行為を非難する表現が多い（例："You should be required..."）。
  - 直接的な負の感情表出: 怒りや嫌悪をストレートに表す語が目立つ（例："Calm the fuck down."）。
  - 社会文化的批判: インフルエンサー文化、社会の矛盾や保守的価値観への抗議が見られる（例："We live in a society" の皮肉的使い方）。
  - 会話的・口語的表現: スラングや感嘆表現、略語（"TBH"）など、インターネット掲示板特有の文体。

- B群との意味的・概念的差異
  - 対人攻撃性の程度: Aは対人攻撃（直接的な罵倒／命令）が高く、Bは対話的・支持的で攻撃性は低い。
  - 目的の違い: Aは不満表明・非難・注意喚起が目的化している場合が多い。Bは情報共有・共感・雑談（社会的交流）を目的とする発話が多い。
  - 抽象化の有無: A群にはしばしば抽象的・一般化的批判（"society"や"we"を用いた一般化）がある一方で、B群は個別事象の言及や具体的助言が中心。

- 抽象的概念や間接表現の有無
  - A群: 抽象的言及（社会、規範）と具体的侮蔑表現が混在。間接的表現よりも直接的攻撃が多いが、皮肉や暗示（"karma whores"）のような間接的軽蔑もみられる。
  - B群: 間接的／婉曲的表現（治療の勧め、励まし）が目立ち、抽象的な社会批判は少ない。

3. 正解ラベル（"annoyance related characteristics"）との比較
- 正解ラベルの妥当性
  - A群の語彙・文脈を踏まえると「annoyance / irritation / annoyance-related characteristics」は適切な要約である。怠惰、怒り、苛立ち、軽蔑といったネガティブ感情がA群の共通要素であり、正解ラベルは妥当。

- LLM生成対比因子（実際の出力が記録されていない／空欄）
  - 実験記録では「LLM生成対比因子」が空白で、BERTスコア・BLEUともに0.0000となっている。これは大きく以下のいずれかを示唆する。
    1. LLMが応答を返さなかった（APIエラー、タイムアウト、生成失敗）。
    2. LLMは生成したが、出力のログが失われた／保存に失敗した（データパイプラインのバグ）。
    3. LLMは生成したが、評価スクリプト側で空文字／無効文字列として扱われた（文字エンコーディングやトークン除去の問題）。
    4. 極端に異なる・無意味な出力（例えば非言語記号のみ）が生成され、評価器がスコア0を返した。
  - したがって、LLMの出力と正解ラベルの一致性を直接評価できない。だが期待される出力（上記正解と同様の一語句ラベルや短い名詞句）を出していればBERTScoreは0になりにくい。従って「出力欠落」か「後処理バグ」の可能性が高い。

- BERTScoreとBLEUが共に0である理由考察
  - BLEU=0: n-gramの一致が一切無い（あるいは生成が空）。BLEUは語彙一致に敏感。
  - BERTScore=0: 通常非常に低くても0未満にならない（0は極めて稀）。BERTScoreが0を返した場合、評価入力が空文字同士、あるいはエンコードに失敗してembeddingが計算できなかった可能性がある。
  - 総合推定: 「評価対象の生成テキストが空文字、または評価に投入される前に失われた」→ 評価器が0を返した。完全な意味的乖離でBERTScoreが真に0になるの現実的には稀であるため、実験プロセス上の問題（出力取得／保存／前処理）が強く疑われる。

4. 実験設定の影響
- Few-shot（1-shot）の影響
  - 1-shotは出力スタイルをある程度誘導できるが、タスクやデータのばらつきが大きいと過学習的に誤った一般化をすることがある。今回のケースで考えられる影響:
    - 例示が「説明文調」か「ラベル」かで出力スタイルが変わる。1ショットだとモデルがその1例のフォーマットに強く引きずられ、期待する「短い名詞句（aspect-like label）」ではなく長文説明を出す可能性がある。
    - ただし本件では出力自体が欠落している可能性が高く、Few-shotそのものが直接ゼロスコアの主原因とは断定できない。

- グループサイズ（group_size=100）の影響
  - 利点: group_sizeが大きいと集合差分の統計的信号が安定する（個々のノイズ発言に左右されにくい）。
  - 問題点: 代表例をどのようにLLMに与えるかが重要。100件をそのまま提示すると長すぎるため、サンプリングや要約が必要。サンプリング方法が雑だとノイズ（複数話題混在）により差分が希薄化する可能性。
  - 本サンプル（A/Bともに100件だが代表例として掲示された20件を見る限り）ではA群の特徴は明瞭で、group_size=100自体は十分な信号を提供していると判断できる。ただし実運用では: (i) サンプル選び／ランダム性、(ii) プロンプトでの要約方法、(iii) 上限トークン数が重要。

- データセット特性の影響
  - インターネット掲示板特有の口語・スラング・[NAME]プレースホルダの存在はノイズだが、A群における攻撃的語の濃度が高く、モデルは語彙的な手がかりで差分を学びやすいはず。
  - 実験記録の欠落がなければ、モデルが正解に近い短いラベル（例："annoyance/irritation"）を出す可能性は高い。

5. 改善の示唆（実装と評価の両面）
- 即時デバッグ項目（優先度高）
  1. 出力ログの完全な確認: APIから返ったraw response（tokens, text）を全て保存し、評価前に内容があるか確認する。空文字やエラーコードを検知したら自動リトライ。
  2. 評価パイプラインの入出力検証: gold label と生成ラベルの前後に不可視文字や全角／半角問題、改行のみの出力がないかをチェック。BERTScore計算時のembedding計算が失敗していないか確認する。
  3. モデルの応答設定確認: temperature=0（再現性向上）、max_tokensを十分に確保、stopシーケンスを適切に設定して生成中断を防止。

- プロンプト／Few-shot改良（実験設計）
  1. ショット数増加: 3～5ショットの例を用意し、フォーマットを厳密に統一（入力：短いサンプルセットの抜粋 → 出力：一語〜短い名詞句）。例を多数示すことで「一意に特定する語彙」に誘導しやすい。
  2. 出力制約の明示: 「出力は英語で1語または1つの名詞句（例: 'annoyance'）で答えよ」と明確に指示。不要な説明文を禁止する。
  3. 重要語の提示: 集合差分の算出（後述の統計手法）で得た上位n語（例: log-oddsで上位20語）をプロンプトに渡し、「以下の語を参考に1語でラベル化せよ」とすると安定する。
  4. ノイズ処理: [NAME]等のプレースホルダを正規化／削除して無意味な語を減らす。
  5. 多段パイプライン: (a) 統計的差分抽出（log-odds ratio / chi-square / TF-IDF）→ (b) LLMによる要約/命名。これによりLMMの出力がより堅牢に。

- 自動化可能な単語レベル支援（候補生成）
  - まず統計手法で「Aに有意に多い語」を拾う（例: log-odds ratio with informative Dirichlet prior）。A群上位語の例：{"fuck","damn","hate","stop","tired","Instagram","followers","stupid","bugging","calm"}。これを候補語としてLLMに提示し、短い名詞句へ圧縮させると良い。

- 評価指標の改良
  1. BLEUは短いラベル評価には不適（1-gramのみでも脆弱）。代わりにBLEURT／BARTScore／MoverScoreの採用を推奨。これらは語彙違いのパラフレーズにも敏感で人手評価との相関が高い。
  2. 出力が短い名詞句の場合はembedding cosine（Sentence-BERT等）による閾値判定＋クラスタ一致率を用いると安定する。
  3. 人手評価（複数アノテータ）を設け、LLMの出力が「人間の解釈に合致するか」を直接確認する。

- 実験拡張案
  1. 複数温度・複数ショットで複数候補生成 → 多数決あるいはメタLLMで選択（ラベルの安定化）。
  2. ラベルの形式を限定し、同一フォーマット（名詞句）で学習させるためにプロンプト設計をテンプレート化。
  3. 小規模なヒューマンラベルデータを用いてBLEURT等の学習済み評価器で最終チューニングする。

まとめ（要点）
- A群は明確に「苛立ち・怒り・軽蔑（annoyance-related）」に結びつく語彙と文脈を持っている。正解ラベルは妥当である。
- 実験でBERT/BLEUが0になっているのはモデルの意味的失敗というより、出力欠落／評価パイプラインの不備（ログ保存、前処理、エンコーディング、空文字処理）である可能性が高い。まずはロギングと評価パイプラインのデバッグを行うべき。
- 改善としては、(1) 出力の強いフォーマット制約、(2) ショット数増加と例の形式統一、(3) 統計的差分語の事前抽出→LLMに提示、(4) 評価指標の見直し（BLEURT/BARTScore等）を優先的に実施することを勧める。

必要であれば、A/B群の全100件に対して自動で上位の差分語（log-odds）を計算し、その出力候補リストを生成して提示します。これを使ったプロンプト例や再実験の手順も作成可能です。どちらを先に進めますか？

## メイン実験全体の考察

以下は「メイン実験（group_size=100 統一）／各データセットに対する対比因子ラベリング実験群（gpt-4o-mini, few-shot=1 を基本）」に関する、個別実験考察ログ（実験1〜37）を総合して導いたカテゴリ全体の分析・洞察です。実務的な原因推定と優先的改善案を含めてまとめます。

要約（結論）
- 最も顕著な事象：多数の個別実験で「LLMの出力が得られていない／評価に回せる形式で取得できていない」ため BERTScore・BLEU が 0.0000 になっている。すなわち「モデルが出力しなかった／出力が消失した／評価パイプラインが壊れている」ことが主要因で、モデル能力の評価は事実上行えていないケースが多い。
- データ側では A 群が典型的に「一貫したドメイン語彙（例：food/service/battery/screen/emotions別語彙）」を示すことが多く、理論上は短い名詞句ラベルで要約可能であるにもかかわらず、実験設定（few-shot=1・入出力前処理・評価方式）の不備で失敗が多発している。

以下、指定観点ごとに整理します。

1. カテゴリ全体の傾向（個別実験から抽出された共通パターン）
共通パターン
- 出力欠落問題が圧倒的に多い
  - 多数の実験で「LLM生成対比因子」が空欄、BERT/BLEU が 0。ログ上は出力欠落（APIエラー／レスポンス保存漏れ／評価入力が空）または評価前処理での消失が最有力。
- A群はドメインに固有の語彙がまとまる
  - semeval/restaurants/laptop/amazon/goemotions 各セットで、A群は明瞭なトピック語（例：food→fresh/menu/taste、service→friendly/attentive、battery→charge/last、screen→resolution/froze、emotion→sad/joy/pride等）に収束している。つまり「集合差分」は存在するケースが多い。
- B群はより分散／雑多／ノイズが多い
  - B群は混在テーマ・技術語・雑談などが多く、対比が単純ではない場合もある（Aの信号を薄める）。
- $T$ や [NAME] のようなプレースホルダ・特殊トークンが入力に多く存在
  - マスク・プレースホルダが意味手がかりを隠すためモデルが迷う、あるいはプレースホルダが雑音となるケースが複数確認された。
- 生成は「短いラベル（名詞句）」の想定なのに、プロンプトやショットが説明文や長文を誘導してしまうケースが目立つ

データセット／アスペクト差
- 感情系（goemotions）
  - A群は明確にその感情カテゴリの語彙（例えば「admiration→amazing/wonderful」「anger→fuck/idiot」）を含む場合が多く、理想的には非常にラベル化しやすい。
- SemEval / Amazon / Laptop 等（属性系）
  - A群は特定アスペクト（food quality, service, battery life, price, delivery 等）を語る語彙で凝縮しており、本来は差分抽出＋命名が実務的に可能。
- 実際の失敗の分布はアスペクト依存より「実験運用（プロンプト・評価・ログ）」に強く依存している

2. パフォーマンスの特徴
スコア分布・傾向
- 並列して報告された多くの実験で BERT/BLEU が 0.0000（完全ゼロ）：実際には「性能低下」より「実験的欠損（出力や評価入力の欠如）」を示す
- 正常に出力が得られていたサブケースがほとんど報告されておらず、スコアの有意な正負分布を評価するデータが不足

高スコアになり得る条件（ログや人手観察からの逆推定）
- A群が語彙的に凝縮（同一アスペクト語彙が高頻度）→ラベリング容易
- プロンプトで出力形式が明示され、few-shot で短い名詞句例を与えている
- 前処理で代表語（top-n tokens by log-odds/TF-IDF）を抽出して入力に含めている（つまり「二段階ワークフロー」）
- 出力検査・フォーマット検証（非空チェック）を行っている

低スコア（今回の大多数）に共通する特徴
- 出力が空、あるいは出力フォーマットが評価用フォーマットと異なる（例：説明文 vs 短いラベル）
- BLEU を単独で評価に使っている（短いラベル評価に BLEU は不適）
- 評価パイプラインで文字コード・トークナイザ・改行などが整合していない

3. 設定パラメータの影響
Few-shot（ショット数）
- 1-shot は不安定要因
  - 多数の事例で「1-shot が出力形式の安定を欠く」→モデルが説明的応答や空応答を返すリスクが高いと示唆
  - 推奨：3～5ショットで例の質（短い名詞句によるラベル例を複数）を確保することで出力安定化

group_size（100）
- group_size=100 は概念抽出には十分
  - ただし「群の純度（A にアスペクトが集中しているか）」が重要。A 内にノイズ（異トピック）が多いと対比が弱まる
  - グループサイズの感度解析は有益（50/100/150/200/300）→既に計画されている通りだが、代表抽出法（クラスタ代表 or top tokens）併用を推奨

モデル（gpt-4o-mini 等）
- gpt-4o-mini は汎用性は高いが、
  - 出力の「短いラベル」を安定的に生成させるにはプロンプトとショット設計が重要
  - 一部の失敗（空出力）はモデルより運用（API/ログ/評価）に起因している可能性が高い
- モデル選択の影響は存在するが、まずはプロンプト・前処理・評価周りを安定化させるのが優先

その他実行パラメータ
- temperature（多様性）→低め(0–0.2)で決定的出力を狙うべき（短いラベル生成は多様性不要）
- max_tokens／stop_sequences：短ラベルを切られないように設定し、長文を強制しない

4. 洞察と示唆（研究・実務への具体的提案）
主要知見（実験群全体から）
1. 実験失敗の主要原因は「運用／パイプライン（出力取得・評価）の欠陥」であり、モデルそのものの性能評価が十分に回収されていないケースが多い。
2. A群は多くの場合、明確な集合的語彙手がかり（discriminative tokens）が存在するため、原理的には自動ラベリングは実現可能である。
3. 単語ベースのみでの判定は限界がある：句・共起・文脈（"at least", "I never knew" 等）を踏まえた処理が必要。
4. BLEU は短いラベル評価に不向き。意味的評価（BERTScore, BLEURT, BARTScore, embedding cosine）が必要。

優先的改善（短期：実験フローを直す）
- A. ログ＆評価健全性の確保（絶対必須）
  1. LLM の raw response（text）を全て保存し、出力の有無を自動チェックして「空なら再試行/アラート」。
  2. 評価前に参照と生成の正規化（UTF-8, strip, lower, NFKC）を行い空入力を弾く。
- B. 出力形式の強制化（プロンプト）
  1. 出力は「1–3語の英語名詞句（short label）」のみと指示し、few-shot で 3–5 例を与える（例は必ず正しい形式）。
  2. 生成候補を n-best で取り、後段で埋め込み類似度で再ランキングする。
- C. 前処理→LLM の二段構成
  1. 統計的差分（log-odds / TF-IDF / chi-square）で上位 discriminative tokens を抽出（A vs B）。
  2. それらトークン＋代表サンプルを LLM に与え、「この語群を要約して短いラベルを1つ出せ」とする。
  - これによりノイズ低減と安定性向上が期待できる。
- D. 評価改善
  1. BLEURT / BARTScore / SBERT cosine を導入（BLEU廃止または補助）。
  2. 多参照（paraphrase set）と人手評価（少量）を用意し、自動指標のキャリブレーションを行う。

中期的研究提案
- 1. アブレーション実験：few-shot数（0/1/3/5）、group_size（50/100/150/200）、前処理有無の 3 因子実験で安定性曲線を描く。
- 2. ハイブリッド法の評価：統計的差分→LLM命名 vs LLM単体（対比）で精度・再現性を比較。
- 3. 出力の根拠提示（explainable output）：LLM にラベルと「根拠トークン3つ」の同時出力を求め、信頼性指標を作る（自動スコア+人手検査で検証）。

運用上の注意点（倫理・安全）
- データに攻撃的／差別語が含まれる場合、モデル応答のフィルタや出力抑制が働く可能性がある（その結果出力が空になるケースあり）。フィルタの有無／ログは必ずチェックすること。
- ラベル化で人を動かす用途（自動モデレーション等）では検出誤差・誤学習に注意。人手介在を残すハイブリッド運用を推奨。

最後に：今すぐ着手すべき実務タスクリスト
1. raw model response の保存／空応答検出を実装（必須）。  
2. 評価前処理の自動チェック（参照・生成が非空であること）を実装。  
3. few-shot を 3-shot に増やしたテンプレートを作り、代表的小実験で安定化（短いラベルを返すかを検証）。  
4. 差分トークン抽出（log-odds）スクリプトを作成して A/B の上位語を確認 → LLM に提示するワークフローを試す。  
5. BLEURT / BARTScore / SBERT-cosine を導入し、少数人手ラベルで指標調整を行う。

もし望まれるなら、次のいずれかをすぐ用意します：
- A. 差分トークン抽出（A/B 各100件）を行う Python スクリプトと上位語リスト（出力形式 csv）  
- B. 改良版 few-shot プロンプト（英語／日本語）テンプレート（3-shot）＋出力検証ロジック（空応答検出）  
- C. 評価パイプライン修正案（BLEURT 並列導入手順・SBERT 電卓）

どれを優先して作成しましょうか？

