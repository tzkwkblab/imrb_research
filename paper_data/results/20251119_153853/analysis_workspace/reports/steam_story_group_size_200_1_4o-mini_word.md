# 実験考察レポート: steam_story_group_size_200_1_4o-mini_word

## 個別実験の詳細考察

以下では、与えられた入力サンプル群（グループA＝発火群、グループB＝非発火群；各200件の代表文）と実験出力（LLM生成対比因子が欠落／スコアが0となっている事象）を踏まえ、要求された5観点（単語レベル、文脈的特徴、正解ラベル比較、実験設定の影響、改善示唆）に沿って詳細に考察します。特に単語レベルの具体例を重視し、原因仮説と改善案を具体的に示します。

要点の先出し（要約）
- 単語レベルでは、グループAは「技術的・操作的問題（stuttering, frame rate, keybindings, bug, control scheme, AI, combat, animations 等）」や「プレイ/進行に関する記述（demo, full release, side stories, 100% 等）」が頻出。グループBは「推薦・高評価表現（recommended, 9/10, 10/10, Heavily recommended, Good story, Good mechanics 等）」や「ポジティブな特徴（Great graphics, developer support, friendly interface 等）」が目立つ。  
- 意味的には、Aは「プレイ体験の不満・細部への記述（低レベル／具体的問題）」が多く、Bは「総評・評価／ストーリーやキャラクターに関する言及」を含む高レベルの賞賛が多い。  
- 正解ラベル（story related characteristics）との整合性は低い。提示サンプルからは「story に関する記述」は両群に散在するが、Aはストーリーよりも技術・操作の記述比率が高い。  
- 実験結果（生成対比因子の欠落・BERT/BLEU=0）は、（1）モデル出力が空または評価スクリプトの整形エラー、（2）トークナイザ／エンコーディングや文字コード問題、（3）プロンプトが不適切で期待する短ラベルを誘導できなかった、のいずれか（または複合）である可能性が高い。  
- 改善策としては：事前の単語頻度／対比的キーワード抽出（log-odds、χ²、tf-idf）→ LLMへ要約させるパイプライン、プロンプト（出力フォーマット）厳密化、few-shot例の質的改善、評価指標の見直し（BLEURT/BARTScore等）と人手評価併用、group_sizeの感度試験の整備、等を推奨する。

以降、各観点について詳述します。

1) 単語レベルでの特徴分析
方法論（推奨の解析手順）
- 単語頻度差（AとBでの出現頻度）→ 正規化頻度（per-1000 tokens）比較  
- 対比的指標：log-odds ratio（加算平滑あり）、χ²検定、情報利得（information gain）で差異語を抽出  
- n-gram（特にバイグラム）も抽出（例：「frame rate」「key bindings」「full release」）  
- ストップワード除去・ステミング/レンマ化・小文字化を実施

代表的な差分ワード（入力代表サンプルに基づく主観的抽出）
- グループAに特徴的な語・表現（例）:
  - 技術・動作系：stuttering, frame rate, 30 frame rate cap, keybindings, control scheme, crash(ed), bug, devs, lost items, stuttering, AFK, crashed
  - 操作・戦闘：combat, dodging, AI, animations, bad AI, hard to control, button mashing
  - 開発/サポート：no mod support, online only, developer(s)
  - ゲーム進行・構成：demo, full release, side stories, DLC, added side stories, tacked on level, copy paste worlds
  - 感情的表現（否定）：sunk by poor customer service, couldnt recommend, sadly i cannot, atrocious
  - 賞賛・ジャンル表現（混在）：beautiful, great potential, greatest building game (Aには肯定表現も混在)
- グループBに特徴的な語・表現（例）:
  - 評価語・推薦：recommended, Heavily recommended, 9/10, 10/10, Good campaign, Good story, Good mechanics, Good price
  - ポジティブ特徴：Great graphics, Great developer support, Great vehicle physics, Friendly interface, Fun coop
  - ストーリー/キャラクター関連：relatable characters, story (explicitに言及する例が多い)
  - 不満語はあるが頻度は低く、Bでは総じて高評価の述語が目立つ

具体的な使用文脈と意味的ニュアンス（例示）
- 「stuttering」「30 frame rate cap」「frame rate」→ 実行性能（パフォーマンス）に関する不満。多くは否定的語脈（"experiencing stuttering and the 30 frame rate cap"）で、技術的な問題がプレイの満足度を下げていることを示す。感情的には苛立ち・失望を伴う（"sadly i cannot [recommend]"）。
- 「keybindings」「control scheme」「button mashing」→ 操作性・UI/UX の不備を指摘する語。プレイヤーの操作フローが阻害されることへのフラストレーション表現。
- 「bug」「lost items」「devs」「stop button mashing」→ バグ報告と開発者とのやり取りに関する具体的事例を示す。ユーザーと開発者対応の信頼性・サポート品質に関する言及。
- 「tacked on level」「copy paste worlds」「corny jokes」→ コンテンツ品質の批判（手抜き感、作り込み不足）。これは「作品の質的欠陥」を示す否定的評価。
- 「Good story」「relatable characters」「story driven」→ 物語性・キャラクターへの肯定的な言及。評価文脈で好意的に使われるため、"story related characteristics" ラベルと直接的に結び付く。

感情的側面のまとめ
- Aは否定的感情（怒り・苛立ち・失望）と具体的な技術問題の列挙が多い。肯定的表現もあるが、構成としては「体験の苦情＋技術改善要求」が目立つ。  
- Bは全体的に高評価・推薦に近い表現が多く、ポジティブな感情（満足・推奨）が強い。ストーリーやキャラクターに触れる文が多く、"物語性"に関する言及比率が相対的に高い。

2) 文脈・意味的ニュアンスの考察
グループAの文脈的特徴（集合としての傾向）
- 「詳細な不満の列挙」：Aのサンプルは個別の問題（バグ、操作性、フレーム落ち、AIのへたさ、コントロールの混乱）を具体的に述べる傾向が強い。  
- 「プレイ体験の実況的叙述」：問題が発生した状況（特定の戦闘シーンや特定挙動）や、発生頻度・制約（30fps cap, lost items）を語る具体性が高い。  
- 「混在する評価」：Aには強い否定的語だけでなく「beautiful」「great potential」といった楽観的／中立的な語も混じり、感情のばらつきが大きい（ノイズが多い集合）。

グループBの文脈的特徴
- 「総評・推薦重視」：Bはレビューとしての総括（ポジティブな要約）や、評価スコア（9/10等）、遊ぶべき／推奨するか否かの明示が多い。  
- 「ストーリー・キャラクター言及が明示的」：Bサンプルには"Good story"や"relatable characters"など、物語性に関する直接的な肯定表現が見られるため、ラベル "story related characteristics" に関連しやすい。

AとBの概念差（意味的に示すもの）
- 粗い分類では、A = 「操作性・技術的問題・プレイ体験の詳細（低レベル）」、B = 「総合評価・推薦・物語や体験の高レベル特徴」 という差が認められる。  
- つまり、Aは「なぜその評価に至ったか（原因・障害）」を述べる語が多く、Bは「評価そのもの（好き/おすすめ/スコア）」や「作品の本質（story, characters）」を述べる語が多い。  
- 抽象概念や間接表現については、Bの方が抽象的な価値判断（"Good story", "Great graphics"）を多用し、Aは抽象よりも具体的記述（"stuttering", "lost items"）に偏る。

抽象的表現の有無
- A：間接的／抽象的（例："great potential"）も存在するが少数。  
- B：抽象的評価が比較的多く（"Good story" 等）、評価概念に直結する単語が頻出。結果として、"story related characteristics" のラベルはB寄りの特徴を反映しやすい。

3) 正解ラベルとの比較
正解ラベル：「story related characteristics」

LLM生成対比因子（実データ上は未提示／欠落）
- 実験ログでは「LLM生成対比因子」が提示されていない（空、あるいは評価対象外）。そのため直接的な語的一致評価は行えない状況。BERTスコアとBLEUがともに0.0000となっている点もこれを裏付ける（以下、その原因考察）。

正解ラベルとの一致度（仮説的評価）
- 与えられた代表サンプルを見ると、"story related characteristics" が当てはまりやすいのはグループBの一部サンプル（Good story, relatable characters, story-driven 等）である。  
- グループAはストーリーに関する言及が散在するものの、集合的特徴としてはストーリーより「操作性／技術問題」が支配的である。従って、もし対比因子ラベルが「story related characteristics」になるならば、モデルはAをBと対比して「Aはstoryに関する特徴が少ない／Bはstoryに関する肯定的表現が多い」と要約すべきである（対比は「Bに比べてAは story に関する言及が少ない」等）。

一致している点と不一致点（想定）
- 一致している可能性：Bがstory関連表現を多く含むため、正解ラベルがB起点での差分（＝story関連）を示すならば妥当。  
- 不一致の可能性：実際のAサンプルにもstoryやside stories等の言及があるため、「完全にstoryに関係しない」とは言えない。加えてAの主要な差異は「技術的問題」に関する明確な語彙的シグナルであり、対比ラベルとして期待されるのはむしろ "technical/performance issues" や "control/UX problems" といった表現のはずである。従って、正解ラベル（story related...）がどちらの群に由来する差異を示すかに依存して、適合度が変わる。

BERTスコアとBLEUがともに0.0000である原因考察
- 可能性A：モデル出力が空文字列、あるいは評価対象のファイルが欠落。いずれも自明にスコア0になる（BERTScore計算が出力なしを0扱いする実装の可能性）。  
- 可能性B：出力に特殊文字列（制御文字、HTMLやURLのみ、または非Unicode文字）しか含まれ、評価スクリプトがそれを無効出力として扱った。  
- 可能性C：評価スクリプト側の不整合（参照ラベルのパス間違い、トークナイザ不一致、言語コードの誤指定）。通常、BERTScoreは0〜1の小数を返すので完全な0は稀。  
- 可能性D：LLMが出力した文がまったく参照ラベルと意味的接点がなく、実装上小数点以下を四捨五入して0.0000表示されている（ただしこれも異常に低い）。  
- 総合評価：最も可能性が高いのは「モデル出力が期待フォーマット（単語ラベル）で出力されていない／出力欠落」あるいは「評価スクリプトの入力取り込みに失敗している」ことであり、まずはログ（raw LLM output）と評価スクリプトの入出力を確認する必要がある。

4) 実験設定の影響
Few-shot設定（1-shot）の影響
- 1-shotは最低限の出力スタイルの誘導に留まるため、「抽象的概念の命名（対比因子を一語／短語で表す）」のような高抽象度タスクでは例示が不十分になりやすい。  
- 典型的な問題：1-shotだとモデルは例示から「出力の語調」は学ぶが、どの程度の抽象度（具体：技術問題 vs 抽象：story-related）で要約すべきかが不安定になる。結果として出力が冗長になったり、空白・曖昧な要約を返す可能性がある。  
- 対策含意：少なくとも3-shot以上で、かつ出力の望ましい粒度（1〜3語のラベル／短いフレーズ）を明示した例を与える方が安定する。

グループサイズ（group_size）の影響
- group_size = 200（AとBとも）という比較的大きな集合は「ノイズを含む多様な文」を内包するため、差分シグナルが希薄化する。  
- 具体的に：あるゲームに関するレビュー群はジャンル・作者・時期など多様な変数を内包しており、単純にランダムに200件を集めると、対比すべき「一貫した概念的差」が希薄になる。  
- 小さすぎると代表性が低く、偏りが出る。大きすぎると局所的特徴（例："stuttering"）が平均化されて目立たなくなる。  
- 実験目的（最適なgroup_size探索）に沿うなら、50/100/150/200/300 の各設定で尤も差異が検出されやすい（評価性能が高い）サイズを検証する必要がある。現状の200は中間値で、A/B双方にノイズが多く「ストーリー性」を明確に抽出するにはやや不利な可能性がある。

データセット特性（Steamレビュー由来）の影響
- Steamレビューは以下の特徴を持つ：短文・断片的表現、感情の極性が強い、UI/UX・技術的問題の詳細な報告が混在する、ゲームジャンルにより言及ポイントが異なる（RPGはstory言及多、競技ゲームはmechanics言及多）。  
- したがって「対比因子」抽出タスクでは、ジャンルや時間的要因の混入がバイアスになる。これをコントロールしないと、LLMはジャンル差や技術世代差を対比因子と誤認する恐れがある。

5) 改善の示唆（具体的手順）
A. デバッグ・即時確認
- ログ確認：まずRaw LLM出力（model response）を全て保存・確認。空出力や特殊トークン（e.g., ""など）が無いか確認。  
- 評価パイプライン確認：参照ラベルのパス、文字コード（UTF-8）、トークナイザ設定（case sensitivity）、空白・改行処理を点検。  
- 単一ケース検証：代表サンプル（10件ずつ）で手作業でプロンプト→モデル出力→評価を追跡し、どの段階で欠落するか特定する。

B. 単語レベル・統計的前処理を導入（LLM入力改良）
- まずA/Bの差分上位トークンを自動抽出（log-odds、χ²、tf-idf）。上位20語＋上位10バイグラムを抽出してLLMに提示する。例：「Aに頻出：stuttering (23件), keybindings (15件), bug (40件) ...」のように数値付きで与える。  
- 理由：200件の生テキストを丸投げするより、差分を圧縮した「証拠セット」を与えることでLLMが正確に対比ラベルを命名しやすくなる。

C. プロンプト設計の見直し
- 出力フォーマットを厳格化（必須）：「返答は最大4語の英語ラベル一件のみで出力せよ。ラベルの後に'|'区切りで1行で3つの代表フレーズ（引用符付き）を出力せよ。」など。これにより評価との整合性を保つ。  
- Few-shotの改善：3〜5ショットの具体例を用意し、各例で（A群,B群,正解ラベル,支持フレーズ）を示す。抽象度の例（technical issue / story related / sound/music / graphics）をカバーしておく。  
- 追加指示：曖昧な場合は"ambiguous"として出力し、その理由（短文）を返すよう促す（ただし評価用の単語ラベルとは別）。

D. 出力の集約とロバスト化
- アンサンブル：同一A/Bに対して複数プロンプト（言い回しかえ、温度変更）で複数ラベルを取り、最頻ラベルまたはスコア付きで最終決定。  
- 根拠提示：LLMに「上位3つの決定要因（語またはフレーズ）と出現件数」を返させ、ヒューマン査読の助けにする。

E. 評価指標の改善
- 自動評価：BLEUは語彙一致に偏るため不適。BERTScoreは有用だがBLEURT/BARTScore/MoverScoreを追加すべき。特にBLEURTは人手との相関が高いので推奨。  
- 人手評価：特に概念ラベル評価は人が最終確定するべきで、少なくとも100例規模でのクラウド評価（複数アノテータ）を併用する。  
- スコアの扱い：小数点以下の丸めやゼロ表示に注意。評価実行ログを保存し、失敗例のヒートマップを作る。

F. 実験設計改善（group_sizeの最適化）
- 局所最適探査：50/100/150/200/300で再度実験を行い、各sizeでのラベル精度（BLEURT等＋人手評価）を比較。安定性評価（標準偏差）も測る。  
- サンプリング制御：ジャンル／リリース年/レビュー長さで層別サンプリングを行い、混入要因を除去してから対比を行う。

G. 追加的解析手法の導入
- まずは自動特徴抽出（tf-idf + logistic regression ／ SVM で重要語ウェイトを見る）を行い、LLMに与える証拠候補を決定。  
- 埋め込みクラスタリング（Sentence-BERT等）でA/B内のサブテーマを抽出し、各クラスタ毎に対比ラベルを生成してアグリゲーションする（「A全体」を一回で扱うよりクラスタごとにまとめて説明させる方が精度向上）。

具体的プロンプト例（改良案）
- 「与えられたA群とB群の差を1〜3語で表す短いラベルを1件だけ出力し、その直後に'|'で区切って、そのラベルを支持する上位3フレーズを'\"'で囲って出力せよ。出力例: narrative focus | "good story", "relatable characters", "story driven"」  
- Few-shotには実際のA/B差分→正解ラベルの例を3つ以上与える（必ずラベルの粒度を一致させる）。

最後に：今回のケースで直ちに行うべき優先アクション
1. raw model responses の確認（最優先）→ 出力があるか・フォーマットは何かを確認。  
2. 評価スクリプトの入出力ログ確認（参照ラベル読み込み確認）。  
3. 上記の単語差分解析（log-odds/top-20 token）を実行し、その結果をLLMに与えるプロンプトで再実行。  
4. 評価指標をBLEURT等に切替、及び人手評価の小規模試験を並行。

まとめ（結論）
- 提示サンプル解析からは、グループAは「技術・操作に関する否定的な具体語」が目立ち、グループBは「総評・推薦・ストーリー評価」に関する肯定的語が目立つ。したがって「story related characteristics」という正解ラベルはBに由来する差分を指す可能性が高いが、A側にも散在するため容易に確定できない。  
- 実験上の直接的問題は「LLM出力の欠落／評価パイプラインのエラー」に起因する可能性が高く、まずはログ確認を行うべきである。  
- 改善は（1）差分語抽出→証拠集積→LLM要約の段階的パイプライン、（2）プロンプトの厳格化とfew-shot例増強、（3）評価指標の刷新と人手評価併用によって実現可能である。

必要であれば、代表サンプル群から自動で上位20語（出現数）とlog-odds上位20語を算出するスクリプト例や、改良プロンプトのテンプレート、評価ワークフローのチェックリストを提供します。どの情報が欲しいか教えてください。

## steam_group_sizeカテゴリ全体の考察

以下は「steam_group_size」カテゴリ（Steamレビューの群比較での対比因子自動生成実験群）に対する、与えられた個別実験考察ログ（20件）を踏まえたカテゴリ全体の総合考察です。問題点の要約、観察された共通パターン、設定要因の影響、今後の示唆を優先度付きで整理しました。

1. カテゴリ全体の傾向（共通パターン）
- 出力欠落／評価ゼロが支配的
  - 多くの実験で「LLM生成対比因子」が実質的に空で、BERT/BLEU が共に 0.0 になっている。これは単なる性能低下ではなく「生成または評価パイプラインの欠陥（出力保存ミス、エンコード/前処理の不整合、タイムアウト／トランケーション等）」を強く示唆する。
- データ側の確度は概ね高い（ラベル妥当性）
  - 代表サンプル観察では、各カテゴリ（gameplay/visual/story/audio）に対応する語彙が群のどちらかに確かに偏在しているケースが多い（例：gameplay→controls/combat、visual→graphics/artstyle、story→narrative/characters、audio→soundtrack/voice）。つまり「正解ラベル自体は妥当」であり、問題はLLMの出力取得・整合化にあることが多い。
- ノイズ・トピック混在が顕著
  - Steamレビューは長文・罵倒・編集タグ（[h1],[b]等）や固有名詞、複数トピック（アート/音楽/操作/価格/サーバ）が混在するため、集合レベルの差分は「単一軸」ではなく複合的になりがち。これがラベル化の難しさを増している。

2. パフォーマンスの特徴（スコア分布と傾向）
- スコア分布
  - ログ上は多くが BERT/BLEU = 0.0。出力が存在すればBLEUは語彙一致により低めになりがち、BERTScore は通常一定の非ゼロ値を示すはずだが今回はゼロが多発しているため「評価不能（出力欠落/処理ミス）」が主因。
- 高スコア実験の共通特徴（観察からの仮説）
  - （観察が限られるが）高評価が期待できる条件は、（1）群内で特定トピック語が高頻度に偏在、（2）プロンプトが短ラベル出力を明確に指示、（3）前処理でノイズを除去し差分語を与えた、という組合せ。
- 低スコア（ゼロ）実験の特徴
  - ほとんど全ての実験に共通：Few-shot=1 のまま生テキストを大量投入、出力の生ログ未保存／評価前処理ミス、BLEUのみ依存等。これらが低スコア（あるいは評価不能）を招いている。

3. 設定パラメータの影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力形式（短いラベル vs 長文説明）」の誘導力が弱く、不安定。タスク（集合差分→短い概念ラベル）では 3–5 shot の方が安定性が上がるというログ中の示唆が一貫している。
- グループサイズ（group_size）
  - 小（50）: ノイズや偶発的サンプルに影響されやすく、代表性が不安定。  
  - 中（100–150）: 差分シグナルが比較的安定して抽出しやすいバランス帯。多くの改善案で推奨されているのはこのレンジを基準に試行すること。  
  - 大（200–300）: 多様性が増えシグナルが希薄化する一方で、十分な前処理（クラスタリング・代表抽出）を行えば安定化も可能。だが生データをそのままプロンプトに入れるとトークン制限や情報過多で失敗しやすい。
- モデル（gpt-4o-mini 等）
  - gpt-4o-mini 自体は汎用性が高いが、長文集合比較や厳密なフォーマット出力（短い名詞句ラベル）に対してはプロンプト工夫と前処理が不可欠。モデル変更（より大きなモデル）で改善は見込めるが、まずはパイプライン／プロンプト改善が先決。
- 評価指標の選択
  - BLEU は短いラベル比較に不向き（語彙揺れに敏感）。BERTScoreは意味的に優れるが今回の0多発はパイプライン問題を示す。BLEURT、BARTScore、埋め込みコサイン等の導入と、多参照/同義語辞書の準備が推奨される。

4. 洞察と示唆（主要知見と今後の研究方向）
A. 主要知見（要点）
- 根本問題は「運用（パイプライン）＋設計（プロンプト／前処理／評価）」にあることが最も多くの実験で示唆される。データ自体はラベルに対応するシグナルを持つことが多いが、LLM出力の取得/正規化/評価のいずれかで失敗している。
- 単にモデルを変えるより先に、（1）出力ログ保存、（2）評価パイプラインの前処理整合、（3）出力フォーマット強制、（4）差分語抽出などの前処理ワークフローを整備することが効果的。
- group_size の調整だけでなく「群内部のトピック収束度（視覚語率、音語率など）」を計測し、最適なサンプル数・代表化方法を決めるべき。

B. 優先度付き改善提案（実務的）
1) 最優先（必ず行う）
  - raw LLM 出力（API応答）を全て保存し、出力が空かどうか、トークン上限で切れていないかなどを検証する。出力が空なら直ちにAPIログ／エラー原因を調査。
  - 評価パイプラインの入出力前処理を固定：正解ラベル・生成ラベルともに同一の正規化（小文字化・trim・Unicode正規化・HTML除去）を行い、評価を再実行する。
2) 高効果（次に実施）
  - Prompt engineering：Few-shot を 3–5 ショットに増やし、出力を「1–3語の英語名詞句のみ (no explanation)」に厳格化。成功例 / 失敗例（bad example）を混ぜて示す。
  - 前処理パイプライン導入：TF-IDF / log-odds で A/B の差分キーワード上位を抽出し、そのリストを LLM に与えてラベル命名させる（二段階化）。またはクラスタリングで代表文を抽出して提示する。
  - 評価指標改善：BLEU廃止→BLEURT/BARTScore/embedding cosine を導入し、同義語マップ（許容ラベル群）を作る。
3) 中長期（実験設計 / 研究）
  - group_size 感度実験：50/100/150/200/300 の各サイズで複数ラン（シード）を実行し、ラベル出力の安定度（同一ラベル再現率、embedding類似度分散）を評価して最適サイズを選定。
  - ハイブリッドワークフロー：統計的手法で候補語を自動抽出 → LLM が短ラベルに正規化 → 小規模人手で承認する運用（半自動ラベリング）を構築。
  - 出力の透明化：LLMにラベルと同時に「支持する代表例/キーワード」を出力させ、説明可能性（explainability）を担保する。

C. 研究的示唆
- 集合差分ラベリング（group-level concept discovery）は「ノイズの多いUGC（Steam等）」では直接LLMに大量テキストを渡すだけでは不安定。統計的差分解析（log-oddsなど）とLLMの組合せ（証拠→命名）が有望である。
- 評価手法研究：短ラベル評価に適する自動指標の検証（BLEURT等）と人手評価の少量混入によるキャリブレーションが必要。
- 出力欠落の発生源（API側タイムアウト・filtering・プロンプト長超過など）を定量的にログし、再現性の高い障害モデルを作ると将来的な改善に寄与する。

5. 最後に：短期チェックリスト（実装担当向け）
- 保存ログの確認（raw responses + HTTP status）→ 出力が無ければAPIログを精査。
- 評価パイプラインの単体テスト（既知のref + hyp でBERT/BLEUが期待値を返すか）。
- 簡素な sanity-check プロンプト（3対3の代表テキストを入力、temperature 0、出力1語）で動作確認。
- 差分語の自動抽出を1回実行（各群 top-20）して、LLMに与えてラベル化する方式を試す（短期実験）。

———

要約：現状の失敗は主に「出力欠落／評価パイプライン不備」＋「プロンプト設計と前処理不足」に起因する。データ自体はラベルと整合するシグナルを含む場合が多い（各aspectごとに該当語彙あり）。まずはログ確認と前処理＋プロンプト改善（3–5 shot・短ラベル強制）、差分語抽出を組み合わせた二段階ワークフローを実装し、その上でgroup_size感度テストと評価指標の改善（BLEURT等）を進めることを強く推奨します。必要であれば、（A）代表サンプルからのTF-IDF/log-odds抽出結果、（B）3–5-shotプロンプトテンプレート、（C）評価パイプラインチェックリストの具体案を作成します。どれを先に出しますか？

