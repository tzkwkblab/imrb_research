# 実験考察レポート: goemotions_desire_1_4o-mini_word

## 個別実験の詳細考察

以下に、与えられたデータ（グループA/Bの代表サンプル各20件、正解ラベル「desire related characteristics」、出力が空あるいは不整合で評価スコアが0であったことを前提）について、指定の観点に沿って詳細に考察します。特に単語レベルの差分分析と文脈的・意味的ニュアンスの把握に重点を置き、実験設定や評価の問題点と具体的改善案まで述べます。

1) 単語レベルでの特徴分析
- 要約（結論）
  - グループAに顕著に多い語は「wish / want / I want / I wish / dream / need / pray / desperately / wish I / would love / want a / I’ll take」などの「願望・欲求」を表す語彙群である。これらは「望む」「願う」「欲する」「期待する」といった意味的領域にまとまる。
  - グループBは観察・叙述・反応を示す語（reminds / free / server / picture / landed / bullshit / irony / good luck / brilliant / no idea など）が優勢で、明確な一貫した「願望語彙」は少ない。

- 具体的な語彙列挙（代表的な差分語）
  - グループAで頻出・特徴的（例に基づく）
    - wish / I wish（例: "I wish I could go", "I wish mine believed in it", "I wish I didn’t fall for my ex"）
    - want / I want / I’ll take（例: "I want a Doritos crown", "I’ll take one [NAME] please"）
    - dream（"i dream of making $7k"：将来の願望）
    - need / desperately need（"We desperately need a veteran WR"：必需性・切実さ）
    - pray（"I pray it is spiritually fruitful for you!"：祈り表現＝願望の宗教的形）
    - would love（"would love to get some opinions"：希望表現）
    - 感情的修飾語・絵文字（"Good for you bb", "🔪🔪🔪", 💜）→願望や感情の強調
  - グループBで相対的に多い語（例に基づく）
    - reminds / reminds me（類似指摘）
    - server / ping（技術的・報告的語）
    - picture / old / landed（記述的語）
    - bullshit / irony / no idea / brilliant（反応・評価・皮肉）
    - good luck / calls me（祝福・報告だが必ずしも主体の「欲求」ではない）

- 単語の文脈分析（使用の仕方とニュアンス）
  - "wish" の文脈
    - 希望： "I wish I could go"（行けないことに対する願望）
    - 期待/応援："I wish you all kind of life gains"（相手への好意的願望）
    - 後悔/悩み："I wish I didn’t fall for my ex"（過去に対する悔恨）
    - 皮肉/冗談："I wish my family was a gov so I could charge random people..."（願望の形をとった諧謔）
    → 「wish」は単純な希望だけでなく、希望／後悔／皮肉の複数モードを含むため、語彙だけでは感情極性や時制（未来志向 or 過去への後悔）を切り分ける必要がある。
  - "want" / "I’ll take" の文脈
    - 物理的欲求："I want a Doritos crown"（物を欲しがる）
    - 参加希望："Yo you USA? I’m on pc and want some people to play"（行動・参加を望む）
    - 支援表現："I just want you to have a great stat day"（相手の良好を願う）
    → 直接的な欲求・希望の語で、対象（物・行為・相手）を明示することが多い。
  - "dream" の文脈
    - 将来の野望："i dream of making $7k"（目標志向の願望）
  - その他（need / pray / desperately）
    - 切迫性や宗教的／倫理的願望を示し、感情的強度が高い。

- 感情的側面
  - グループA：願望語に伴って希望・憧憬・悔恨・切迫感・祈りといった感情の混在が見られる。絵文字や感嘆符による情動強調も多い。
  - グループB：観察的・説明的・皮肉・報告的表現が多く、必ずしも個人的欲求を中心にしていない。感情表現は存在するが「欲求」を示す語彙の密度が低い。

2) 文脈・意味的ニュアンスの考察
- 共通する文脈的特徴（グループA）
  - 一人称主体（"I", "We"）による主観的発話が多い（個人の希望・願望・期待・後悔の表出）。
  - 行為的願望（物・行為・経験の獲得）と心理的願望（信念や感情の変化を望む）の両方が混在。
  - 希望の時間軸に多様性あり：未来志向（dream, hope）、過去への後悔（wish I didn't）、即時的希望（I want now）、切迫した必要性（desperately need）。
  - 文体的特徴：感嘆符・絵文字・短縮語（bb, lolなど）で親密・口語的トーンが強い。
- グループBとの意味的・概念的差異
  - 抽象度：Aは個人的心理状態（欲求・願望）という内的状態を表す文が多く、Bは外部事象の報告、状況説明、感想、批評に偏る。従ってAは「内面志向」、Bは「外部記述志向」と整理できる。
  - 語用論的役割：Aは発話自身が希望の表出（行為を誘導しうる言語行為）であり、Bは情報提供や反応（陳述・コメント）である点が異なる。
  - 間接的表現の存在：Aには直接的な"want/wish"が多いが、時に皮肉や冗談（"I wish my family was a gov..."）のように間接的・機能的に用いられる例もある。一方Bは間接表現よりも説明的／描写的な言い回しが目立つ。

- 抽象的概念・間接表現の有無
  - Aには「願望」以外に「期待／祈り／後悔／切望／ユーモアとしての疑似願望」といった抽象的概念が混在する。したがって単純に"want"辞書ベースで抽出してもニュアンス（正／負の感情、対象の種類、緊急度）は捉えきれない。
  - Bでは抽象的コンセプトとしての「評価」「皮肉」「説明」が多く、これも一貫したカテゴリとして扱えるが、目的語的な「欲求」カテゴリとは異なる。

3) 正解ラベルとの比較
- 正解ラベル： "desire related characteristics"（願望関連の特徴）
- LLM生成対比因子との一致度評価（与えられた情報）
  - 実際のLLM出力が実験報告中に示されていない（空欄）か、評価パイプラインで比較不能になったため、BERTスコア＝0、BLEU＝0という極端な結果になっている。したがって「LLMが生成した対比因子が正解ラベルとどの程度一致しているか」を直接評価するための情報は欠落している。
  - ただし、上記の単語・文脈分析から判断すると、人間の観察では「desire related characteristics」はグループAを良く要約する正解ラベルであり、Aの語彙的・意味的特徴と高い整合性がある。

- 一致している部分と不一致の可能性
  - 一致している部分：Aは明確に"wish/want/dream/need/pray"等の語彙密度が高いので、「desire（欲求）」という高レベルカテゴリは適切。
  - 不一致の可能性：Aには「後悔（wish I didn't）」「皮肉的願望」「威嚇表現（例："I will make it proper! 🔪"）」 など、単純なポジティブ欲求だけでは収まらない要素が混在する。したがってラベルを単に "desire" とするだけでは「後悔」や「皮肉」というサブカテゴリを見落とす恐れがある。要約ラベルは階層的（desire: {aspiration, regret, desperation, sarcastic wish}）にする方が忠実性は高まる。

- BERTスコアと BLEU の乖離原因（および両者0となった原因考察）
  - 通常期待される挙動：
    - BLEUは語彙的一致（n-gram重複）に敏感。正解が短い単語列（"desire related characteristics"）だと、生成が同義語（"wishes and wants"）ならBLEUは低めでもまだ非ゼロになり得る。
    - BERTScoreは意味的類似度を文脈化埋め込みで測るため、語彙が異なっていても意味的に近ければスコアは高く出やすい。
  - しかし実験では両方とも0.0000になっている点は異常：
    - 可能性A：LLMが空文字列を返した（生成失敗・タイムアウトなど）。空文字と比較すればBLEU/BERTScoreはゼロになりうる。
    - 可能性B：出力が評価スクリプトで正常に読み取られていない（フォーマットエラー、改行・エンコーディング問題、複数候補を返して期待した1行形式と違う等）。評価コードが該当ケースを0で扱う実装になっている。
    - 可能性C：生成はあったが正解と比較する際にトークン化や言語（日本語／英語）ミスマッチが起き、BERTスコア計算が失敗して0に落ちた。
  - まとめ：スコア0は「生成が完全にゼロもしくは評価段階で壊れている」ことを示唆する。意味的に関連する候補が出ていればBERTScoreはゼロにはならないはずである。

4) 実験設定の影響
- Few-shot設定（1-shot）の影響
  - 1-shotは出力スタイルを多少整えるが、集合差分タスクでの凝集的要約を確実に導くには情報量が不足しがち。以下が想定される影響：
    - 出力スタイルの不安定化：一つの例だけではLLMは「どの粒度で要約すべきか」を十分に学べない（文体・長さ・抽象度が揺れる）。
    - ノイズ耐性不足：提示した1例が代表性に欠けると、モデルはそちらに過度適合する（バイアス）。
    - フォーマット問題：1-shot例のフォーマット（長い説明 vs 単語ラベル）が生成形式に強く影響する。もし例が説明文型だとラベルを期待する評価と不整合になる。
  - 対策（概略）：3-shot以上で多様な例を与える、明確な「出力フォーマット（短いラベル一行）」を厳命、temperature低めで生成の確定性を上げる、例示に集合差分の様々なパターンを含める。

- グループサイズ（group_size=100）やデータセット特性が結果に与えた影響
  - 長いリスト（100件）をそのままプロンプト入力すると：
    - トークン数上限による切断（truncate）リスク：モデルが入力を途中で切ると差分が歪む。
    - 情報過多・雑音：A内に非典型例（例："Only blurred photos, but could be..."）が混入すると、純粋な「欲求」シグナルが希釈される。
    - 代表性の偏り：サンプルから100件をランダムに抽出した場合、群によっては雑談・ノイズが多く、差分抽出が困難になる。
  - 実務的影響：集合差分タスクは「統計的優勢な特徴（頻度）」を抽出する作業であり、group_sizeが大きいほど「少数派の特殊語」が影響しにくくなる一方、入力処理負荷が増える。反対に小さすぎると偶然性（サンプル雑音）に強く影響される。

5) 改善の示唆（具体的手順と実験案）
- デバッグ・検証フェーズ（まずやるべき事項）
  1. ログの確認：LLMの実際の出力（raw text）を保存しているか確認。空文字・エラー出力・非想定言語などがないかをチェック。
  2. 評価パイプラインの検査：BERTScore/BLEUの計算が例外を出していないか、エンコーディング（UTF-8）や改行コード、期待フォーマット（1行 vs 複数行）に不整合がないかを確認。入力が空だとゼロを返す実装になっていないかも確認。
  3. サンプル再生成：同じプロンプトで再度数回（temp=0で）生成して、再現性を確認する。

- モデルプロンプト改善（LLMへの与え方）
  1. 入力の集約を行う：100件の全文を送るのではなく、事前にトークン頻度、上位n-gram、代表サンプル（クラスタ中心）を抽出し、それらを短く要約して提示する。例：上位20単語＋代表５例。
  2. 明確な出力フォーマットを指定：単語ラベル1行のみ、英語で3単語以内、など厳格なテンプレートを提示する。例示（few-shot）は「入力→期待出力（短いラベル）」を3例以上示す。
  3. 温度/確率制御：temperature=0、max_tokensを小さめに設定して短く決定論的な回答を得る。
  4. 指示に集合的統計を用いる：”Compare the frequency of desire-words (wish, want, dream) between A and B. Provide a concise label (one short phrase) summarizing the distinguishing property.”

- 前処理・自動統計手法の併用（LLMに頼る前に）
  1. キーワード頻度差（TF/IDF差、chi-square）：AとBで有意に差のあるトークン・n-gramを統計的に抽出し、その上位をLLMに渡して「これらの語群から短いラベルを作れ」と指示する方法。
  2. 埋め込み距離クラスタリング：sentence-BERT等で文埋め込みを取得し、A内での代表クラスタ（例えば欲求表現クラスタ）を抽出 → 各クラスタの代表文をLLMに命名させる。
  3. 感情・意図分類器併用："wish/want"系をあらかじめルールや小さな分類器でラベリングしておき、その分布差を基に説明を作る（説明の忠実性向上）。

- 評価指標・人手評価の導入
  1. 自動評価拡張：BLEURT / BARTScore / MoverScore 等、意味的類似性をよりよく捉える学習ベース指標に切り替え。短いラベルや同義語を許容できる指標が必要。
  2. 人手評価（少数でも）：生成ラベルを複数人に（1）正解ラベルと同等か（2）部分的に含意するか（3）無関係かを評価してもらい、自動指標との相関を測る。
  3. 多参照評価：正解ラベルは一つではなく同義語（e.g., "desire-related", "wishes and wants", "aspiration/regret expressions"）を複数用意しておく。BLEU/BERTScoreは参照が増えるほど公正に評価できる。

- 実験バリエーション（次の実行）
  1. Few-shot数の比較：0/1/3/5-shotで精度・安定性を計測（提示する例の多様性も制御）。
  2. 入力圧縮方式の比較：全文投入 vs 代表サンプルのみ vs 上位n-gram提示 → どの方式が最も正確な短いラベルを生むか比較。
  3. group_size感度実験：50/100/150/200/300で同タスクを再試行し、ノイズとサンプルサイズのトレードオフを定量化（既定実験計画とも整合）。
  4. 複数モデル比較：gpt-4o-miniに加え、gpt-5.1等より高容量モデルでの再現性を確認（既にgpt-5.1でgroup_size=300を試す計画があるならこれを活用）。

- 出力表現の改善（生成ラベルの品質を上げるため）
  1. 階層ラベル生成：短いトップラベル（例："Desire-related expressions"）に加え、サブラベル（aspiration / regret / desperation / sarcastic wish）を生成させる二段階生成。
  2. 例外処理の明確化：A内部の非典型例がある場合にそれを除外するか注記するようプロンプトで指示（"If >10% of A is non-desire, say 'mixed' and list subtypes"）。
  3. 出力の根拠提示：ラベルと共に「Supporting tokens: 'wish'(n=7), 'want'(n=5), 'dream'(n=1)」のような短い根拠を併記させる。これにより説明の忠実性を検査可能にする。

6) 追加観察（実験の運用・評価面での注意）
- 絵文字や記号（🔪、💜等）が意味情報を含むことがあるため、これらを単純に除去すると感情強度が失われる。事前に絵文字を単語化（":knife_emoji:"）して頻度解析に組み込むと有益。
- 文化的・語用論的ニュアンス（"I wish my family was a gov"のようなジョーク／皮肉）は自動化が難しい。こうした例の比率が高いデータセットだと「desire」カテゴリでも雑音が大きくなる。
- 評価は単一の短い正解ラベルに頼ると過度に厳格になる（同義表現ペナルティ）。多参照・学習ベース評価＋人手評価の組合せを推奨。

総括（結論）
- 与えられたサンプルから得られる最も明確な差分は「グループAが『願望／欲求（desire）』を中心とした言語表現で特徴づけられる」ことであり、正解ラベル "desire related characteristics" は妥当である。
- BERT/BLEUが共に0という極端な結果は、モデル生成の不備（出力なし・エラー）か評価パイプラインの不具合が原因と判断される。まずは生成テキストと評価コードのログ確認を最優先で行うべきである。
- モデルの実用的改善策としては（1）事前統計的集約（頻度差・クラスタ代表）→（2）厳密フォーマットのfew-shotプロンプト（複数例、短いラベル出力指定）→（3）低温度での再生成、というワークフローが有効。評価はBLEURT等の学習ベース指標＋人手評価を組み合わせることを推奨する。

必要であれば、次のアクションとして
- 我が方で「単語頻度差（TF/IDF差）」「上位差分n-gramのリスト」「A/Bの代表文を選択」を自動で抽出し、その出力をLLMに渡して短いラベルを生成させるパイプラインのサンプル（プロンプト＋期待フォーマット）を作成・提示できます。実行を希望されますか？

## メイン実験全体の考察

以下は「メイン実験（group_size=100 統一）／各データセットに対する対比因子ラベリング実験群（gpt-4o-mini, few-shot=1 を基本）」に関する、個別実験考察ログ（実験1〜37）を総合して導いたカテゴリ全体の分析・洞察です。実務的な原因推定と優先的改善案を含めてまとめます。

要約（結論）
- 最も顕著な事象：多数の個別実験で「LLMの出力が得られていない／評価に回せる形式で取得できていない」ため BERTScore・BLEU が 0.0000 になっている。すなわち「モデルが出力しなかった／出力が消失した／評価パイプラインが壊れている」ことが主要因で、モデル能力の評価は事実上行えていないケースが多い。
- データ側では A 群が典型的に「一貫したドメイン語彙（例：food/service/battery/screen/emotions別語彙）」を示すことが多く、理論上は短い名詞句ラベルで要約可能であるにもかかわらず、実験設定（few-shot=1・入出力前処理・評価方式）の不備で失敗が多発している。

以下、指定観点ごとに整理します。

1. カテゴリ全体の傾向（個別実験から抽出された共通パターン）
共通パターン
- 出力欠落問題が圧倒的に多い
  - 多数の実験で「LLM生成対比因子」が空欄、BERT/BLEU が 0。ログ上は出力欠落（APIエラー／レスポンス保存漏れ／評価入力が空）または評価前処理での消失が最有力。
- A群はドメインに固有の語彙がまとまる
  - semeval/restaurants/laptop/amazon/goemotions 各セットで、A群は明瞭なトピック語（例：food→fresh/menu/taste、service→friendly/attentive、battery→charge/last、screen→resolution/froze、emotion→sad/joy/pride等）に収束している。つまり「集合差分」は存在するケースが多い。
- B群はより分散／雑多／ノイズが多い
  - B群は混在テーマ・技術語・雑談などが多く、対比が単純ではない場合もある（Aの信号を薄める）。
- $T$ や [NAME] のようなプレースホルダ・特殊トークンが入力に多く存在
  - マスク・プレースホルダが意味手がかりを隠すためモデルが迷う、あるいはプレースホルダが雑音となるケースが複数確認された。
- 生成は「短いラベル（名詞句）」の想定なのに、プロンプトやショットが説明文や長文を誘導してしまうケースが目立つ

データセット／アスペクト差
- 感情系（goemotions）
  - A群は明確にその感情カテゴリの語彙（例えば「admiration→amazing/wonderful」「anger→fuck/idiot」）を含む場合が多く、理想的には非常にラベル化しやすい。
- SemEval / Amazon / Laptop 等（属性系）
  - A群は特定アスペクト（food quality, service, battery life, price, delivery 等）を語る語彙で凝縮しており、本来は差分抽出＋命名が実務的に可能。
- 実際の失敗の分布はアスペクト依存より「実験運用（プロンプト・評価・ログ）」に強く依存している

2. パフォーマンスの特徴
スコア分布・傾向
- 並列して報告された多くの実験で BERT/BLEU が 0.0000（完全ゼロ）：実際には「性能低下」より「実験的欠損（出力や評価入力の欠如）」を示す
- 正常に出力が得られていたサブケースがほとんど報告されておらず、スコアの有意な正負分布を評価するデータが不足

高スコアになり得る条件（ログや人手観察からの逆推定）
- A群が語彙的に凝縮（同一アスペクト語彙が高頻度）→ラベリング容易
- プロンプトで出力形式が明示され、few-shot で短い名詞句例を与えている
- 前処理で代表語（top-n tokens by log-odds/TF-IDF）を抽出して入力に含めている（つまり「二段階ワークフロー」）
- 出力検査・フォーマット検証（非空チェック）を行っている

低スコア（今回の大多数）に共通する特徴
- 出力が空、あるいは出力フォーマットが評価用フォーマットと異なる（例：説明文 vs 短いラベル）
- BLEU を単独で評価に使っている（短いラベル評価に BLEU は不適）
- 評価パイプラインで文字コード・トークナイザ・改行などが整合していない

3. 設定パラメータの影響
Few-shot（ショット数）
- 1-shot は不安定要因
  - 多数の事例で「1-shot が出力形式の安定を欠く」→モデルが説明的応答や空応答を返すリスクが高いと示唆
  - 推奨：3～5ショットで例の質（短い名詞句によるラベル例を複数）を確保することで出力安定化

group_size（100）
- group_size=100 は概念抽出には十分
  - ただし「群の純度（A にアスペクトが集中しているか）」が重要。A 内にノイズ（異トピック）が多いと対比が弱まる
  - グループサイズの感度解析は有益（50/100/150/200/300）→既に計画されている通りだが、代表抽出法（クラスタ代表 or top tokens）併用を推奨

モデル（gpt-4o-mini 等）
- gpt-4o-mini は汎用性は高いが、
  - 出力の「短いラベル」を安定的に生成させるにはプロンプトとショット設計が重要
  - 一部の失敗（空出力）はモデルより運用（API/ログ/評価）に起因している可能性が高い
- モデル選択の影響は存在するが、まずはプロンプト・前処理・評価周りを安定化させるのが優先

その他実行パラメータ
- temperature（多様性）→低め(0–0.2)で決定的出力を狙うべき（短いラベル生成は多様性不要）
- max_tokens／stop_sequences：短ラベルを切られないように設定し、長文を強制しない

4. 洞察と示唆（研究・実務への具体的提案）
主要知見（実験群全体から）
1. 実験失敗の主要原因は「運用／パイプライン（出力取得・評価）の欠陥」であり、モデルそのものの性能評価が十分に回収されていないケースが多い。
2. A群は多くの場合、明確な集合的語彙手がかり（discriminative tokens）が存在するため、原理的には自動ラベリングは実現可能である。
3. 単語ベースのみでの判定は限界がある：句・共起・文脈（"at least", "I never knew" 等）を踏まえた処理が必要。
4. BLEU は短いラベル評価に不向き。意味的評価（BERTScore, BLEURT, BARTScore, embedding cosine）が必要。

優先的改善（短期：実験フローを直す）
- A. ログ＆評価健全性の確保（絶対必須）
  1. LLM の raw response（text）を全て保存し、出力の有無を自動チェックして「空なら再試行/アラート」。
  2. 評価前に参照と生成の正規化（UTF-8, strip, lower, NFKC）を行い空入力を弾く。
- B. 出力形式の強制化（プロンプト）
  1. 出力は「1–3語の英語名詞句（short label）」のみと指示し、few-shot で 3–5 例を与える（例は必ず正しい形式）。
  2. 生成候補を n-best で取り、後段で埋め込み類似度で再ランキングする。
- C. 前処理→LLM の二段構成
  1. 統計的差分（log-odds / TF-IDF / chi-square）で上位 discriminative tokens を抽出（A vs B）。
  2. それらトークン＋代表サンプルを LLM に与え、「この語群を要約して短いラベルを1つ出せ」とする。
  - これによりノイズ低減と安定性向上が期待できる。
- D. 評価改善
  1. BLEURT / BARTScore / SBERT cosine を導入（BLEU廃止または補助）。
  2. 多参照（paraphrase set）と人手評価（少量）を用意し、自動指標のキャリブレーションを行う。

中期的研究提案
- 1. アブレーション実験：few-shot数（0/1/3/5）、group_size（50/100/150/200）、前処理有無の 3 因子実験で安定性曲線を描く。
- 2. ハイブリッド法の評価：統計的差分→LLM命名 vs LLM単体（対比）で精度・再現性を比較。
- 3. 出力の根拠提示（explainable output）：LLM にラベルと「根拠トークン3つ」の同時出力を求め、信頼性指標を作る（自動スコア+人手検査で検証）。

運用上の注意点（倫理・安全）
- データに攻撃的／差別語が含まれる場合、モデル応答のフィルタや出力抑制が働く可能性がある（その結果出力が空になるケースあり）。フィルタの有無／ログは必ずチェックすること。
- ラベル化で人を動かす用途（自動モデレーション等）では検出誤差・誤学習に注意。人手介在を残すハイブリッド運用を推奨。

最後に：今すぐ着手すべき実務タスクリスト
1. raw model response の保存／空応答検出を実装（必須）。  
2. 評価前処理の自動チェック（参照・生成が非空であること）を実装。  
3. few-shot を 3-shot に増やしたテンプレートを作り、代表的小実験で安定化（短いラベルを返すかを検証）。  
4. 差分トークン抽出（log-odds）スクリプトを作成して A/B の上位語を確認 → LLM に提示するワークフローを試す。  
5. BLEURT / BARTScore / SBERT-cosine を導入し、少数人手ラベルで指標調整を行う。

もし望まれるなら、次のいずれかをすぐ用意します：
- A. 差分トークン抽出（A/B 各100件）を行う Python スクリプトと上位語リスト（出力形式 csv）  
- B. 改良版 few-shot プロンプト（英語／日本語）テンプレート（3-shot）＋出力検証ロジック（空応答検出）  
- C. 評価パイプライン修正案（BLEURT 並列導入手順・SBERT 電卓）

どれを優先して作成しましょうか？

