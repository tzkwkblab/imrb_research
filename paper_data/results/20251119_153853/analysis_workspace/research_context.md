# 研究背景と経緯

## 研究タイトル

説明可能 AI のための対比因子ラベル生成手法に関する研究


## 概要


概要


## 序章


### 背景：説明可能AI（XAI）の必要性と従来の限界


近年，深層学習モデルの社会実装が進む中で，特に医療・法務・教育といった**高責任領域**では，予測の根拠を示す**説明可能性（XAI）**が強く求められている．例えば医療では，臨床・倫理・法的観点から説明可能性が中核要件として整理されている．法領域では，GDPR 文脈で反事実説明が説明責任に資する実務的枠組みとして提案されている．また，高リスク意思決定においては，後付け説明よりも本質的に解釈可能なモデルの採用が推奨される．一方，多くのモデルは出力根拠を提示しない**ブラックボックス**である．特に大規模言語モデル（LLM）では，基盤モデルの内部仕様や学習データが非公開であり，スケールに伴い予期せぬ結果が創発するため，出力根拠の事前予測と検証が難しい．こうした信頼性の課題には，まず LIME や SHAP が登場し，入力の摂動や Shapley 分解により予測を支える要因を**局所的・モデル非依存**に可視化し，望ましくない特徴依存の特定，不具合の切り分け，データ品質の点検，監査報告時の根拠提示といった実務に具体的に寄与してきた．その後，深層モデル向けには勾配に基づく帰属・可視化（Integrated Gradients，Grad-CAM）が提案され，入力と出力の感度やクラス特有の領域を可視化することで，**人間の直感に沿った手掛かり提示**が広がった．


### 課題：事後説明の信頼性危機と人手依存のボトルネック


従来の事後説明手法（LIME や SHAP）は，入力摂動や Shapley 値に基づき個別予測への特徴寄与を可視化する．しかし，相関・共線性を含む特徴群に対して説明が不安定になりやすく，同一の事例でもわずかな条件の違いで説明が入れ替わるなど，本質的な曖昧さや敵対的文脈での操作可能性という信頼性上の問題が残る．Bordt ら は，説明責任・検証可能性・規範適合といった目的に照らし，事後説明アルゴリズムが法的・倫理的文脈で要求される透明性の達成に不適切であると結論づけている． この限界を踏まえ，説明の単位をピクセルやトークンから人間にとって意味のある概念へと持ち上げる C-XAI の流れが強まっている．たとえば Concept Bottleneck Models（CBM）や TCAV は概念レベルでモデル挙動を記述するが，CBM では「縞模様」のような概念の定義・注釈を人手で用意する必要があり，大規模運用では高コストな概念整備がボトルネックとなる．さらにメカニスティック解釈における Attribution Graphs でも，抽出された特徴・ノードへの命名や群分けは自動化されておらず，研究者による手作業に依存するのが現状である．結果として，非教師ありの概念抽出（UCBM，CCE）においても，得られた表現に自然言語ラベルを与える最終段の整備が不十分であり，いわゆる「最後のワンマイル」が未解決の課題として残っている．


### 問題設定：LLM による対比因子ラベルの自動生成


本研究は，「人手ラベリングへの依存」と「意味付けの手作業」というボトルネックに対し，LLM を活用する．具体的に，ニューロン発火により得られた**テキスト集合 A（発火群）と B（非発火群）**を入力とし，その差異を自然言語で要約することで，ニューロンの発火条件に対応する**対比因子ラベル**を生成する問題を定式化する．これは概念の「発見」と「命名」を融合する新しいパラダイムであり，**忠実な説明**の自動化を志向する．


### 目的と貢献


目的は，LLM（**GPT-4o-mini**）を用いた対比因子ラベル自動生成の**実現可能性**を検証することにある．本研究の貢献は次のとおりである．


## 関連研究


### 従来の事後説明とコントラスト説明の限界


局所的特徴帰属法（LIME/SHAP） LIME や SHAP は，局所線形近似や Shapley 値により個別予測の特徴寄与を可視化する．Attention（Transformer） と組み合わせる応用もあるが，**単一インスタンス中心**であり，集合レベルの*一般的差分*を自然言語で要約する枠組みではない．加えて，曖昧さ・操作可能性が信頼性の課題として指摘されている． 反事実的説明との粒度の違い 反事実（Counterfactual）系は，Wachter ら および CEM に代表され，最小変更で予測を反転させる*what-if* の洞察を与える．しかしこれは**個別事例**を対象とし，本研究が扱う**集合（グループ）間の本質的差異**の自然言語要約とは粒度が異なる．


#### 局所的特徴帰属法（LIME/SHAP）


LIME や SHAP は，局所線形近似や Shapley 値により個別予測の特徴寄与を可視化する．Attention（Transformer） と組み合わせる応用もあるが，**単一インスタンス中心**であり，集合レベルの*一般的差分*を自然言語で要約する枠組みではない．加えて，曖昧さ・操作可能性が信頼性の課題として指摘されている．


#### 反事実的説明との粒度の違い


反事実（Counterfactual）系は，Wachter ら および CEM に代表され，最小変更で予測を反転させる*what-if* の洞察を与える．しかしこれは**個別事例**を対象とし，本研究が扱う**集合（グループ）間の本質的差異**の自然言語要約とは粒度が異なる．


### 非教師ありコンセプト発見と自動命名の課題


概念レベルの説明（C-XAI）として TCAV は概念感度を定量化するが，人手定義概念に依存する．非教師ありの流れとして，UCBM（Unsupervised CBM） は事前定義なしに概念を抽出し，CCE は合成的表現の自動発見を志向する．しかし，抽出されるのは*潜在ベクトル*であり，**自然言語の命名**は依然として人手に依存する．最先端の Attribution Graphs においても特徴の意味付けは手作業である．本研究は，これらで得られる概念ベクトル（に相当するサンプル群）に対し，LLM による**スケーラブルな自動命名モジュール**として機能することを目指す．


### コントラスティブ要約と LLM による自動ラベリング


本研究は「予測が の集合 A と の集合 B の本質的差分は何か？」という**集合差分説明**（Group-Difference Explanation）を扱う．最も近い設定は NLP の**コントラスティブ要約**であり，Lerman\&McDonald などの古典的研究に加え，近年は STRUM-LLM が属性付き構造化要約を提示する．本研究は，このタスク設定と LLM アプローチを XAI（ニューロン発火）に適用し，Web 検索等を含む複雑な多段パイプラインではなく**Few-shot プロンプティング**のみで実現可能性を検証する点で異なる．また，LLM による命名の文脈では，ABSA の標準ベンチマークである SemEval-2014 Task 4 の*アスペクト名*を対比因子ラベルの正解データと見なし，比較を行う．ChatABSA のような Few-shot による教師あり抽出と異なり，本研究は**非教師ありかつコントラスティブ**に差分ラベルを生成する点で差別化される．


### 評価指標の課題と学習ベース指標の必要性


本研究では，意味的類似度（BERTScore）と語彙一致（BLEU）に乖離が見られ，特に BLEU は本タスクに対し不適切である可能性を示唆した．今後は BERTScore を維持しつつ，人手評価との相関が高い**学習ベース指標**の導入が必要である．候補として，BLEURT（人手評価データで事前学習），BARTScore（生成確率に基づく多角的評価），MoverScore（文脈化埋め込みと Earth Mover's Distance による語彙非依存の意味距離）を挙げる．これらは，抽象概念（例：recommended, suggestion）での命名性能の評価や語彙多様性の影響をより適切に捉えるために有用である．


## 提案手法


### タスク設定


テキスト集合 A（発火）と B（非発火）を入力とし，A に特徴的で B に現れにくい表現・内容を自然言語で記述する．これを対比因子ラベルとする．


### LLM プロンプト設計


指示：*A/B を比較し，A に特徴的で B に見られない主要な違いを簡潔に述べよ*．Few-shot 例示（0/1/3-shot）を可変とし，言語・長さを指定して一貫性を確保する．


### Few-shot の役割


例示は出力スタイルを「説明的叙述」から「一意に特定する語彙」へ誘導し，精度向上を期待する．

