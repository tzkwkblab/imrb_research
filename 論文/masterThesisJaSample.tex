\documentclass[a4paper]{jreport}	% 日本語の場合

\usepackage{masterThesisJa}
\DeclareUnicodeCharacter{2248}{\ensuremath{\approx}}
\setcounter{tocdepth}{3}
\setcounter{page}{-1}

% 【必須】主題：\maintatile{日本語}{英語}
\maintitle{説明可能 AI のための対比因子ラベル生成手法に関する研究}{A Study on Generating Contrastive Factor Labels for Explainable AI}

% 【任意】副題：\subtitle{日本語}{英語}
% 副題が不要な場合は次の行をコメントアウトしてください
% \subtitle{—情報学学位プログラムの場合—}{: A Case Study in the Master's Program in Informatics}

% 【必須】発表年月：\publish{年}{月}
\publish{2025}{12}

% 【必須】学生情報：\student{学籍番号}{氏名（日本語：氏名の間は1文字空ける）}{氏名（英語：Twins登録の表記）}
\student{202421675}{清野　駿}{Seino Shun}

% 【必須】日本語の概要：\jabst{概要}
\jabst{
　大規模言語モデル（LLM）の実運用では、判断の妥当性を説明可能にすることが重要であるが、既存手法は未知データごとに人手で解釈ラベルを付与する負担が大きい。本研究は、二つのテキスト集合（A: ニューロン発火群, B: 非発火群）間の差異を自然言語で要約し、発火条件を表す「対比因子ラベル」を自動生成する枠組みの実現可能性を検証する。手法は、A/B の代表テキストを入力として、Few-shot 例示（0/1/3-shot）付きプロンプトで LLM に差分説明を生成させる。評価は SemEval レストランレビューと Steam ゲームレビューの二データセットを用い、各グループ 300 件・LLM は GPT-4o-mini で固定し、生成文と正解アスペクト名との類似度を BERT スコアおよび BLEU スコアで測定した。結果として、全体平均で BERT ≈ 0.551、BLEU ≈ 0.007 を得て、意味レベルの一致は中程度だが語彙一致は低いことが示された。また Few-shot では 1-shot が最も良く、出力スタイルが「説明的叙述」から「一意に特定する語彙」へ矯正される傾向が確認された。アスペクト別には gameplay/food 等の語彙的に安定な概念で高く、recommended/suggestion 等の抽象概念で低い。以上より、本枠組みは一定の条件で有効であり、人手ラベリングの一部代替となる可能性がある一方、抽象概念の扱いと評価指標の高度化（人手・LLM 補助、非語彙的類似）が今後の課題である。
}

% 【任意】英語の概要：\eabst{Abstract}
% 英語の概要が不要な場合は\eabst{}をすべてコメントアウトしてください
\eabst{
This study examines the feasibility of generating contrastive factor labels that explain neuron activation conditions by comparing two text groups (A: activated, B: non-activated). We prompt an LLM (GPT-4o-mini) with 0/1/3-shot examples to produce concise differences, and evaluate semantic and lexical alignment against gold aspect labels using BERT and BLEU on SemEval restaurant and Steam game reviews (300 samples per group). Results show moderate semantic similarity (BERT ≈ 0.551) but very low lexical overlap (BLEU ≈ 0.007), with 1-shot giving the best performance by shifting outputs toward uniquely identifying terms. Performance is higher for lexically stable aspects (e.g., gameplay, food) and lower for abstract ones (e.g., recommended, suggestion). The approach can partially reduce manual labeling, while handling abstract concepts and improving evaluation beyond lexical matches remain as future work.
}

% 【必須】研究指導教員（氏名の間は1文字空ける）：\advisors{主研究指導教員}{副研究指導教員}
\advisors{若林　啓}{伊藤　寛祥}


% 以下，本文を出力
\begin{document}

\makecover

\addtolength{\textheight}{-5mm}	% 本文の下限を5mm上昇
\setlength{\footskip}{15mm}	% フッタの高さを15mmに設定
\fontsize{11pt}{15pt}\selectfont

% 目次・表目次を出力
\pagebreak\setcounter{page}{1}
\pagenumbering{roman} % I, II, III, IV 
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

% 本文
\parindent=1zw	% インデントを1文字分に設定
\pagebreak\setcounter{page}{1}
\pagenumbering{arabic} % 1,2,3
\pagestyle{plain}

% 章：\chapter{}
% 節：\section{}
% 項：\subsection{}

\chapter{序章}
\section{背景：説明可能AI（XAI）の必要性と従来の限界}
近年，深層学習モデルの社会実装が進む中で，特に医療・法務・教育といった\textbf{高責任領域}では，予測の根拠を示す\textbf{説明可能性（XAI）}が強く求められている．例えば医療では，臨床・倫理・法的観点から説明可能性が中核要件として整理されている\cite{Amann2020}．法領域では，GDPR 文脈で反事実説明が説明責任に資する実務的枠組みとして提案されている\cite{Wachter2017}．また，高リスク意思決定においては，後付け説明よりも本質的に解釈可能なモデルの採用が推奨される\cite{Rudin2019}．一方，多くのモデルは出力根拠を提示しない\textbf{ブラックボックス}である．特に大規模言語モデル（LLM）では，基盤モデルの内部仕様や学習データが非公開であり\cite{OpenAI2023GPT4}，スケールに伴い予期せぬ結果が創発するため\cite{Wei2022Emergent}，出力根拠の事前予測と検証が難しい．こうした信頼性の課題には，まず LIME\cite{Ribeiro2016} や SHAP\cite{Lundberg2017} が登場し，入力の摂動や Shapley 分解により予測を支える要因を\textbf{局所的・モデル非依存}に可視化し，望ましくない特徴依存の特定，不具合の切り分け，データ品質の点検，監査報告時の根拠提示といった実務に具体的に寄与してきた．その後，深層モデル向けには勾配に基づく帰属・可視化（Integrated Gradients\cite{Sundararajan2017}，Grad-CAM\cite{Selvaraju2017}）が提案され，入力と出力の感度やクラス特有の領域を可視化することで，\textbf{人間の直感に沿った手掛かり提示}が広がった．もっとも，これらは「どこが効いていそうか」を示す便益はある一方で，説明の信頼性には課題が残る．例えば，サリエンシー図がモデルの学習内容と無関係でも類似の結果を返し得ること（sanity check）\cite{Adebayo2018}，予測を変えずに説明だけを意図的に操作し得ること\cite{Slack2020} が報告されている．

\section{課題：事後説明の信頼性危機と人手依存のボトルネック}
従来の事後説明手法（LIME，SHAP\cite{Lundberg2017}）は，入力摂動や Shapley 値に基づき個別予測の寄与を可視化するが，共線性を持つ特徴群に対して不安定になり得るなど，説明の\textit{曖昧さ・操作可能性}に起因する\textbf{信頼性の問題}が指摘されている．Bordt ら\cite{Bordt2022} は，法・倫理の文脈で期待される透明性の目的に対し事後説明は「\textit{unsuitable}」と結論付ける．この限界を克服する潮流として，説明の粒度はピクセルやトークンから\textbf{コンセプト（概念）}へ（C-XAI）と移行しているが，CBM は概念の定義・注釈を人手に依存し，\textbf{高コストな概念キュレーション}が新たなボトルネックとなる．さらにメカニスティック解釈（MI）における Attribution Graphs\cite{AttributionGraphs} でも，発見された特徴・ノードの\textbf{意味付け（命名）}は自動化されておらず，研究者による\textit{manual grouping} に依存する．これは UCBM や CCE が直面する\textbf{「最後のワンマイル」問題}に相当する．

\section{問題設定：LLM による対比因子ラベルの自動生成}
本研究は，「人手ラベリングへの依存」と「意味付けの手作業」というボトルネックに対し，LLM を活用する．具体的に，ニューロン発火により得られた\textbf{テキスト集合 A（発火群）と B（非発火群）}を入力とし，その差異を自然言語で要約することで，ニューロンの発火条件に対応する\textbf{対比因子ラベル}を生成する問題を定式化する．これは概念の「発見」と「命名」を融合する新しいパラダイムであり，\textbf{忠実な説明}の自動化を志向する．

\section{目的と貢献}
目的は，LLM（\textbf{GPT-4o-mini}）を用いた対比因子ラベル自動生成の\textbf{実現可能性}を検証することにある．本研究の貢献は次のとおりである．
\begin{itemize}
  \item \textbf{新規タスクの定義}：XAI における「概念の発見」と「命名」を統合する対比因子生成タスクと評価枠組みを提示
  \item \textbf{実現可能性の検証}：SemEval-2014 レストランレビューおよび Steam ゲームレビューを用いた 0/1/3-shot による Few-shot 検証
  \item \textbf{Few-shot 効果の分析}：1-shot が最良で，出力スタイルを「説明的叙述」から「一意に特定する語彙」へ\textbf{矯正}（全体平均 \(\mathrm{BERT} \approx 0.551\)，\(\mathrm{BLEU} \approx 0.007\)）し，\textit{意味一致は中程度・語彙一致は極めて低い}という評価上の課題を示す
\end{itemize}


\chapter{関連研究}
\section{従来の事後説明とコントラスト説明の限界}
\subsection{局所的特徴帰属法（LIME/SHAP）}
LIME\cite{Ribeiro2016} や SHAP\cite{Lundberg2017} は，局所線形近似や Shapley 値により個別予測の特徴寄与を可視化する．Attention（Transformer）\cite{Vaswani2017} と組み合わせる応用もあるが，\textbf{単一インスタンス中心}であり，集合レベルの\textit{一般的差分}を自然言語で要約する枠組みではない．加えて，曖昧さ・操作可能性が信頼性の課題として指摘されている．

\subsection{反事実的説明との粒度の違い}
反事実（Counterfactual）系は，Wachter ら\cite{Wachter2017} および CEM\cite{Dhurandhar2018} に代表され，最小変更で予測を反転させる\textit{what-if} の洞察を与える．しかしこれは\textbf{個別事例}を対象とし，本研究が扱う\textbf{集合（グループ）間の本質的差異}の自然言語要約とは粒度が異なる．

\section{非教師ありコンセプト発見と自動命名の課題}
概念レベルの説明（C-XAI）として TCAV\cite{Kim2018} は概念感度を定量化するが，人手定義概念に依存する．非教師ありの流れとして，UCBM（Unsupervised CBM）\cite{UnsupervisedCBM2024} は事前定義なしに概念を抽出し，CCE\cite{CCE2024} は合成的表現の自動発見を志向する．しかし，抽出されるのは\textit{潜在ベクトル}であり，\textbf{自然言語の命名}は依然として人手に依存する．最先端の Attribution Graphs\cite{AttributionGraphs} においても特徴の意味付けは手作業である．本研究は，これらで得られる概念ベクトル（に相当するサンプル群）に対し，LLM による\textbf{スケーラブルな自動命名モジュール}として機能することを目指す．

\section{コントラスティブ要約と LLM による自動ラベリング}
本研究は「予測が \(y{=}1\) の集合 A と \(y{=}0\) の集合 B の本質的差分は何か？」という\textbf{集合差分説明}（Group-Difference Explanation）を扱う．最も近い設定は NLP の\textbf{コントラスティブ要約}であり，Lerman\&McDonald\cite{Lerman2009} などの古典的研究に加え，近年は STRUM-LLM\cite{STRUM2024} が属性付き構造化要約を提示する．本研究は，このタスク設定と LLM アプローチを XAI（ニューロン発火）に適用し，Web 検索等を含む複雑な多段パイプラインではなく\textbf{Few-shot プロンプティング}のみで実現可能性を検証する点で異なる．また，LLM による命名の文脈では，ABSA の標準ベンチマークである SemEval-2014 Task 4\cite{SemEval2014} の\textit{アスペクト名}を対比因子ラベルの正解データと見なし，比較を行う．ChatABSA\cite{ChatABSA2024} のような Few-shot による教師あり抽出と異なり，本研究は\textbf{非教師ありかつコントラスティブ}に差分ラベルを生成する点で差別化される．

\section{評価指標の課題と学習ベース指標の必要性}
本研究では，意味的類似度（BERTScore）と語彙一致（BLEU）に乖離が見られ，特に BLEU\cite{BLEU2002} は本タスクに対し不適切である可能性を示唆した．今後は BERTScore を維持しつつ，人手評価との相関が高い\textbf{学習ベース指標}の導入が必要である．候補として，BLEURT\cite{BLEURT2020}（人手評価データで事前学習），BARTScore\cite{BARTScore2021}（生成確率に基づく多角的評価），MoverScore\cite{MoverScore2019}（文脈化埋め込みと Earth Mover's Distance による語彙非依存の意味距離）を挙げる．これらは，抽象概念（例：recommended, suggestion）での命名性能の評価や語彙多様性の影響をより適切に捉えるために有用である．

\chapter{提案}
\section{タスク設定}
テキスト集合 A（発火）と B（非発火）を入力とし，A に特徴的で B に現れにくい表現・内容を自然言語で記述する．これを対比因子ラベルとする．

\section{LLM プロンプト設計}
指示：\textit{A/B を比較し，A に特徴的で B に見られない主要な違いを簡潔に述べよ}．Few-shot 例示（0/1/3-shot）を可変とし，言語・長さを指定して一貫性を確保する．

\section{Few-shot の役割}
例示は出力スタイルを「説明的叙述」から「一意に特定する語彙」へ誘導し，精度向上を期待する．

\chapter{評価実験}
\section{データセット}
SemEval-2014 レストランレビューと Steam ゲームレビューを用いる．各データはアスペクト（food, price, service / gameplay, story, audio 等）を持つ．

\section{設定}
各グループ 300 件，LLM は GPT-4o-mini，Few-shot は 0/1/3-shot を比較．

\section{評価指標}
生成文と正解アスペクト名の類似度を BERT スコア（意味）と BLEU（語彙）で評価する．

\chapter{結果}
\section{全体統計}
全体平均は \(\mathrm{BERT} \approx 0.551\)，\(\mathrm{BLEU} \approx 0.007\) であり，意味的一致は中程度，語彙一致は極めて低い．

\section{Few-shot の影響}
0/1/3-shot の比較で 1-shot が最良．例示が出力スタイルを一意化し，正解語彙への収束を促す．

\section{アスペクト別傾向}
ゲームレビューでは gameplay, story, audio が高く，recommended, suggestion は低い．レストランでは price が間接表現の多さから低下する傾向がある．

\chapter{考察}
\section{意味と語彙の乖離}
BERT は中程度だが BLEU は極低であり，意味は合致する一方，語彙は多様で一致しにくい．

\section{抽象アスペクトの困難}
recommended/suggestion のような抽象・メタ的概念は低スコア．price でも間接表現が多い場合に低下．

\section{Few-shot の効果}
1-shot が最良で，最小限の例示が出力を一意化することが示唆される．

\chapter{まとめ}
\section{結論}
LLM を用いた対比因子ラベル生成は一定条件で有効で，人手ラベリングの一部代替となる可能性がある．

\section{今後の課題}
評価の高度化（人手・LLM 補助，語彙非依存指標），レビュー以外のデータセットへの拡張，TF-IDF 等の非 LLM ベースラインの構築．

\chapter*{謝辞}
\addcontentsline{toc}{chapter}{謝辞}


% 参考文献（References）
\newpage
\addcontentsline{toc}{chapter}{参考文献}
\renewcommand{\bibname}{参考文献}

%% 参考文献に bibtex を使う場合
%\bibliographystyle{junsrt}
%\bibliography{hoge}

%% 参考文献を直接ファイルに含めて書く場合
\begin{thebibliography}{99}

\bibitem{SemEval2014}
Pontiki, M., Galanis, D., Papageorgiou, H., Androutsopoulos, I., Manandhar, S., et al., "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", Proceedings of SemEval 2014, pp. 27-35.

\bibitem{BLEU2002}
Papineni, K., Roukos, S., Ward, T., Zhu, W. J., "BLEU: a Method for Automatic Evaluation of Machine Translation", Proceedings of ACL 2002, pp. 311-318.
\bibitem{BLEURT2020}
Sellam, T., Das, D., Parikh, A. P., "BLEURT: Learning Robust Metrics for Text Generation", Proceedings of ACL 2020.

\bibitem{BARTScore2021}
Yuan, W., Neubig, G., Liu, P., "BARTScore: Evaluating Generated Text as Text Generation", Advances in Neural Information Processing Systems (NeurIPS), 2021.

\bibitem{MoverScore2019}
Zhao, W., Peyrard, M., Liu, F., et al., "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance", Proceedings of EMNLP-IJCNLP 2019.


\bibitem{BERT2018}
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", arXiv:1810.04805, 2018.

\bibitem{AttributionGraphs}
Transformer Circuits Team, "Attribution Graphs", 2025, https://transformer-circuits.pub/2025/attribution-graphs/methods.html

\bibitem{UnsupervisedCBM2024}
"Unsupervised Concept Bottleneck Models", arXiv:2407.03921, 2024, https://arxiv.org/abs/2407.03921.

\bibitem{CCE2024}
"Compositional Concept Extraction", Proceedings of PMLR, vol. 235, 2024, https://proceedings.mlr.press/v235/stein24b.html.

\bibitem{Ribeiro2016}
Ribeiro, M. T., Singh, S., Guestrin, C., "Why Should I Trust You?" Explaining the Predictions of Any Classifier, Proceedings of KDD 2016, pp. 1135-1144.

\bibitem{Lundberg2017}
Lundberg, S. M., Lee, S.-I., "A Unified Approach to Interpreting Model Predictions", Advances in Neural Information Processing Systems (NeurIPS), 2017, pp. 4765-4774.

\bibitem{Bordt2022}
Bordt, S., Sachdev, S., von Luxburg, U., "Post-hoc Explanations Fail to Achieve the Goals of Transparency", arXiv:2206.10961, 2022.

\bibitem{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., et al., "Attention Is All You Need", Advances in Neural Information Processing Systems (NeurIPS), 2017.

\bibitem{Wachter2017}
Wachter, S., Mittelstadt, B., Russell, C., "Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR", Harvard Journal of Law \& Technology, 31(2), 2017.

\bibitem{Dhurandhar2018}
Dhurandhar, A., Chen, P.-Y., Luss, R., et al., "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives", Advances in Neural Information Processing Systems (NeurIPS), 2018.

\bibitem{Kim2018}
Kim, B., Wattenberg, M., Gilmer, J., et al., "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", Proceedings of ICML 2018.

\bibitem{Lerman2009}
Lerman, K., McDonald, R., "Contrastive Summarization: Highlighting Differences between Two Datasets", Proceedings of NAACL Workshop, 2009.

\bibitem{STRUM2024}
Günel, B., Saha, T., et al., "STRUM-LLM: Attributed and Structured Contrastive Summarization with LLMs", 2024.

\bibitem{ChatABSA2024}
Luo, Z., et al., "ChatABSA: Aspect-Based Sentiment Analysis via ChatGPT with Few-shot Prompts", arXiv:2401.00000, 2024.

\bibitem{Rudin2019}
Rudin, C., "Stop explaining black-box machine learning models for high-stakes decisions", Nature Machine Intelligence, 1(5), 2019, pp. 206--215.

\bibitem{Amann2020}
Amann, J., Blasimme, A., Vayena, E., Frey, D., Madai, V. I., "Explainability for artificial intelligence in healthcare: a multidisciplinary perspective", BMC Medical Informatics and Decision Making, 20, 2020, Article 310.

\bibitem{Sundararajan2017}
Sundararajan, M., Taly, A., Yan, Q., "Axiomatic Attribution for Deep Networks", Proceedings of ICML 2017, pp. 3319-3328.

\bibitem{Selvaraju2017}
Selvaraju, R. R., Cogswell, M., Das, A., et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization", Proceedings of ICCV 2017, pp. 618-626.

\bibitem{Adebayo2018}
Adebayo, J., Gilmer, J., Muelly, M., et al., "Sanity Checks for Saliency Maps", Advances in Neural Information Processing Systems (NeurIPS), 2018.

\bibitem{Slack2020}
Slack, D., Hilgard, S., Jia, E., Singh, S., Lakkaraju, H., "Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods", Proceedings of AIES 2020, pp. 180-186.

\bibitem{Wei2022Emergent}
Wei, J., Wang, X., Schuurmans, D., et al., "Emergent Abilities of Large Language Models", Transactions on Machine Learning Research (TMLR), 2022. arXiv:2206.07682.

\bibitem{OpenAI2023GPT4}
OpenAI, "GPT-4 Technical Report", arXiv:2303.08774, 2023.

\end{thebibliography}

\end{document}