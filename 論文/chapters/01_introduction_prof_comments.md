# 01_introduction.tex 教授コメントメモ

- 作成日: 2025-12-02
- 対象ファイル: `論文/chapters/01_introduction.tex`

---

## コメント1: 事後説明 vs CBM / Mechanistic の整理

**元コメント**

> ここはちょっと納得できませんでした．この文が言っているのは，説明の仕方の変化だけであって，「事後説明」であることには変わりないような言い回しになっています．この先の議論を読むと，(1)最初から解釈が容易なモデルを構築するアプローチ（CBMなど）と，(2)中間層のベクトルを別の解釈可能な（かつ出力が変化しないような）ベクトルに全て置き換えることで事後説明を試みるアプローチ（Mechanistic Interpretability）の2つが出てきます．ちょっと説明の整理を検討してもらえますでしょうか．
>
> @01_introduction.tex (10-11)

**対象箇所（抜粋）**

```tex
事後説明手法の限界を克服するため，近年の XAI 研究では，低レベルな特徴（どのピクセルやトークン ID が重要かなど）を用いた説明から，人間がそのまま理解できる概念（価格、サービスへの言及など）による説明へと移行している．コンセプトボトルネックモデル（CBM）~\cite{koh2020concept}や TCAV（Testing with Concept Activation Vectors）~\cite{kim2018interpretability}といったコンセプトベース XAI（C-XAI）は，モデル内部に解釈可能な中間層を導入し，モデルがどの概念にもとづいて予測を行ったのかを中間層の値の変動から読み取ることで，概念レベルで説明することを目指すアプローチである．
```

**理解メモ**

- 現状の文だと「説明の表現レベル（特徴→概念）の違い」のみが強調されており，「事後説明か／もともと解釈しやすいモデルか」の区別が曖昧。
- この後に出てくる2系統：
  - (1) **最初から解釈しやすいモデル構築**（例: CBM）
  - (2) **学習済みモデルの中間ベクトルを解釈可能ベクトルに写像する事後説明**（Mechanistic Interpretability）
- 導入部では，上記2つの立場を意識して整理した説明に書き換える必要がある。

---

## コメント2: Mechanistic Interpretability の範囲（回路だけでない）

**元コメント**

> Mechanistic Interpretabilityの研究で，本研究に近いものに， https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning があります．TransformerのMLP部分の解釈において，ニューロン単位ではsuperposition効果で解釈できないが，overcompleteなSparse Auto-Encoder (SAE) を用いて特徴抽出することにより，特定の言語の文字列，base64文字列，HTMLタグなどに反応する特徴が埋め込まれていることを明らかにしたものです．この論文のように，Mechanistic Interpretabilityの実際の研究範囲では回路の話は必ずしも出てこないので，ここの表現には少し違和感を感じました．こちらの文献も見てみて，少し表現を調整してもらうと良いと思います．
>
> @01_introduction.tex (14-16)

**対象箇所（抜粋）**

```tex
モデルの内部動作を回路レベルで明らかにしようとするメカニスティック解釈の研究においても，モデルからニューロン同士の結合構造や回路パターンを抽出したあとで，それらに人間が理解できる概念を与える作業は依然として手作業に頼っている．Anthropic による Attribution Graphs~\cite{anthropic2025biology}では，ある出力が計算されるまでにネットワーク内部でどのニューロンや重みがどのような順序で使われたかという計算の流れを，出力側からさかのぼって辿る．その際，各ニューロンや結合が最終出力にどの程度影響したかを数値化し，影響の大きい経路どうしを結んだグラフとして可視化することで，ニューロン間の回路構造を抽出している．
```

**理解メモ**

- 現在の書き方は「Mechanistic Interpretability = 回路構造の解析」という印象が強い。
- 一方で，Anthropic の *Towards Monosemanticity: Decomposing Language Models with Dictionary Learning* のように，
  - Transformer の MLP を overcomplete な Sparse Auto-Encoder (SAE) で分解し，
  - 特定言語の文字列・base64 文字列・HTML タグなどに反応する特徴を抽出する
 という「特徴分解・辞書学習ベース」の研究も Mechanistic Interpretability に含まれる。
- 導入部の表現としては，「回路レベルの解析」に限定せず，特徴分解・概念的な方向の研究も含むように表現を広げる／中立化する必要がある。

---

## コメント3: XAI 研究活発化へのサーベイ文献の追加

**元コメント**

> 説明可能 AI（Explainable AI，XAI）の研究が活発化してきた．
>
> →ここは論拠となる参考文献が欲しいと思います．XAIのサーベイ論文など，このような主張をしている文献を参照してみてもらえますか．
>
> @論文/chapters/01_introduction.tex

**対象箇所（抜粋）**

```tex
近年の深層学習モデルは，医療，金融，自動運転といった社会的重要性の高いドメインにおいて優れた予測性能を達成している．しかし，その内部はブラックボックスであり、深層学習モデルの意思決定プロセスが人間にとって解釈不能であるという点は、モデルの信頼性・公平性の担保において大きな問題として存在し続けている．この問題の解決に向けて，モデルの判断根拠を説明する説明可能 AI（Explainable AI，XAI）の研究が活発化してきた．
```

**理解メモ**

- 「XAI 研究が活発化してきた」という主張に対して，総説・サーベイ論文を根拠として引用する必要がある。
- XAI 全体の動向を俯瞰したサーベイ（例: post-hoc／intrinsic，グローバル／ローカルなどを整理している論文）を 1〜2 本探し，ここに引用を追加する方向で検討する。

---

## コメント4: 事後説明手法に対する批判の論拠（Rudin 論文）

**元コメント**

> この事後説明手法はいくつかの課題を抱えている．
>
> →この段落で指摘されている課題は，論拠を示した方がいいです（第二の課題には論拠が示されていますが）。
>
> kwakaba
>
> 2 December, 4:06 pm
>
> 例えばRudinの以下の論文は事後説明手法を批判していて，ここの論旨に合致しているかなと思いました．読んでみてください． https://www.nature.com/articles/s42256-019-0048-x

**対象箇所（抜粋）**

```tex
この事後説明手法はいくつかの課題を抱えている．第一に，事後説明手法が与える説明は，ブラックボックスモデル内部で実際に行われた判断の過程から判断根拠となる要素を直接抽出したものではなく，入力摂動に対する出力変化から，どの特徴が重要であったのかを外挿的に推測した結果である．このため，解析条件や近傍サンプリングの取り方をわずかに変えるだけで，重要と見なされる特徴量は容易に入れ替わり，事後説明の内容は大きく変動しうる．したがって，これらの手法が提供するものは，厳密な意味でのモデルの判断根拠とは言えない．
```

**理解メモ**

- 第一の課題（外挿的な推測であり，条件設定により説明が大きく変動しうる点）にも，批判的な先行研究を引用した方がよい。
- Rudin, C. (2019). "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead." *Nature Machine Intelligence* 1, 206–215. など，事後説明手法批判の代表的な論文を読み，ここでの議論と接続できる形で引用を追加する。