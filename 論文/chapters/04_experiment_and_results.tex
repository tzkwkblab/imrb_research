\chapter{評価実験と結果}

\section{実験の目的と概要}
本章では，提案する大規模言語モデル（LLM）を用いた対比因子ラベル自動生成手法の有効性と汎用性を定量的に検証した結果を報告する．
本実験は，モデル内部の特定のニューロン発火条件に対応する集合的な意味的差分を，LLM（GPT-4o-mini 等）によるコントラスティブ要約によって自然言語ラベルとして抽出可能であるか，また生成されたラベルが人手アノテーションによる正解ラベルとどの程度意味的に類似するかを，定量的指標および LLM による評価を用いて客観的に測定することを目的とする．

提案手法のドメイン汎用性を検証するために，レビュー，感情分類，画像キャプションという多様なドメインに属する 4 種類のデータセットを用いた．
これらのデータセットは，それぞれが持つ正解アスペクトラベルを，LLM が生成する対比因子ラベルの妥当性を評価するためのグラウンド・トゥルースとして使用した．

使用したデータセットの概要を表~\ref{tab:dataset_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{使用データセットの概要}
  \label{tab:dataset_overview}
  \begin{tabular}{lll}
    \toprule
    データセット & ドメイン & 特徴 \\
    \midrule
    SemEval-2014 ABSA & レビュー & 具体的アスペクト（Food, Service等） \\
    GoEmotions & 感情分類 & 抽象的概念（28感情カテゴリ） \\
    Steam Review Aspect Dataset & レビュー & 専門的アスペクト（Gameplay等） \\
    COCO Retrieved Concepts & 画像キャプション & 視覚的概念記述 \\
    \bottomrule
  \end{tabular}
\end{table}

本実験は，以下の 6 つの実験カテゴリで構成された．
メイン実験では，SemEval-2014 ABSA，GoEmotions，Steam Review Aspect Dataset の 3 データセットを用いて，提案手法の基本性能を評価した．
Few-shot 設定による性能比較実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．
グループサイズの影響分析実験では，group\_size を 50，100，150，200，300 の 5 段階で変化させた．
モデル比較実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを比較した．
アスペクト説明文の効果検証実験では，アスペクト説明文の有無による性能差を検証した．
COCO Retrieved Concepts 実験では，正解ラベルがない画像キャプションデータセットに対する対比因子生成を検証した．

各実験カテゴリの概要を表~\ref{tab:experiment_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{実験カテゴリの概要}
  \label{tab:experiment_overview}
  \begin{tabular}{ll}
    \toprule
    実験カテゴリ & 目的・検証内容 \\
    \midrule
    メイン実験 & 基本性能評価（3データセット） \\
    Few-shot実験 & 0/1/3-shot設定の比較 \\
    グループサイズ比較 & group\_size（50--300）の影響分析 \\
    モデル比較 & GPT-4o-mini vs GPT-5.1 \\
    アスペクト説明文比較 & 説明文の有無による性能差 \\
    COCO実験 & 正解ラベルなしデータセットでの検証 \\
    \bottomrule
  \end{tabular}
\end{table}

評価の観点として，以下の 3 つの指標を採用した（各指標の詳細な定義は\ref{subsec:evaluation_metrics}節を参照）．
有効性の評価には，BERTScore を主要指標として使用し，生成ラベルと正解ラベルの意味的類似度を測定した．
汎用性の評価には，複数のドメイン（レビュー，感情分類，画像キャプション）と複数のアスペクトタイプ（具体的アスペクト，抽象的概念）に対する性能を測定した．
性能の評価には，BLEU スコアを参考指標として使用し，語彙レベルの一致度を補助的に確認した．
また，LLM による意味的類似度評価を補助指標として採用し，GPT-4o-mini による 5 段階評価を実施した．

採用した評価指標の概要を表~\ref{tab:evaluation_metrics_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{評価指標の概要}
  \label{tab:evaluation_metrics_overview}
  \begin{tabular}{lll}
    \toprule
    評価指標 & 役割 & 位置づけ \\
    \midrule
    BERTScore & 意味的類似度測定 & 主要指標 \\
    BLEU & 語彙レベル一致度確認 & 参考指標 \\
    LLM評価 & 意味的類似度評価（5段階） & 補助指標 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{データセット}
\label{sec:dataset}

\subsection{データセットの選定理由}
本実験では，ドメインの多様性を確保するため，以下の 4 種類のデータセットを選定した．
SemEval-2014 ABSA は，アスペクトベース感情分析の標準的なベンチマークとして広く使用されており，正解ラベルが人手でアノテーションされている~\cite{pontiki-EtAl:2014:SemEval2014Task4}．
GoEmotions は，感情という抽象的な概念を扱うデータセットであり，具体的な物理的実体を持たない概念の命名精度を測るために選定した~\cite{demszky2020goemotions}．
Steam Review Aspect Dataset は，ゲームという特定の製品ドメインに特化したデータセットであり，専門性の高いテキスト集合間の意味的差分を抽出する能力を検証するために選定した~\cite{srec:steam-review-aspect-dataset}．
Retrieved Concepts（COCO Captions）は，画像キャプションデータセットであり，視覚的概念記述の生成能力を検証するために選定した~\cite{lin2014microsoft}．

ドメインの多様性として，レビューテキスト（SemEval-2014，Steam），感情分類テキスト（GoEmotions），画像キャプション（COCO）という異なるテキストタイプを網羅した．
また，具体的なアスペクト（Food，Price，Gameplay，Technical）と抽象的な概念（Atmosphere，Story，感情カテゴリ）の両方を扱うことで，概念の具体性による性能差を検証できるようにした．

\subsection{Steam Review Aspect Dataset}
Steam Review Aspect Dataset は，Steam ゲームレビューから収集されたテキストデータであり，特定のゲームアスペクトに関する言及を含むデータセットである~\cite{srec:steam-review-aspect-dataset}．

\begin{itemize}
  \item \textbf{概要}: 英語の Steam ゲームレビュー 1,100 件（学習用 900 件，テスト用 200 件）で構成される．
  データ収集は SRec データベースのスナップショットに基づき行われた．
  \item \textbf{アスペクト}: レビューを特徴づける 8 種類のアスペクトが人手でアノテーションされている．
  内訳は Recommended（推奨），Story（物語），Gameplay（ゲームプレイ），Visual（視覚），Audio（聴覚），Technical（技術），Price（価格），Suggestion（提案・要望）である．
  \item \textbf{特徴}: ゲームという特定の製品ドメインに特化しており，特に「Gameplay」や「Technical」といったゲーム固有のメカニクスや技術的側面に関するアスペクトを含む．
  テストセットにおけるアスペクト件数の例として，Gameplay が 154 件，Recommended が 148 件，Story が 89 件である．
  これにより，LLM が専門性の高いテキスト集合間の意味的差分を抽出する能力を検証するためのベンチマークとして機能する．
\end{itemize}

実験での使用方法として，各アスペクトについて，そのアスペクトを含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出した．
分割タイプは \texttt{aspect\_vs\_others} を採用し，特定のアスペクトが含まれるテキストと含まれないテキストを比較した．
グループ A と B の抽出は，各アスペクトのラベルに基づいて行い，group\_size パラメータに応じてサンプル数を調整した．
メイン実験では，group\_size = 100 を採用し，各グループから最大 100 件のテキストを抽出した．

% メイン実験で使用したアスペクト一覧は第\ref{sec:experiment_setup}節でまとめて述べる．

\subsection{SemEval-2014 ABSA（Restaurants）}
SemEval-2014 ABSA（Restaurants）は，アスペクトベース感情分析（ABSA）の標準的なベンチマークとして広く使用されるレストランレビューのデータセットである~\cite{pontiki-EtAl:2014:SemEval2014Task4}．

\begin{itemize}
  \item \textbf{概要}: レストランレビューのテキストを含み，各文に対してアスペクト（観点）とそれに対する感情極性が人手でアノテーションされている．
  \item \textbf{アスペクト}: 本研究では，主に Food（食べ物），Service（サービス），Price（価格），Atmosphere（雰囲気）の 4 種類のアスペクトを採用した．
  \item \textbf{特徴}: このデータセットは，LLM による自動命名の性能が概念の具体性に依存するかを検証するための鍵となるデータを含む．
  「Food」や「Price」は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトである一方，「Atmosphere」は広範な文脈や比喩的表現からの高度な推論を必要とする抽象的なアスペクトに分類される．
  本研究の主要な定量評価ベンチマークとして使用された．
\end{itemize}

実験での使用方法として，SemEval-2014 データセットにおいて，Restaurant ドメインから Food と Service，Laptop ドメインから Battery と Screen の 4 アスペクトを採用した．
各アスペクトについて，そのアスペクトを含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出し，split\_type = \texttt{aspect\_vs\_others} で分割した．

使用したアスペクトの選定理由として，Food は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトとして，Service，Battery，Screen は製品の属性に関する具体的なアスペクトとして選定した．
これにより，具体的なアスペクトにおける命名性能を評価できるようにした．

\subsection{GoEmotions}
GoEmotions は，細粒度感情分類タスクのために Reddit コメントから収集されたデータセットである~\cite{demszky2020goemotions}．

\begin{itemize}
  \item \textbf{概要}: Demszky らによって構築されたもので，総レコード数 63,812 件から成るマルチラベル形式のデータセットである．
  \item \textbf{アスペクト}: 28 の感情カテゴリ（27 感情 + neutral）でラベル付けされている．
  主要なカテゴリには Joy（喜び），Anger（怒り），Admiration（称賛），Neutral（中立）などが含まれる．
  \item \textbf{特徴}: 感情という極めて抽象的な概念を対比因子として扱えるかを検証するための挑戦的なデータセットとして位置づけられる．
  このデータセットでは，感情という内在的な状態をテキストの集合差分から推論する必要があり，具体的な物理的実体を持たない概念の命名精度を測るために使用された．
  実験では，任意の感情アスペクト（例：joy）を指定し，「その感情を含むテキスト群 $A$」と「その他のアスペクトを含むテキスト群 $B$」を比較する設定が採用された．
\end{itemize}

実験での使用方法として，GoEmotions データセットから全 28 感情カテゴリを採用した．
各感情カテゴリについて，その感情を含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出し，split\_type = \texttt{aspect\_vs\_others} で分割した．

28 感情カテゴリの選定理由として，感情は物理的な実体を持たない高度に抽象的な概念であり，具体的なアスペクトと比較して命名精度が低下する傾向があるかを検証するために，全カテゴリを対象とした．
これにより，抽象的な概念における命名性能を包括的に評価できるようにした．

\subsection{Retrieved Concepts（COCO Captions）}
Retrieved Concepts（COCO Captions）は，視覚的概念記述の生成能力を検証するために，画像キャプションデータセット COCO に基づいて構築されたデータセットである~\cite{lin2014microsoft}．
MS-COCO 2017 train split の画像に対して，実験協力者のFarnoosh Javar によって訓練された非教師ありコンセプト発見モデル（personal communication）から得られた 300 個の潜在コンセプト埋め込みと，CLIP（ViT-B/32）による画像埋め込みとのコサイン類似度を計算し，各コンセプトについて類似度が高い画像 Top-100 と低い画像 Bottom-100 を取得している．この潜在コンセプト埋め込みは，UCBM~\cite{schrodi2024unsupervised} に代表される辞書学習型の非教師ありコンセプト発見手法と同様に，既存モデルの中間表現から自動的に概念ベクトルを抽出するタイプのモデルにより学習されているが，本論文では上流モデルの詳細には立ち入らず，得られた concept embeddings と，それに対して CLIP 類似度にもとづき取得された Top/Bottom 画像およびその COCO 由来キャプションのみを利用する．
各画像には COCO 由来の人手キャプションが 5 つ付与されており，本研究ではこれらのキャプション集合のみをテキストデータとして利用する．

\begin{itemize}
  \item \textbf{概要}: 非教師ありに学習された 300 の潜在コンセプト（concept\_0 ～ concept\_299）に対し，CLIP 類似度にもとづき取得された Top-100/Bottom-100 画像とそのキャプションからなるデータセットである．
  \item \textbf{特徴}: 各コンセプトに対して人手の正解ラベル（アスペクト名）は与えられておらず，潜在コンセプトとその Top/Bottom 例のみが提供される．
  これにより，SemEval などのラベル付きデータセットと比べて，「非教師ありコンセプト抽出＋命名」という本研究の理想的な問題設定により近い状況で，対比因子ラベル生成を検証できる．
  LLM がテキストの集合差分から視覚的な特徴を抽象化した概念記述を生成できるかを確認するために使用され，本手法が将来的に画像モデルのニューロン解釈（例：縞模様，空の色といった視覚的概念）に適用可能であるかを探るための基礎的なデータを提供する．
\end{itemize}

実験では，300 コンセプトのうち concept\_0，concept\_1，concept\_2，concept\_10，concept\_50 の 5 コンセプトを採用した．
各コンセプトについて，潜在コンセプト埋め込みと画像埋め込みとの CLIP（ViT-B/32）コサイン類似度に基づき，類似度が高い順に Top-100，低い順に Bottom-100 の画像を選び，それらに付与されたキャプションをグループ A（Top 側），グループ B（Bottom 側）として用いた．
分割タイプは \texttt{aspect\_vs\_bottom100} を採用した．

正解ラベルが存在しないため，BERTScore と BLEU スコアは参考値として記録するにとどめ，主に生成された対比因子と対応する画像群との整合性に基づく定性的評価を行った．

\section{実験設定}
\label{sec:experiment_setup}

\subsection{実験パイプラインの概要}
本実験では，コントラスティブ要約に基づく対比因子ラベル生成器として，GPT-4o-mini を含む複数の大規模言語モデルを採用した．
提案手法は，統一されたパイプラインとして構築され，その目的は，特定の概念に対応するテキスト集合 $A$（グループA）と，そうでないテキスト集合 $B$（グループB）の差分から，意味的な対比因子ラベル $L$ を LLM に生成させることである．理想的なタスク定義では，$A$ と $B$ はニューロン活性値に基づき構成されるが，本章の実験では，第\ref{sec:dataset}節で述べたように，SemEval，GoEmotions，Steam では人手アスペクトラベルに基づき，COCO Retrieved Concepts では潜在コンセプトと CLIP 類似度により定義された Top/Bottom 画像群に付与されたキャプションにもとづいて構成したグループA/Bを，このタスクの近似として用いる．

\begin{itemize}
  \item \textbf{タスク定式化}: 第3章で定義したとおり，集合 $A$ と $B$ の差分から自然言語ラベル $L$ を生成する写像 $\text{textContrastiveNaming}(A,B) \to L$ を用いる．理想的には，$A$ と $B$ はニューロン $N$ の活性化度に基づいて構成されるが，本章の実験では，各データセットごとに定義されたグルーピング規則にもとづき構成したグループA/Bをその具体例として用いる．
  \item \textbf{グルーピング}: 各データセットごとに定義された規則に従い，ハイパーパラメータ $group\_size$ を用いて集合 $A$ と $B$ を抽出する．SemEval，GoEmotions，Steam ではアスペクトラベルの有無にもとづき，COCO Retrieved Concepts では各潜在コンセプトに対する Top-100/Bottom-100 画像のキャプションにもとづいてグループA/Bを構成する．
  メイン実験では，プロンプトのコンテキスト長制限を考慮し，$group\_size = 100$ を採用した．
  \item \textbf{Few-shot ICL の検証}: LLM の出力形式の揺らぎや語彙の安定性を確保するための検証手段として，Few-shot インコンテキスト・ラーニング（ICL）のバリエーション（0-shot, 1-shot, 3-shot）を定量的に検証した．
  LLM は，プロンプト内でグループAとグループBを比較し，グループAに特徴的でグループBには欠如している意味的側面を推論するよう指示された．
\end{itemize}

実験パイプラインの全体フローを以下に示す．

\begin{enumerate}
  \item \textbf{データセット読み込み}：各データセットからテキストを読み込む．
\item \textbf{グループA/B抽出}：各データセットで定義されたグルーピング規則（アスペクトラベル，あるいは Retrieved Concepts における Top/Bottom 構造）に基づき，グループA（特定概念を含むテキスト群）とグループB（含まないテキスト群）を抽出する．
  \item \textbf{プロンプト生成}：グループAとグループBのテキストリストをプロンプトに組み込む．Few-shot例が設定されている場合は，プロンプトにFew-shot例を挿入する（オプション）．
  \item \textbf{LLM生成}：GPT-4o-mini等のLLMにプロンプトを入力し，対比因子ラベルを生成する．
  \item \textbf{評価}：生成されたラベルと正解ラベルの意味的類似度をBERTScore，BLEU，LLM評価により測定する．
\end{enumerate}

メイン実験で対象としたアスペクトは，SemEval-2014 から Food，Service，Battery，Screen の 4 種類，GoEmotions から全 28 感情カテゴリ，Steam から Gameplay，Visual，Story，Audio の 4 種類である．

\subsection{LLMモデルとパラメータ設定}
本実験では，対比因子ラベル生成器として GPT-4o-mini を主要モデルとして採用した．
モデル比較実験では，GPT-5.1 も使用した．
GPT-4o-mini を選択した理由は，コスト効率が高く，かつ十分な性能を発揮することが既存研究で確認されているためである．

各実験カテゴリでのモデル選択として，メイン実験，Few-shot 実験，グループサイズ比較実験，アスペクト説明文比較実験では GPT-4o-mini を統一して使用した．
モデル比較実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを比較した．
COCO Retrieved Concepts 実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを使用した．

温度パラメータ（temperature）の設定として，メイン実験では temperature = 0.0 を採用した．
この設定により，決定論的な出力が得られ，実験の再現性が確保される．
Few-shot 実験，グループサイズ比較実験，モデル比較実験，アスペクト説明文比較実験でも temperature = 0.0 を採用した．
COCO Retrieved Concepts 実験でも temperature = 0.0 を採用した．

最大トークン数（max\_tokens）の設定として，メイン実験では max\_tokens = 2000 に設定した．
Few-shot 実験，モデル比較実験では max\_tokens = 100 に設定した．
グループサイズ比較実験，アスペクト説明文比較実験，COCO Retrieved Concepts 実験では max\_tokens = 2000 に設定した．

その他の生成パラメータとして，top\_p や frequency\_penalty はデフォルト値を使用した．

\subsection{プロンプト設計}
プロンプトテンプレートは，以下の構造を持つ．
まず，タスクの説明として「2つのデータグループを比較して，グループAに特徴的でグループBには見られない表現パターンや内容の特徴を特定してください」という指示を提示する．
次に，Few-shot 例が存在する場合は \texttt{examples\_section} に挿入される．
Few-shot 例の形式は「【例題$N$】グループA: [...] グループB: [...] 回答: [正解ラベル]」である．
その後，実際のデータとして【グループA】と【グループB】のテキストリストが提示される．
各テキストは「- [テキスト内容]」の形式で列挙され，コンテキスト長制限を考慮して最大 100 件に制限される．
最後に，出力形式の指示として「英語で5-10単語程度で，グループAに特徴的でグループBには見られない主要な違いを簡潔に回答してください」が追加される．

グループA/Bの提示方法として，各テキストは「- [テキスト内容]」の形式で列挙され，グループAとグループBはそれぞれ【グループA】と【グループB】の見出しで区別された．
コンテキスト長制限を考慮し，各グループから最大 100 件のテキストを抽出した．

出力形式の指示方法として，「英語で5-10単語程度で，グループAに特徴的でグループBには見られない主要な違いを簡潔に回答してください」という指示をプロンプトの末尾に追加した．

Few-shot 例の挿入方法として，Few-shot 設定が 0 より大きい場合，プロンプトのタスク説明の後に \texttt{examples\_section} を挿入した．
Few-shot 例の形式は「【例題$N$】グループA: [...] グループB: [...] 回答: [正解ラベル]」であり，$N$ は例題番号である．

アスペクト説明文の使用方法として，アスペクト説明文比較実験では，アスペクトの説明文をプロンプトの冒頭に追加した．
例えば，「Food」アスペクトの場合，「Food refers to mentions of food quality, taste, menu items, or dining experience」といった説明文を挿入した．
説明文ありの条件と説明文なしの条件を比較することで，アスペクト説明文の効果を検証した．

\subsection{データの前処理と分割方法}
テキストの前処理手順として，各データセットからテキストを読み込み，アスペクトラベルに基づいてグループ A とグループ B に分割した．
テキストの前処理として，特殊文字の処理や正規化は行わず，データセットの生のテキストをそのまま使用した．

グループA/Bの抽出方法として，各アスペクトについて，そのアスペクトを含むテキスト群をグループA，含まないテキスト群をグループBとして抽出した．
分割タイプは \texttt{aspect\_vs\_others} を採用し，特定のアスペクトが含まれるテキストと含まれないテキストを比較した．
COCO Retrieved Concepts 実験では，分割タイプとして \texttt{aspect\_vs\_bottom100} を採用し，Top-100のキャプションをグループA，Bottom-100のキャプションをグループBとして抽出した．

グループサイズ（group\_size）の決定方法として，メイン実験では group\_size = 100 を採用した．
この値は，プロンプトのコンテキスト長制限を考慮して決定された．
グループサイズ比較実験では，group\_size を 50，100，150，200，300 の 5 段階で変化させた．

サンプリング方法として，各グループから指定された group\_size 件数のテキストを先頭から順序的に抽出した．
ランダムサンプリングや重み付きサンプリングは使用しなかった．

コンテキスト長制限への対応として，各グループから最大 100 件のテキストを抽出し，プロンプトのコンテキスト長を制限内に収めた．
メイン実験では group\_size = 100 を採用し，グループサイズ比較実験では最大 300 まで検証したが，コンテキスト長超過エラーを回避するため，実際のプロンプトでは必要に応じてテキスト数を制限した．

\subsection{Few-shot例の作成方法}
Few-shot例の選定基準として，各データセットのアスペクトラベルに基づき，正解ラベルが明確な例を選定した．
Few-shot例は，グループAとグループBのテキストリストと，それに対応する正解ラベルで構成された．

例の品質管理方法として，Few-shot 例は，各データセットのアスペクトラベルに基づいて作成し，正解ラベルが明確であることを確認した．
Few-shot 例の形式は「【例題$N$】グループA: [...] グループB: [...] 回答: [正解ラベル]」であり，$N$ は例題番号である．

0-shot，1-shot，3-shot の違いと設定方法として，0-shot 設定では Few-shot 例を挿入せず，タスク説明のみを提示した．
1-shot 設定では，1 つの Few-shot 例を \texttt{examples\_section} に挿入した．
3-shot 設定では，3 つの Few-shot 例を \texttt{examples\_section} に挿入した．
Few-shot 実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．

\subsection{実験カテゴリの定義}
各実験カテゴリのパラメータ設定を表~\ref{tab:experiment_config}に，実験成功/失敗数のサマリーを表~\ref{tab:experiment_summary}に示す．

\begin{table}[htbp]
  \centering
  \caption{実験カテゴリ別パラメータ設定}
  \label{tab:experiment_config}
  \begin{tabular}{lllllll}
    \toprule
    実験カテゴリ & temperature & max\_tokens & few\_shot & group\_size & GPTモデル & LLM評価 \\
    \midrule
    メイン実験 & 0.0 & 2000 & 0 & 100 & gpt-4o-mini & 有効 \\
    Few-shot実験 & 0.0 & 100 & 0-3 & 100 & gpt-4o-mini & 有効 \\
    グループサイズ比較 & 0.0 & 100 & 0 & 50-300 & gpt-4o-mini & 有効 \\
    モデル比較 & 0.0 & 100 & 0 & 100 & gpt-4o-mini/gpt-5.1 & 有効 \\
    アスペクト説明文比較 & 0.0 & 2000 & 0 & 100 & gpt-4o & 有効 \\
    COCO実験 & 0.0 & 2000 & 0 & 100 & gpt-4o-mini & 無効 \\
    \bottomrule
  \end{tabular}
\end{table}

BERTScoreに対しFriedman検定を行った結果、p=0.4724で0/1/3-shot間に有意差は確認されなかった（Holm補正付き事後比較も全組み合わせ非有意）。

\begin{table}[htbp]
  \centering
  \caption{実験成功/失敗数のサマリー}
  \label{tab:experiment_summary}
  \begin{tabular}{lccc}
    \toprule
    実験カテゴリ & 総実験数 & 成功数 & 失敗数 \\
    \midrule
    メイン実験 & 36 & 36 & 0 \\
    Few-shot実験 & 12 & 12 & 0 \\
    グループサイズ比較 & 20 & 20 & 0 \\
    モデル比較 & 8 & 8 & 0 \\
    アスペクト説明文比較 & 8 & 8 & 0 \\
    COCO実験 & 5 & 5 & 0 \\
    \midrule
    合計 & 89 & 89 & 0 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{比較手法とベンチマーク}
本研究は，非教師ありコンセプト抽出（UCBM や CCE など）が発見した潜在ベクトルに名前を付与する「命名モジュール」としての機能に特化している~\cite{schrodi2024unsupervised,stein2024towards}．
そのため，生成されたラベルの品質を，人手アノテーションされた既存の ABSA ベンチマーク（SemEval-2014 Restaurant/Laptop，Steam レビューなど）の正解ラベルとの意味的類似性と比較することで評価した．
このアプローチにより，本手法の命名性能が，高コストな人手ラベリングによって確立された基準に対してどの程度妥当であるかを検証した．

\section{評価指標}
\label{sec:evaluation_metrics}
生成された自然言語ラベル $L$ の品質を評価するために，BERTScore，BLEU，および LLM による意味的類似度評価を採用した．

\subsection{評価指標の選定理由}
\label{subsec:evaluation_metrics}
本実験では，BERTScore，BLEU，LLM 評価の 3 つの指標を採用した．
BERTScore を選んだ理由は，生成ラベルと正解ラベルの意味的類似度を測定するためである．
本タスクでは，LLM が生成するラベルが「食べ物の品質に関する言及」のような説明的なフレーズとなるのに対し，正解ラベルは「food」のような単一の単語であるため，語彙レベルの一致度を超えたセマンティックな評価が必要である．
BLEU を選んだ理由は，語彙レベルの一致度を補助的に確認するためである．
LLM 評価を選んだ理由は，LLM による意味的類似度評価を補助指標として採用するためである．

各指標の役割と位置づけとして，BERTScore は主要指標として位置づけられ，生成ラベルと正解ラベルの意味的類似度を測定した．
BLEU は参考指標として位置づけられ，語彙レベルの一致度を補助的に確認した．
LLM 評価は補助指標として位置づけられ，GPT-4o-mini による 5 段階評価を実施した．

\subsection{BERTScore}
\begin{itemize}
  \item \textbf{定義と役割}: BERTScore は，生成されたラベル $L$ と人手アノテーションされた正解ラベル $L_{\mathrm{ref}}$（またはその説明文）との間の文脈的意味的な類似性を測る主要な指標として採用された \mbox{[1, 2, 17, 37]}．
  この指標は，BERT などの事前学習済み言語モデルによって得られる文脈化埋め込み表現のコサイン類似度に基づき，語彙レベルの一致度を超えたセマンティックな評価を提供する．
  \item \textbf{位置づけ}: 本タスクにおいては，LLM が集合差分という複雑な推論タスクの結果を要約した自然言語フレーズを生成するため，意味的な妥当性を定量的に示す BERTScore が最も重要な評価基準として位置づけられた．
\end{itemize}

実装上は，SentenceTransformer（'all-MiniLM-L6-v2'）~\cite{reimers2019sentence}による埋め込みのコサイン類似度を 0.0〜1.0 に正規化した値を用いた．
値が 1.0 に近いほど意味的に類似していることを示す．

\subsection{BLEU（Bilingual Evaluation Understudy）}
\begin{itemize}
  \item \textbf{定義と役割}: BLEU は，生成ラベルと正解ラベルとの間の語彙レベルの一致度（n-gram の重複）を確認するために補助的に使用された．
  \item \textbf{本タスクにおける制約}: 本タスクの性質上，BLEU スコアは極めて低い値を示すことが前提とされた．
  これは，正解ラベルが「food」「price」のような単一の言葉または簡潔なフレーズであるのに対し，
  LLM が生成するラベルはしばしば「食べ物の品質に関する言及」「価格設定の側面」といった説明的なフレーズとなるため，語彙的な重複（n-gram overlap）が本質的に生じにくいためである．
  したがって，BLEU スコアの低さはモデルの命名失敗を意味するものではなく，単に語彙的一致度を測る指標が本タスクの性質に適合していないことを示す参考値として扱われた．
\end{itemize}

計算方法の詳細として，BLEU スコアは NLTK~\cite{bird2009natural} の sentence\_bleu を使用し，SmoothingFunction.method1 を適用した．
評価範囲は 0.0 から 1.0 であり，1.0 に近いほど一致度が高いことを示す．

\subsection{LLM評価スコア}
LLM 評価の目的と位置づけとして，LLM による意味的類似度評価を補助指標として採用した．
評価に使用するモデルとして，メイン実験，Few-shot 実験，グループサイズ比較実験では GPT-4o-mini を使用し，temperature = 0.0 に設定した．
モデル比較実験，アスペクト説明文比較実験では GPT-4o を使用し，temperature = 0.0 に設定した．
COCO Retrieved Concepts 実験では LLM 評価を無効化した．

評価プロンプトの設計として，以下のプロンプトを使用した．
「参照テキストと候補テキストの意味的類似度を5段階（1-5）で評価してください．
参照テキスト: \{reference\_text\}
候補テキスト: \{candidate\_text\}
評価基準:
- 5: 完全に同じ意味
- 4: ほぼ同じ意味（細かい違いのみ）
- 3: 類似しているが一部異なる
- 2: 部分的に類似している
- 1: ほとんど異なる
出力形式（JSON形式）:
\{
    "score": 4,
    "normalized\_score": 0.8,
    "reasoning": "評価理由を簡潔に説明"
\}」

評価基準（5段階評価の詳細）として，1 から 5 の整数で評価し，5 が最も類似度が高く，1 が最も類似度が低い．
正規化方法として，5 段階評価（1-5）を 0.0-1.0 に正規化し，normalized\_score = (score - 1) / 4 として計算した．

BERTScore との関係として，LLM 評価スコアは BERTScore を補完する補助指標として位置づけられ，両指標を併用することで生成ラベルの品質を多角的に評価した．

\section{実験結果}

\subsection{メイン実験結果}
実験設定の詳細（パラメータ一覧）として，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0），アスペクト記述 = 無効とした．

総実験数と成功/失敗数として，総実験数は 36 実験であり，成功数は 36 実験，失敗数は 0 実験であった．

データセット別の結果を表~\ref{tab:main_dataset_results}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：データセット別評価スコア}
  \label{tab:main_dataset_results}
  \begin{tabular}{lccc}
    \toprule
    データセット & BERTScore & BLEU & LLM \\
    \midrule
    SemEval-2014 & & & \\
    \quad 平均 & 0.7531 & 0.0220 & 0.5500 \\
    \quad 最小 & 0.7181 & 0.0123 & 0.4000 \\
    \quad 最大 & 0.8012 & 0.0278 & 0.6000 \\
    \midrule
    GoEmotions & & & \\
    \quad 平均 & 0.7127 & 0.0073 & 0.4714 \\
    \quad 最小 & 0.5437 & 0.0 & 0.2 \\
    \quad 最大 & 0.8941 & 0.0408 & 0.8 \\
    \midrule
    Steam & & & \\
    \quad 平均 & 0.5403 & 0.0 & 0.3 \\
    \quad 最小 & 0.5164 & 0.0 & 0.2 \\
    \quad 最大 & 0.5612 & 0.0 & 0.6 \\
    \bottomrule
  \end{tabular}
\end{table}

アスペクト別の結果（主要なアスペクト）を表~\ref{tab:main_aspect_results}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：主要アスペクト別評価スコア}
  \label{tab:main_aspect_results}
  \begin{tabular}{lcccc}
    \toprule
    データセット & アスペクト & BERTScore & BLEU & LLM \\
    \midrule
    \multirow{4}{*}{SemEval-2014} & Food & 0.7286 & 0.0123 & 0.4000 \\
    & Service & 0.7181 & 0.0240 & 0.6000 \\
    & Battery & 0.7646 & 0.0278 & 0.6000 \\
    & Screen & 0.8012 & 0.0240 & 0.6000 \\
    \midrule
    \multirow{4}{*}{GoEmotions} & Joy & 0.8192 & 0.0000 & 0.8000 \\
    & Anger & 0.7214 & 0.0000 & 0.6000 \\
    & Disgust & 0.8316 & 0.0240 & 0.8000 \\
    & Embarrassment & 0.8941 & 0.0408 & 0.8000 \\
    \midrule
    \multirow{4}{*}{Steam} & Gameplay & 0.5612 & 0.0000 & 0.2000 \\
    & Visual & 0.5164 & 0.0000 & 0.2000 \\
    & Story & 0.5383 & 0.0000 & 0.6000 \\
    & Audio & 0.5452 & 0.0000 & 0.2000 \\
    \bottomrule
  \end{tabular}
\end{table}

全体の統計を表~\ref{tab:main_overall_stats}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：全体統計}
  \label{tab:main_overall_stats}
  \begin{tabular}{lccc}
    \toprule
    評価指標 & 平均 & 最小 & 最大 \\
    \midrule
    BERTScore & 0.6980 & 0.5164 & 0.8941 \\
    BLEU & 0.0082 & 0.0000 & 0.0408 \\
    LLM & 0.4611 & 0.2000 & 0.8000 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{主要実験結果：自動命名の定量分析}
SemEval-2014 データセット（レストランレビュー）におけるメイン実験の結果として，BERTScore の平均は 0.7531，最小は 0.7181，最大は 0.8012 を記録した．
BLEU スコアの平均は 0.0220，最小は 0.0123，最大は 0.0278 を記録した．
LLM スコアの平均は 0.5500，最小は 0.4000，最大は 0.6000 を記録した．

Few-shot 実験における結果として，0-shot 設定では BERTScore の平均は 0.5526，1-shot 設定では 0.6530，3-shot 設定では 0.5754 を記録した．
1-shot 設定が他の設定と比較して最も高い BERTScore を記録した．

BLEU スコアの傾向として，Few-shot 実験では全ての Few-shot 設定において BLEU スコアの平均は 0.0 を記録した．
メイン実験では BLEU スコアの平均は 0.0082，最小は 0.0000，最大は 0.0408 を記録した．

\subsection{Few-shot設定による性能比較}
実験設定として，Steam データセットを用いて，Few-shot 設定（0-shot，1-shot，3-shot）による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 100，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 12 実験（4 アスペクト × 3 Few-shot 設定）であり，全実験が成功した．

Few-shot 別の平均スコアを表~\ref{tab:fewshot_summary}に，アスペクト別の Few-shot 効果を表~\ref{tab:fewshot_aspect}に示す．
\begin{table}[htbp]
  \centering
  \caption{Few-shot設定による性能比較：平均スコア}
  \label{tab:fewshot_summary}
  \begin{tabular}{lccc}
    \toprule
    Few-shot設定 & BERTScore & BLEU & LLM \\
    \midrule
    0-shot & & & \\
    \quad 平均 & 0.5526 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5462 & 0.0 & 0.2 \\
    \quad 最大 & 0.5596 & 0.0 & 0.4 \\
    \midrule
    1-shot & & & \\
    \quad 平均 & 0.6530 & 0.0 & 0.3500 \\
    \quad 最小 & 0.5111 & 0.0 & 0.2 \\
    \quad 最大 & 0.8356 & 0.0 & 0.8 \\
    \midrule
    3-shot & & & \\
    \quad 平均 & 0.5754 & 0.0 & 0.4000 \\
    \quad 最小 & 0.5416 & 0.0 & 0.2 \\
    \quad 最大 & 0.6449 & 0.0 & 0.6 \\
    \bottomrule
  \end{tabular}
\end{table}

BERTScoreについてFriedman検定を実施し，$p=0.3309$で$group\_size$ 50--300間に有意差は確認されなかった（Holm補正付き事後比較も全て非有意）。

\begin{table}[htbp]
  \centering
  \caption{Few-shot設定による性能比較：アスペクト別BERTScore}
  \label{tab:fewshot_aspect}
  \begin{tabular}{lccc}
    \toprule
    アスペクト & 0-shot & 1-shot & 3-shot \\
    \midrule
    Gameplay & 0.5462 & 0.6802 & 0.5644 \\
    Visual & 0.5562 & 0.5111 & 0.6449 \\
    Story & 0.5483 & 0.8356 & 0.5416 \\
    Audio & 0.5596 & 0.5850 & 0.5505 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{グループサイズの影響分析}
実験設定として，Steam データセットを用いて，group\_size（50，100，150，200，300）による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 20 実験（4 アスペクト × 5 group\_size）であり，全実験が成功した．

グループサイズ別の性能を表~\ref{tab:groupsize_results}に示す．
Friedman 検定では p=0.3309 となり，Holm 補正後の全ペア比較も含め有意差は確認されなかった．
\begin{table}[htbp]
  \centering
  \caption{グループサイズによる性能比較（各group\_sizeでの全アスペクト平均値）}
  \label{tab:groupsize_results}
  \begin{tabular}{lccc}
    \toprule
    group\_size & BERTScore & BLEU & LLM \\
    \midrule
    50 & 0.5489 & 0.0 & 0.2 \\
    100 & 0.5514 & 0.0 & 0.2 \\
    150 & 0.5406 & 0.0 & 0.2 \\
    200 & 0.5487 & 0.0 & 0.2 \\
    300 & 0.5603 & 0.0 & 0.2 \\
    \bottomrule
  \end{tabular}
\end{table}

全体の統計を表~\ref{tab:groupsize_overall_stats}に示す．
\begin{table}[htbp]
  \centering
  \caption{グループサイズ比較実験：全体統計}
  \label{tab:groupsize_overall_stats}
  \begin{tabular}{lccc}
    \toprule
    評価指標 & 平均 & 最小 & 最大 \\
    \midrule
    BERTScore & 0.5396 & 0.5019 & 0.5636 \\
    BLEU & 0.0000 & 0.0000 & 0.0000 \\
    LLM & 0.2800 & 0.2000 & 0.6000 \\
    \bottomrule
  \end{tabular}
\end{table}

コンテキスト長との関係として，group\_size が大きくなるほど，プロンプトのコンテキスト長が増加する．
メイン実験では group\_size = 100 を採用し，コンテキスト長制限を考慮して各グループから最大 100 件のテキストを抽出した．

\subsection{モデル比較実験}
GPT-4o-mini vs GPT-5.1 の比較として，Steam データセットを用いて，2 モデルによる性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 100，few\_shot = 0，group\_size = 100，LLM 評価 = 有効（gpt-4o，temperature = 0.0）とした．
総実験数は 8 実験（4 アスペクト × 2 モデル）であり，全実験が成功した．

モデル別の性能差を表~\ref{tab:model_comparison_summary}に，アスペクト別の性能差を表~\ref{tab:model_comparison_aspect}に示す．
対応のある Wilcoxon 検定では p=0.8750（中央値差 -0.0158，Holm 補正後も非有意）となり，統計的には差は確認されなかった．
\begin{table}[htbp]
  \centering
  \caption{モデル比較実験：平均スコア}
  \label{tab:model_comparison_summary}
  \begin{tabular}{lccc}
    \toprule
    モデル & BERTScore & BLEU & LLM \\
    \midrule
    GPT-4o-mini & & & \\
    \quad 平均 & 0.5453 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5214 & 0.0 & 0.2 \\
    \quad 最大 & 0.5600 & 0.0 & 0.4 \\
    \midrule
    GPT-5.1 & & & \\
    \quad 平均 & 0.5375 & 0.0 & 0.2500 \\
    \quad 最小 & 0.5167 & 0.0 & 0.2 \\
    \quad 最大 & 0.5621 & 0.0 & 0.4 \\
    \bottomrule
  \end{tabular}
\end{table}

BERTScoreに対する対応のあるWilcoxon検定ではp=0.8750で、GPT-4o-miniとGPT-5.1の差は有意ではなかった（中央値差 5.1−4o-mini = -0.0158）。

\begin{table}[htbp]
  \centering
  \caption{モデル比較実験：アスペクト別BERTScore}
  \label{tab:model_comparison_aspect}
  \begin{tabular}{lcc}
    \toprule
    アスペクト & GPT-4o-mini & GPT-5.1 \\
    \midrule
    Gameplay & 0.5600 & 0.5423 \\
    Visual & 0.5425 & 0.5287 \\
    Story & 0.5573 & 0.5167 \\
    Audio & 0.5214 & 0.5621 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{アスペクト説明文の効果検証}
実験設定として，Steam データセットを用いて，アスペクト説明文の有無による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 8 実験（4 アスペクト × 2 条件（説明文あり/なし））であり，全実験が成功した．

説明文の有無による性能差を表~\ref{tab:aspect_desc_summary}に，アスペクト別の効果を表~\ref{tab:aspect_desc_aspect}に示す．
対応のある Wilcoxon 検定では p=0.3750（Holm 補正後も非有意）となり，性能差は傾向レベルであった．
\begin{table}[htbp]
  \centering
  \caption{アスペクト説明文の効果検証：平均スコア}
  \label{tab:aspect_desc_summary}
  \begin{tabular}{lccc}
    \toprule
    条件 & BERTScore & BLEU & LLM \\
    \midrule
    説明文なし & & & \\
    \quad 平均 & 0.5395 & 0.0 & 0.2500 \\
    \quad 最小 & 0.5311 & 0.0 & 0.2 \\
    \quad 最大 & 0.5544 & 0.0 & 0.4 \\
    \midrule
    説明文あり & & & \\
    \quad 平均 & 0.5496 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5186 & 0.0 & 0.2 \\
    \quad 最大 & 0.5810 & 0.0 & 0.4 \\
    \bottomrule
  \end{tabular}
\end{table}

BERTScoreで対応のあるWilcoxon検定を行い，$p=0.3750$で説明文あり/なしの差は有意ではなかった（中央値差 $with\_desc - no\_desc = 0.0131$）。

\begin{table}[htbp]
  \centering
  \caption{アスペクト説明文の効果検証：アスペクト別BERTScore}
  \label{tab:aspect_desc_aspect}
  \begin{tabular}{lcc}
    \toprule
    アスペクト & 説明文なし & 説明文あり \\
    \midrule
    Gameplay & 0.5335 & 0.5523 \\
    Visual & 0.5311 & 0.5186 \\
    Story & 0.5392 & 0.5467 \\
    Audio & 0.5544 & 0.5810 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{COCO Retrieved Concepts実験}
実験設定として，COCO Retrieved Concepts データセットを用いて，正解ラベルがない画像キャプションデータセットに対する対比因子生成を検証した．本データセットは，各画像に対して COCO データセット由来の複数のキャプションが付与されている一方で，画像をどのような基準でクラスタリングすべきかという「正解となるクラスタラベル」は与えられていない．本データセットは，300個の潜在概念（concept embeddings）が学習されたモデルから生成されており，各概念について，CLIP（ViT-B/32）を用いて計算した画像埋め込みとのコサイン類似度に基づいて，MS-COCO 2017 train splitから画像を取得している．具体的には，各概念に対して最も類似度が高い100枚の画像を Top-100，最も類似度が低い100枚の画像を Bottom-100 として抽出している．このとき，画像は各概念に対する類似度に基づいて Top-100 と Bottom-100 に分割されるものの，その分割がどのような視覚的アスペクト（どのような概念・特徴の「発火／非発火」に対応するか）に基づいて行われたのかは明示的にはわからない，という状況を想定している．

この分割結果に対して，Top-100をグループA（類似度が高い＝発火側），Bottom-100をグループB（類似度が低い＝非発火側）として抽出した．各グループについて，画像ごとに付与された COCO キャプション群を用いて対比因子生成を行うことで，「各概念がどのような基準（どのような視覚的アスペクト）によって Top-100 と Bottom-100 に分割されているのか」を自然言語の対比因子として汲み取れるかを検証することを狙いとした．これは，発火・非発火によって分かれたサンプル集合に対してアスペクトを付与する操作に相当し，LLM のニューロン発火条件の解明という本来のタスクにより近い状況で，提案手法の有効性を確認する試みである．

実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 無効とした．総実験数は 5 実験（5 コンセプト）であり，全実験が成功した．正解ラベルがないため，BERTScore と BLEU スコアは参考値として記録するにとどめ，評価指標としては用いなかった．代わりに，生成された対比因子と，各コンセプトに対応する画像群とを見比べることで，対比因子が画像の視覚的特徴を適切に記述しているか，すなわち各概念に対する類似度ランキングの分割基準を妥当な形で言語化できているかを確認した．

視覚的概念記述の生成能力の検証結果を表~\ref{tab:coco_results}に示す．
各コンセプトについて，生成された対比因子と画像との整合性を確認するため，代表的な画像を図~\ref{fig:coco_concept0}--\ref{fig:coco_concept50}に示す．

\begin{table}[htbp]
  \centering
  \caption{COCO Retrieved Concepts実験：評価スコア}
  \label{tab:coco_results}
  \begin{tabular}{lcc}
    \toprule
    評価指標 & 平均 & 範囲 \\
    \midrule
    BERTScore & 0.6173 & 0.5714--0.6537 \\
    BLEU & 0.0000 & 0.0000--0.0000 \\
    LLM & --- & （無効） \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 500 332]{image/coco/concept_0_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 415]{image/coco/concept_0_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_0の代表画像例（生成対比因子：「Group A features everyday scenes and objects, while Group B focuses on events and people in formal settings.」）}
  \label{fig:coco_concept0}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 427]{image/coco/concept_1_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 425]{image/coco/concept_1_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_1の代表画像例}
  \label{fig:coco_concept1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 500 332]{image/coco/concept_2_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 612 612]{image/coco/concept_2_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_2の代表画像例}
  \label{fig:coco_concept2}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 427]{image/coco/concept_10_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 480 480]{image/coco/concept_10_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_10の代表画像例}
  \label{fig:coco_concept10}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 565 640]{image/coco/concept_50_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 456]{image/coco/concept_50_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_50の代表画像例（生成対比因子：「Group A focuses on electronics and mobile devices.」）}
  \label{fig:coco_concept50}
\end{figure}

\subsection{概念の具体性による性能比較}
LLM による対比因子ラベル生成の性能は，対象となる概念の具体性（Concrete vs.\ Abstract）によって差異が観測された．
データセット別・概念タイプ別の性能比較を表~\ref{tab:concreteness_comparison}に，主要アスペクト別の詳細を表~\ref{tab:concreteness_aspects}に示す．

\begin{table}[htbp]
  \centering
  \caption{概念の具体性による性能比較：データセット別BERTScore}
  \label{tab:concreteness_comparison}
  \begin{tabular}{lcc}
    \toprule
    データセット & 概念タイプ & BERTScore平均 \\
    \midrule
    SemEval-2014 & 具体的アスペクト & 0.7531 \\
    GoEmotions & 抽象的概念 & 0.7127 \\
    Steam & 抽象的アスペクト & 0.5403 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{概念の具体性による性能比較：主要アスペクト別BERTScore}
  \label{tab:concreteness_aspects}
  \begin{tabular}{lcc}
    \toprule
    データセット & アスペクト & BERTScore \\
    \midrule
    \multirow{4}{*}{SemEval-2014（具体的）} & Food & 0.7286 \\
    & Service & 0.7181 \\
    & Battery & 0.7646 \\
    & Screen & 0.8012 \\
    \midrule
    \multirow{4}{*}{Steam（抽象的）} & Gameplay & 0.5612 \\
    & Visual & 0.5164 \\
    & Story & 0.5383 \\
    & Audio & 0.5452 \\
    \midrule
    \multirow{4}{*}{GoEmotions（抽象的）} & Joy & 0.8192 \\
    & Disgust & 0.8316 \\
    & Embarrassment & 0.8941 \\
    & Neutral & 0.5437 \\
    \bottomrule
  \end{tabular}
\end{table}

SemEval-2014 の具体的なアスペクト（BERTScore 平均 0.7531）は，Steam のアスペクト（BERTScore 平均 0.5403）と比較して高い値を記録した．
GoEmotions の感情カテゴリ（BERTScore 平均 0.7127）は，Steam のアスペクトと比較して高い値を記録したが，SemEval-2014 の具体的なアスペクトと比較して低い値を記録した．
GoEmotions の一部の感情カテゴリ（Disgust 0.8316，Embarrassment 0.8941，Joy 0.8192）では，高い BERTScore を記録した一方，neutral（0.5437）では低い値を記録した．

\subsection{エラー分析と限界}
本実験では，全ての実験カテゴリにおいて実行上の失敗は発生せず，予定した 89 実験を完了した．
一方で，コンテキスト長制限を考慮し，各グループから最大 100 件のテキストを抽出する制約を設けており，大規模なテキスト集合に対する性能は十分に評価できていないという限界がある．
また，具体的なアスペクト（SemEval-2014 の Food，Service，Battery，Screen）では高い BERTScore を記録した一方で，抽象度の高いアスペクト（Steam の Gameplay，Visual，Story，Audio）や一部の感情カテゴリ（GoEmotions の neutral など）ではスコアが低く，概念の抽象性が命名性能に影響することが示唆された．

\subsection{補足分析}
Few-shot ICL では，少数の例題をプロンプトの \texttt{examples\_section} に挿入し，モデルが出力形式や語彙選択を安定させることを狙った．
Steam データセットを用いた Few-shot 実験では，1-shot 設定が 0-shot および 3-shot と比較して最も高い BERTScore を示し，過度な例示よりも代表的な例を少数与える方が有効である可能性が示された．
さらに，SemEval-2014，GoEmotions，Steam の結果を比較すると，具体的アスペクトを扱う SemEval-2014 で最も高いスコアが得られ，抽象度が高い Steam のアスペクトでスコアが低下するなど，概念の具体性が対比因子ラベル生成の難易度を規定する一因であることがうかがえた．

\section{統計的分析}
Few-shot（0/1/3-shot）とグループサイズ比較について Holm 補正済み Friedman 検定を実施し，それぞれ p=0.4724，p=0.3309 でいずれも有意差は確認されなかった．
モデル比較（n=4）とアスペクト説明文有無（n=4）については，対応のある Wilcoxon 検定を実施し，p=0.8750（中央値差 -0.0158），p=0.3750 といずれも非有意であった（Holm 補正後も非有意）．
詳細な統計量と検定結果は \texttt{stat\_tests.md} を参照されたい．

外れ値の分析を表~\ref{tab:outliers}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験における外れ値分析}
  \label{tab:outliers}
  \begin{tabular}{lcc}
    \toprule
    指標 & 最小値 & 最大値 \\
    \midrule
    BERTScore & 0.5164（Steam Visual） & 0.8941（GoEmotions Embarrassment） \\
    BLEU & 0.0000 & 0.0408（GoEmotions Embarrassment） \\
    LLM & 0.2000 & 0.8000（GoEmotions Joy, Disgust, Embarrassment） \\
    \bottomrule
  \end{tabular}
\end{table}

GoEmotions の感情カテゴリでは，neutral アスペクトで BERTScore 0.5437 と低い値を記録したが，これは他の感情カテゴリと比較して低い値であった．

実験間の一貫性の確認として，全実験カテゴリにおいて実験間の一貫性が確認された．
