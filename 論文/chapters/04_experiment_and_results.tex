\chapter{評価実験と結果}

\section{実験の目的と概要}
本章では、提案する大規模言語モデル（LLM）を用いた対比因子ラベル自動生成手法の有効性と汎用性を定量的に検証した結果を報告する。
本実験は、モデル内部の特定のニューロン発火条件に対応する集合的な意味的差分を、LLM（GPT-4o-mini 等）によるコントラスティブ要約によって自然言語ラベルとして抽出可能であるか、また生成されたラベルが人手アノテーションによる正解ラベルとどの程度意味的に類似するかを、定量的指標および LLM による評価を用いて客観的に測定することを目的とする。

提案手法のドメイン汎用性を検証するために、レビュー、感情分類、画像キャプションという多様なドメインに属する 4 種類のデータセットを用いた。
これらのデータセットは、それぞれが持つ正解アスペクトラベルを、LLM が生成する対比因子ラベルの妥当性を評価するためのグラウンド・トゥルースとして使用した。

% TODO: 実験の全体像（実験カテゴリの概要）
% TODO: 評価の観点（有効性、汎用性、性能など）

\section{データセット}

\subsection{データセットの選定理由}
% TODO: なぜこれらのデータセットを選んだか
% TODO: ドメインの多様性の説明

\subsection{Steam Review Aspect Dataset}
Steam Review Aspect Dataset は、Steam ゲームレビューから収集されたテキストデータであり、特定のゲームアスペクトに関する言及を含むデータセットである \mbox{[6, 8]}。

\begin{itemize}
  \item \textbf{概要}: 英語の Steam ゲームレビュー 1,100 件（学習用 900 件、テスト用 200 件）で構成される。
  データ収集は SRec データベースのスナップショットに基づき行われた。
  \item \textbf{アスペクト}: レビューを特徴づける 8 種類のアスペクトが人手でアノテーションされている。
  内訳は Recommended（推奨）、Story（物語）、Gameplay（ゲームプレイ）、Visual（視覚）、Audio（聴覚）、Technical（技術）、Price（価格）、Suggestion（提案・要望）である。
  \item \textbf{特徴}: ゲームという特定の製品ドメインに特化しており、特に「Gameplay」や「Technical」といったゲーム固有のメカニクスや技術的側面に関するアスペクトを含む。
  テストセットにおけるアスペクト件数の例として、Gameplay が 154 件、Recommended が 148 件、Story が 89 件である。
  これにより、LLM が専門性の高いテキスト集合間の意味的差分を抽出する能力を検証するためのベンチマークとして機能する。
\end{itemize}

% TODO: 実験での使用方法（グループA/Bの作成方法）
% TODO: 使用したアスペクトとその理由

\subsection{SemEval-2014 ABSA（Restaurants）}
SemEval-2014 ABSA（Restaurants）は、アスペクトベース感情分析（ABSA）の標準的なベンチマークとして広く使用されるレストランレビューのデータセットである \mbox{[2, 6, 12--14]}。

\begin{itemize}
  \item \textbf{概要}: レストランレビューのテキストを含み、各文に対してアスペクト（観点）とそれに対する感情極性が人手でアノテーションされている。
  \item \textbf{アスペクト}: 本研究では、主に Food（食べ物）、Service（サービス）、Price（価格）、Atmosphere（雰囲気）の 4 種類のアスペクトを採用した。
  \item \textbf{特徴}: このデータセットは、LLM による自動命名の性能が概念の具体性に依存するかを検証するための鍵となるデータを含む。
  「Food」や「Price」は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトである一方、「Atmosphere」は広範な文脈や比喩的表現からの高度な推論を必要とする抽象的なアスペクトに分類される。
  本研究の主要な定量評価ベンチマークとして使用された。
\end{itemize}

% TODO: 実験での使用方法
% TODO: 使用したアスペクトとその理由

\subsection{GoEmotions}
GoEmotions は、細粒度感情分類タスクのために Reddit コメントから収集されたデータセットである \mbox{[7, 19, 20]}。

\begin{itemize}
  \item \textbf{概要}: Demszky らによって構築されたもので、総レコード数 63,812 件から成るマルチラベル形式のデータセットである。
  \item \textbf{アスペクト}: 28 の感情カテゴリ（27 感情 + neutral）でラベル付けされている。
  主要なカテゴリには Joy（喜び）、Anger（怒り）、Admiration（称賛）、Neutral（中立）などが含まれる。
  \item \textbf{特徴}: 感情という極めて抽象的な概念を対比因子として扱えるかを検証するための挑戦的なデータセットとして位置づけられる。
  このデータセットでは、感情という内在的な状態をテキストの集合差分から推論する必要があり、具体的な物理的実体を持たない概念の命名精度を測るために使用された。
  実験では、任意の感情アスペクト（例：joy）を指定し、「その感情を含むテキスト群 $A$」と「その他のアスペクトを含むテキスト群 $B$」を比較する設定が採用された。
\end{itemize}

% TODO: 実験での使用方法
% TODO: 28感情カテゴリの選定理由

\subsection{Retrieved Concepts（COCO Captions）}
Retrieved Concepts（COCO Captions）は、視覚的概念記述の生成能力を検証するために、画像キャプションデータセット COCO に基づいて構築されたデータセットである \mbox{[7]}。

\begin{itemize}
  \item \textbf{概要}: COCO Captions に基づく 300 の概念（concept\_0 ～ concept\_299）を扱う。
  データには、Top-100/Bottom-100 の類似度順キャプションデータが含まれる。
  \item \textbf{特徴}: LLM がテキストの集合差分から視覚的な特徴を抽象化した概念記述を生成できるかを確認するために使用された。
  この検証は、本手法が将来的に画像モデルのニューロン解釈（例：縞模様、空の色といった視覚的概念）に適用可能であるかを探るための基礎的なデータを提供する。
\end{itemize}

% TODO: 実験での使用方法
% TODO: Top-100/Bottom-100の選定方法
% TODO: 正解ラベルがない場合の評価方法

\section{実験設定}

\subsection{実験パイプラインの概要}
本実験では、コントラスティブ要約に基づく対比因子ラベル生成器として、GPT-4o-mini を含む複数の大規模言語モデルを採用した。
提案手法は、統一されたパイプラインとして構築され、その目的は、特定の概念に対応するテキスト集合 $A$（特徴あり群）と、そうでないテキスト集合 $B$（特徴なし群）の差分から、意味的な対比因子ラベル $L$ を LLM に生成させることである。

\begin{itemize}
  \item \textbf{タスク定式化}: ニューロン $N$ が強く活性化するテキスト集合 $A$ と、活性化しない集合 $B$ を入力とし、集合 $A$ に特有で $B$ には見られない意味的差分を $L$ として生成する写像 $(A, B) \to L$ を定式化した。
  \item \textbf{グルーピング}: 活性化値に基づき、ハイパーパラメータ $group\_size$ を用いて集合 $A$ と $B$ を抽出する。
  メイン実験では、プロンプトのコンテキスト長制限を考慮し、$group\_size = 100$ を採用した。
  \item \textbf{Few-shot ICL の検証}: LLM の出力形式の揺らぎや語彙の安定性を確保するための検証手段として、Few-shot インコンテキスト・ラーニング（ICL）のバリエーション（0-shot, 1-shot, 3-shot）を定量的に検証した \mbox{[2, 32--34]}。
  LLM は、プロンプト内で集合 $A$ と $B$ を比較し、$A$ に特徴的で $B$ には欠如している意味的側面を推論するよう指示された。
\end{itemize}

% TODO: パイプラインの全体フロー図（オプション）

\subsection{LLMモデルとパラメータ設定}
% TODO: 使用モデル（GPT-4o-mini等）の詳細
% TODO: 各実験カテゴリでのモデル選択理由
% TODO: 温度パラメータ（temperature）の設定と理由
% TODO: 最大トークン数（max_tokens）の設定と理由
% TODO: その他の生成パラメータ

\subsection{プロンプト設計}
% TODO: プロンプトテンプレートの構造
% TODO: グループA/Bの提示方法
% TODO: 出力形式の指示方法
% TODO: Few-shot例の挿入方法
% TODO: アスペクト説明文の使用方法（該当実験のみ）
% TODO: プロンプトの具体例

\subsection{データの前処理と分割方法}
% TODO: テキストの前処理手順
% TODO: グループA/Bの抽出方法
  % TODO: グループサイズ（group_size）の決定方法
  % TODO: サンプリング方法（ランダム/順序/重み付き）
% TODO: 分割タイプの説明（aspect_vs_others等）
% TODO: コンテキスト長制限への対応

\subsection{Few-shot例の作成方法}
% TODO: Few-shot例の選定基準
% TODO: 例の品質管理方法
% TODO: 0-shot, 1-shot, 3-shotの違いと設定方法

\subsection{実験カテゴリの定義}
% TODO: メイン実験の定義と目的
% TODO: Few-shot実験の定義と目的
% TODO: グループサイズ比較実験の定義と目的
% TODO: モデル比較実験の定義と目的
% TODO: アスペクト説明文比較実験の定義と目的
% TODO: COCO実験の定義と目的

\subsection{比較手法とベンチマーク}
本研究は、非教師ありコンセプト抽出（UCBM や CCE など）が発見した潜在ベクトルに名前を付与する「命名モジュール」としての機能に特化している \mbox{[29, 35]}。
そのため、生成されたラベルの品質を、人手アノテーションされた既存の ABSA ベンチマーク（SemEval-2014 Restaurant/Laptop、Steam レビューなど）の正解ラベルとの意味的類似性と比較することで評価した。
このアプローチにより、本手法の命名性能が、高コストな人手ラベリングによって確立された基準に対してどの程度妥当であるかを検証した。

\section{評価指標}
生成された自然言語ラベル $L$ の品質を評価するために、BERTScore、BLEU、および LLM による意味的類似度評価を採用した。

\subsection{評価指標の選定理由}
% TODO: なぜBERTScore, BLEU, LLM評価を選んだか
% TODO: 各指標の役割と位置づけ

\subsection{BERTScore}
\begin{itemize}
  \item \textbf{定義と役割}: BERTScore は、生成されたラベル $L$ と人手アノテーションされた正解ラベル $L_{\mathrm{ref}}$（またはその説明文）との間の文脈的意味的な類似性を測る主要な指標として採用された \mbox{[1, 2, 17, 37]}。
  この指標は、BERT などの事前学習済み言語モデルによって得られる文脈化埋め込み表現のコサイン類似度に基づき、語彙レベルの一致度を超えたセマンティックな評価を提供する。
  \item \textbf{位置づけ}: 本タスクにおいては、LLM が集合差分という複雑な推論タスクの結果を要約した自然言語フレーズを生成するため、意味的な妥当性を定量的に示す BERTScore が最も重要な評価基準として位置づけられた。
\end{itemize}

% TODO: 計算方法の詳細（モデル、正規化方法）
% TODO: 評価範囲と解釈方法

\subsection{BLEU（Bilingual Evaluation Understudy）}
\begin{itemize}
  \item \textbf{定義と役割}: BLEU は、生成ラベルと正解ラベルとの間の語彙レベルの一致度（n-gram の重複）を確認するために補助的に使用された。
  \item \textbf{本タスクにおける制約}: 本タスクの性質上、BLEU スコアは極めて低い値を示すことが前提とされた。
  これは、正解ラベルが「food」「price」のような単一の言葉または簡潔なフレーズであるのに対し、
  LLM が生成するラベルはしばしば「食べ物の品質に関する言及」「価格設定の側面」といった説明的なフレーズとなるため、語彙的な重複（n-gram overlap）が本質的に生じにくいためである。
  したがって、BLEU スコアの低さはモデルの命名失敗を意味するものではなく、単に語彙的一致度を測る指標が本タスクの性質に適合していないことを示す参考値として扱われた。
\end{itemize}

% TODO: 計算方法の詳細（スムージング関数等）
% TODO: 低値が予想される理由の詳細

\subsection{LLM評価スコア}
% TODO: LLM評価の目的と位置づけ
% TODO: 評価に使用するモデル
% TODO: 評価プロンプトの設計
% TODO: 評価基準（5段階評価の詳細）
% TODO: 正規化方法
% TODO: BERTScoreとの関係

\section{実験結果}

\subsection{メイン実験結果}
% TODO: 実験設定の詳細（パラメータ一覧）
% TODO: 総実験数と成功/失敗数
% TODO: データセット別の結果
  % TODO: SemEvalの結果（平均、最小、最大）
  % TODO: GoEmotionsの結果
  % TODO: Steamの結果
% TODO: アスペクト別の結果（主要なアスペクト）
% TODO: 主要な発見と考察

\subsection{主要実験結果：自動命名の定量分析}
SemEval-2014 データセット（レストランレビュー）における Few-shot 設定ごとの平均 BERTScore および BLEU スコアは、以下の定量的な傾向を示した。

\begin{itemize}
  \item \textbf{BERTScore の達成値}: LLM によるコントラスティブ要約の結果、生成された対比因子ラベルは、正解ラベルとの間で平均約 0.551 という中適度な意味的関連性を達成した。
  この値は、LLM がニューロンの発火群と非発火群というテキストの集合差分から、その集合の本質的な意味的核を抽出できる能力を有していることを示唆する。
  \item \textbf{Few-shot ICL の影響}: Few-shot ICL のバリエーション（0-shot, 1-shot, 3-shot）を比較した結果、1-shot 設定が他の設定と比較して最も高い BERTScore を示す傾向が観測された。
  これは、LLM がタスクの定義と出力スタイルを学習する上で、少数の適切に選定された例（1 組）が最も効率的かつ効果的に機能することを示している。
  \item \textbf{BLEU スコアの傾向}: 一方、BLEU スコアは全ての Few-shot 設定において極めて低値（平均約 0.007）を示した。
  この低値は、生成ラベルと正解ラベルとの間で語彙的な重複がほとんど存在しないという、本タスクの性質を裏付ける結果である。
\end{itemize}

\subsection{Few-shot設定による性能比較}
% TODO: 実験設定（0-shot, 1-shot, 3-shot）
% TODO: Few-shot別の平均スコア
% TODO: アスペクト別のFew-shot効果
% TODO: 最適なFew-shot設定の考察

\subsection{グループサイズの影響分析}
% TODO: 実験設定（50, 100, 150, 200, 300）
% TODO: グループサイズ別の性能
% TODO: 最適なグループサイズの考察
% TODO: コンテキスト長との関係

\subsection{モデル比較実験}
% TODO: GPT-4o-mini vs GPT-5.1の比較
% TODO: モデル別の性能差
% TODO: アスペクトによる性能差の違い
% TODO: モデル選択の示唆

\subsection{アスペクト説明文の効果検証}
% TODO: 実験設定（説明文あり/なし）
% TODO: 説明文の有無による性能差
% TODO: アスペクト別の効果の違い
% TODO: 説明文の有用性の考察

\subsection{COCO Retrieved Concepts実験}
% TODO: 実験設定（正解ラベルなし）
% TODO: 生成された対比因子の例
% TODO: 画像との整合性確認方法
% TODO: 視覚的概念記述の生成能力の検証結果

\subsection{概念の具体性による性能比較}
LLM による対比因子ラベル生成の性能は、対象となる概念の具体性（Concrete vs.\ Abstract）によって明確な差異を示す傾向が観測された。

\begin{itemize}
  \item \textbf{具体的なアスペクトにおける優位性}: SemEval-2014 における「Food」や「Price」といった語彙的に安定した具体的なアスペクトの命名において、本手法は相対的に高い BERTScore を達成した。
  これらの概念は、具体的な製品や属性に関する明確な語彙的証拠（例：ピザ、高すぎる、割引）がテキスト集合 $A$ に含まれやすく、LLM が差分を容易に抽出できたことを示唆する。
  Steam Review Aspect Dataset における「Technical」や「Gameplay」といった具体的特性も、同様に比較的高い意味的類似性を示す傾向があった。
  \item \textbf{抽象的な概念における性能劣位}: 対照的に、SemEval-2014 における「Atmosphere」や Steam Review Dataset における「Story」といった抽象的なアスペクトの命名精度は、具体的なアスペクトと比較して劣位となる傾向が確認された。
  さらに、GoEmotions データセットで検証された 28 の感情カテゴリ（例：Joy, Anger, Admiration など）は物理的な実体を持たない高度に抽象的な概念であり、これらの概念に対する対比因子ラベルの生成は、具体的なアスペクトと比較して総じて低い BERTScore を記録した。
  \item \textbf{傾向の対比}: これらの結果は、LLM が集合差分を推論する際、具体的な語彙や構造に強く依存する「抽出」タスクに近い性能を確保できる一方で、
  広範な文脈や感情的なニュアンスといった抽象的な要素を要約し、簡潔なラベルとして命名するタスク（高度な「推論」を必要とする）においては、性能が相対的に低下する傾向があることを定量的に示した。
\end{itemize}

% TODO: 定量的な数値による裏付け

\subsection{エラー分析と限界}
% TODO: 失敗した実験の分析
% TODO: エラーの種類と原因
% TODO: 手法の限界の考察

\subsection{補足分析}
Few-shot ICL の導入は、LLM の生成ラベルのスタイルを正解ラベルの簡潔なスタイルに近づけることを目的として検証された。
具体的には、Few-shot の例をプロンプトの \texttt{examples\_section} として挿入し、モデルがその出力形式を模倣する特性を利用した。
1-shot 設定が最適であったことは、LLM がタスク定義と 1 つの高品質な例から、集合差分を命名するための有効な生成戦略を迅速に確立したことを示す。

また、SemEval-2014 や Steam レビューデータは、LLM 命名の評価ベンチマークとして使用された。
GoEmotions データセットは、総レコード数 63,812 件に及ぶ大規模な Reddit コメントから収集されており、
28 の感情カテゴリは、本手法のロバスト性を抽象概念の集合に拡張するための重要な挑戦的要素として機能した。
実験パイプラインでは、グループ $A$ と $B$ を抽出する際、コンテキスト長超過エラーを回避するため、$group\_size$ が最大 100 件に制限された設定が用いられた。

さらに、BLEU スコア（約 0.007）の低さは、生成ラベルが「～に関する言及」といった説明文であり、正解ラベル（例：「food」）と直接的な語彙重複を持たないという、
評価指標とタスクの性質とのミスマッチによって生じている。
この観測結果は、語彙的重複を測る指標が本タスクに不適であり、文脈的意味類似度を測る BERTScore の採用が妥当であるという評価戦略の選択を裏付ける定量的証拠となる。
BERTScore が約 0.551 という中適度な値を示したことは、語彙レベルでは一致しないが意味レベルでは関連性が保持されているという事実を客観的に示す。

最後に、実験結果は、LLM（GPT-4o-mini）によるコントラスティブ要約が XAI におけるニューロン対比因子命名タスクの実現可能性を示し、
SemEval-2014 ベンチマークにおいて BERTScore で約 0.551 という妥当な意味的関連性を達成したことを定量的に示した。
また、概念の具体性が命名精度に系統的に影響を与え、具体的なアスペクトにおいて優位性を示す傾向が確認された。

\section{統計的分析}
% TODO: 統計的有意性の検定（必要に応じて）
% TODO: 信頼区間の計算
% TODO: 外れ値の分析
% TODO: 実験間の一貫性の確認
