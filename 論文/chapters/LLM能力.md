大規模言語モデルにおける意味理解、パターン抽出、および要約能力の包括的評価：語用論から XAI 概念キュレーションまで 1. 序論人工知能（AI）の歴史において、自然言語処理（NLP）は常に中核的な課題であり続けてきた。特に、2017 年の Transformer アーキテクチャの登場以降、大規模言語モデル（LLM）は急速な進化を遂げ、単なる統計的な単語予測器の枠を超え、高度な意味理解や論理推論を模倣するシステムへと変貌している。GPT-4 や Gemini、Llama 3 といった最新のモデルは、数百億から数兆のパラメータを有し、ウェブ上の膨大なテキストデータを学習することで、人間と同等、あるいは特定のタスクにおいては人間を凌ぐ言語生成能力を獲得したとされる。しかし、「流暢な生成」と「真の意味理解」の間には依然として深い溝が存在するという指摘も根強い。本報告書は、2020 年から 2025 年にかけての主要な研究成果を網羅的に調査し、LLM がテキストの文脈をどのように理解し、そこから意味パターンを抽出し、情報を要約・対比し、さらには自身の内部表現を人間に説明可能な概念として提示できるかについて、その能力と限界を厳密に評価することを目的とする。本稿では、特に以下の 5 つの領域に焦点を当てる。第一に、文脈理解能力である。ここでは、単語の意味を超えた「語用論（Pragmatics）」の領域、すなわち含意、皮肉、比喩といった高度な言語現象の処理能力を検証する。第二に、意味パターン抽出である。膨大な非構造化データから共通のテーマや法則性を帰納的に見出す能力は、科学的発見やデータ分析において極めて重要である。第三に、自然言語要約能力である。情報の圧縮と再構成という古典的なタスクにおいて、LLM が従来のファインチューニングモデルといかなる差異を示すか、そしてその評価手法がどのように変遷しているかを論じる。第四に、対比的・比較的タスクである。単一の文書処理を超え、複数の情報源を統合・比較し、構造化された洞察を導き出す能力について、最新のフレームワーク（STRUM-LLM 等）を基に分析する。最後に、説明可能 AI（XAI）と概念キュレーションへの応用である。LLM の内部で何が起きているのかを解明するための自動ラベリング技術や、社会科学・人文学における概念発見ツールとしての可能性と課題を掘り下げる。これらの分析を通じて、LLM が「確率的なオウム（Stochastic Parrots）」を超え、真の知識処理基盤となり得るか、その技術的到達点と残された課題を浮き彫りにする。2. 文脈理解能力に関する研究：語用論、比喩、長文脈の壁 LLM の文脈理解能力は、表層的な構文解析から深層的な意味理解へとその評価軸を移している。特に近年注目されているのは、明示されていない情報を文脈から補完する能力、すなわち「語用論的推論」である。2.1 語用論的推論と PUB ベンチマークによる評価語用論（Pragmatics）は、言語使用の文脈、話し手の意図、共有された知識などを考慮して意味を解釈する言語学の一分野である。LLM が人間との自然な対話を実現するためには、文字通りの意味（Semantics）だけでなく、発話の背後にある意図（Pragmatics）を理解することが不可欠である。2.1.1 PUB ベンチマークの構築とタスク設計 2024 年に提案された Pragmatics Understanding Benchmark (PUB) は、LLM の語用論的能力を包括的に評価するための画期的なフレームワークである 1。PUB は、以下の 4 つの主要な語用論的現象をカバーする 14 のタスクで構成されている。含意（Implicature）: 発話者が明示的に述べずに伝達しようとする意味。例えば、「寒いね」という発話が「窓を閉めてほしい」という依頼を意味する場合など。前提（Presupposition）: 発話が真であるために当然と見なされる背景情報。参照（Reference）: 代名詞や指示詞が指し示す対象の特定。直示（Deixis）: 発話の状況（時間、場所、人物）に依存して意味が決まる表現（「明日」「ここ」「私」など）。PUB は既存のデータセットを統合し、さらに 6,000 件の新規アノテーションを追加して合計 28,000 件の事例を含むテストスイートを構築した。評価形式は、会話形式の質問応答（MCQA）を採用しており、モデルが文脈を読み取り、適切な解釈を選択できるかを測定する 1。2.1.2 人間と LLM の性能ギャップ PUB を用いた評価実験の結果、LLM（GPT-4、Gemini、Llama-2 等）は依然として人間レベルの語用論的理解には到達していないことが明らかになった 1。間接的応答の解釈: 特にモデルが苦戦したのは、間接的な応答（Indirect Response）の解釈である（Task 1, 2）。例えば、「今夜の映画に行ける？」という質問に対し、「明日は早朝会議があるんだ」と答えた場合、人間は即座に「行けない（拒絶）」と解釈する。しかし、多くのモデルはこの含意を捉えきれず、字義通りの情報（会議があるという事実）のみに注目する傾向が見られた 4。皮肉とユーモア: 皮肉（Sarcasm）の検出（Task 6）においても、LLM の性能は限定的であった。皮肉の理解には、文脈だけでなく、常識、話者の性格、感情的トーンといった多層的な情報の統合が必要であり、現在の LLM の学習パラダイムでは捕捉しきれない「心の理論（Theory of Mind）」の欠如が示唆されている 4。タスク感度と脆弱性: 人間は多様なタスク形式に対して安定した理解を示すのに対し、LLM はプロンプトのわずかな変化（例：ヒントの有無、話者の順序の入れ替え）によって性能が大きく変動することが確認された 5。これは、モデルの理解が堅牢な概念理解に基づいているのではなく、学習データ内の表面的なパターンへの過適合（Overfitting）に依存している可能性を示している。2.1.3 モデルサイズとファインチューニングの影響 PUB の分析からは、モデルの規模とトレーニング手法に関する重要な知見も得られている。チャット形式でのファインチューニング（Instruction Tuning）は、特に小規模なモデルにおいて語用論的能力を大幅に向上させることが判明した 6。しかし、パラメータ数が巨大なベースモデル（Base Models）においては、ファインチューニングによる上積みは限定的であり、ベースモデル自体が事前学習段階である程度の語用論的パターンを獲得していることが示唆された。このことは、言語モデルのスケーリング則が語用論的領域にもある程度適用される一方で、特定の壁（皮肉や高度な含意）を超えるには量的な拡大だけでは不十分である可能性を示している 2。2.2 比喩理解の深層と「語彙的類似性」の罠比喩（Metaphor）は、ある概念領域の構造を別の領域にマッピングする認知プロセスであり、LLM の抽象的思考能力を測る試金石となる。2.2.1 MUNCH データセットによる検証 Metaphor Understanding Challenge (MUNCH) データセットを用いた研究は、LLM の比喩理解能力に深刻な疑問符を投げかけている 7。MUNCH は、比喩を含む文（例："The rumor flew through the office"）に対し、適切な言い換え（Apt Paraphrase: "spread quickly"）と、不適切な言い換え（Inapt Paraphrase: "traveled by air"）を選択させるタスクを含む。ここで重要なのは、不適切な言い換えが「ランダムな間違い」ではなく、元の比喩表現（"flew"）と語彙的な類似性が高いが文脈的に誤っている語句として設計されている点である。2.2.2 表面的な連想への依存実験の結果、LLM はしばしば適切な言い換えよりも不適切な言い換えを選択してしまう傾向があることが確認された 7。これは、LLM が比喩の深層的な意味（ターゲット領域へのマッピング）を理解して処理しているのではなく、事前学習における単語の共起頻度や埋め込み空間における近接性（Lexical Similarity）に強く引きずられていることを示唆している。「噂が飛ぶ（flew）」という表現を見たとき、モデルは「飛ぶ」という単語から「空を移動する」という文字通りの意味を強く連想し、文脈（オフィス、噂）との不整合を看過してしまうのである。一方で、GPT-4 などの最先端モデルは、セルビアの詩の英訳における比喩解釈など、一部のタスクでは大学生レベルの人間を凌駕する解釈を生成したという報告もある 9。これは、モデルが一般的な比喩パターン（Conventional Metaphors）に対しては高度な適応を見せる一方で、文脈との緻密な照合が必要な新規性の高い比喩や、意図的な引っかけ問題（MUNCH のような）に対しては脆弱性を露呈するという、能力の不均一性を示している。2.3 長文脈理解における「大域的整合性」の欠如 LLM のコンテキストウィンドウは拡大の一途を辿り、10 万トークン（小説 1 冊分）以上の入力を処理可能とするモデルも登場している。しかし、「入力できる」ことと「理解できる」ことは同義ではない。2.3.1 TLDM ベンチマークと「迷子の真ん中」Too Long, Didn't Model (TLDM) ベンチマークを用いた研究は、フロンティアモデルであっても 64k トークンを超える長文脈において、物語の大域的な理解（Global Understanding）が崩壊することを示している 10。多くの長文脈ベンチマーク（Needle in a Haystack 等）は、大量のテキストの中から特定の事実を検索する「検索タスク」に過ぎない。これに対し、TLDM は物語のプロット要約、世界観の設定（Storyworld Configuration）、時間経過の把握といった、文書全体に散らばる情報を統合し、推論する必要があるタスクを課す。結果として、モデルの性能は入力長が増加するにつれて低下し、特に文脈の中間部分にある情報の統合に失敗する「Lost in the Middle」現象が確認された。2.3.2 ChapterBreak による評価同様の傾向は ChapterBreak データセットを用いた研究でも報告されている 11。このタスクでは、物語のある章の終わりを与え、続く次章の冒頭を複数の候補から選ばせる。正解するためには、直前の文脈だけでなく、物語全体を通じた伏線やトーン、並行して進むサブプロットの理解が必要となる。実験の結果、長距離言語モデル（LRLM）は、このタスクにおいてセグメント単位で学習されたモデルよりも性能が劣る場合があり、長距離の文脈情報を有効に活用できていないことが露呈した。これは、Transformer の Attention 機構が局所的な文脈（直近数千トークン）に強くバイアスされているためと考えられる。3. 意味パターン抽出に関する研究：帰納的推論と内部表現の解明 LLM は、明示的なルールを与えられなくても、提示された例からパターンを学習し、未知のデータに適用する「文脈内学習（In-Context Learning）」能力を持つ。これは高度な帰納的推論（Inductive Reasoning）の一形態と見なせる。3.1 帰納的推論のメカニズムと限界 3.1.1 推論トレースの逆説的効果一般に、CoT（Chain-of-Thought）プロンプティングのように推論過程を明示させることは、モデルの性能を向上させると考えられている。しかし、帰納的推論タスクにおいては、推論トレースが長くなるほど精度が低下する現象が観察されている 13。研究者らは、100 以上の失敗事例を分析し、以下の 3 つの主要なエラータイプを特定した：数学の過剰適用（Math Overuse）: モデルが記号的な入力（例：トランプの模様やチェスの駒）に対して、不適切に算術的な演算を適用しようとする現象。過度な一般化（Overgeneralization）: わずかな例から性急にルールを導き出し、例外を無視して適用してしまう。幻覚ルール（Hallucinated Rules）: 入力データには存在しない制約条件や法則を勝手に捏造し、それに固執する。これらのエラーは、LLM が真の論理的推論を行っているのではなく、学習データに含まれる「推論っぽいパターン」を模倣しようとするあまり、文脈にそぐわない定型的な処理を適用してしまうことに起因すると考えられる。3.2 概念誘導フレームワーク：LLooM テキストデータの集合からボトムアップで概念（Concepts）を抽出し、構造化するタスクにおいて、LLM は従来の手法を凌駕しつつある。3.2.1 LLooM のアプローチ LLooM (Large Language Model-based Concept Induction) は、LLM を用いてテキストデータから概念を反復的に生成・統合するフレームワークである 14。従来のトピックモデル（LDA や BERTopic）が単語の共起に基づく統計的なクラスタリング（Bag-of-Words）に依存していたのに対し、LLooM は LLM の言語理解能力を活用して、各クラスタに対して「人間が理解可能な自然言語による定義」を生成する。3.2.2 従来手法との比較比較実験において、LLooM はトピックの「一貫性（Coherence）」と「多様性（Diversity）」の両面で LDA や BERTopic を上回る結果を示した 16。特に、社会科学や医療分野（臨床試験データの抽出など）への適用において、LLM 支援型の手法は、完全な人手作業に比べて大幅な時間短縮を実現しつつ、97%以上の精度で情報を抽出できることが実証されている 17。これは、LLM が単なるキーワードマッチングではなく、文脈に応じた意味的な抽象化（Abstraction）を行えることを示している。3.3 内部表現の解明：PRISM と多義性の克服 LLM が抽出する「意味」が、モデル内部でどのように表現されているかを解明する研究も進んでいる。ここで最大の壁となるのが「多義性（Polysemanticity）」である。3.3.1 多義性と重ね合わせの問題多くのニューロンは単一の概念に反応するのではなく、一見無関係に見える複数の概念（例：「引用文献」と「HTTP リクエスト」）に対して活性化する。これは「重ね合わせ（Superposition）」と呼ばれる現象で、モデルが限られたニューロン数でより多くの特徴を表現するための圧縮技術であると仮説立てられている 18。しかし、この多義性は人間による解釈を極めて困難にする。3.3.2 PRISM フレームワークによる解明 PRISM (Polysemantic FeatuRe Identification and Scoring Method) は、この多義性を前提とした解釈フレームワークである 20。PRISM は以下の 3 ステップで機能する。Percentile Sampling: 対象となるニューロンが高く活性化するテキスト断片を、単なる Top-K ではなく、活性化分布のパーセンタイルに基づいて幅広くサンプリングする。Concept Clustering: サンプリングされたテキストを埋め込みベクトル化し、クラスタリングを行うことで、そのニューロンが反応する「異なる意味のモード」を分離する。Cluster Labeling: 各クラスタに対して LLM を用いて要約ラベルを生成する。これにより、単一のニューロンに対して「70%は学術的文脈、30%はプログラミングコードに反応」といった多面的な解釈（Multi-concept description）が可能となり、解釈の忠実性（Faithfulness）を定量化する「多義性スコア」も導入された。4. 自然言語要約能力に関する研究：精度と評価のパラダイムシフト要約は LLM の最も実用的な応用分野の一つであるが、その評価手法と「事実性」の担保については激しい議論が続いている。4.1 GPT-4 対ファインチューニングモデル 4.1.1 ROUGE スコアの逆説従来の要約研究では、ROUGE スコア（参照要約との n-gram 一致率）が標準的な評価指標であった。しかし、最新の研究では、BART や T5 などのファインチューニングモデルが GPT-4 よりも高い ROUGE スコアを出す一方で、人間による評価では GPT-4 が圧倒的に好まれるという「逆転現象」が確認されている 22。原因: 人間が作成した参照要約（Gold Standard）は、必ずしも最適解ではない。また、GPT-4 は原文の情報を保持しつつ、より流暢で抽象度の高い（Abstractive）言い換えを行うため、参照要約との単語レベルの重複が減少し、ROUGE スコアが低下する。含意: これは、ROUGE のような表層的な一致指標が、LLM 時代の要約評価にはもはや不適切であることを示唆している。4.2 事実整合性と幻覚のメカニズム要約における最大のリスクは、原文にない情報を生成する「幻覚（Hallucination）」である。4.2.1 幻覚ニューロン（H-Neurons）の発見 2024 年の研究において、LLM 内部に幻覚の発生と強く相関する特定のニューロン群（H-Neurons）が存在することが発見された 24。これらは全ニューロンの 0.1%未満であるが、これらの活性化を監視することで、モデルが幻覚を生成しようとしているかを高い精度で予測できる。さらに、H-Neurons への介入実験により、これらのニューロンが「過剰適合（Over-compliance）」、すなわち「分からなくても無理やり答えようとする性質」と因果関係があることが示された。これは、幻覚が単なるエラーではなく、モデルが「有用であろうとする」訓練の結果として獲得した機能的な回路の一部であることを示唆する衝撃的な発見である。4.2.2 事実不整合の検出：UIEFID 事実性を担保するための工学的アプローチとして、UIEFID (Universal Information Extraction-enhanced Factual Inconsistency Detection) が提案されている 26。この手法は、要約の生成と検証を分離し、LLM にまずテキストから構造化データ（スキーマ）を抽出させ、その構造レベルで原文と要約を比較する。これにより、推論のブラックボックス性を回避し、説明可能な形で矛盾を検出することが可能となる。4.3 新たな評価指標：LLM-as-a-JudgeROUGE の限界を受け、LLM 自体を評価者として用いる LLM-as-a-Judge アプローチが標準化しつつある。G-Eval: GPT-4 を用いて、要約の「一貫性」「整合性」「流暢さ」「関連性」を評価するフレームワーク。人間の評価との相関が従来の指標よりも遥かに高いことが実証されている 27。Elo レーティング: 複数のモデルの要約をペアワイズ比較（A と B どちらが良いか）させ、チェスのレートのようにスコア化する手法も、評価の安定性を高めるために導入されている 28。5. 対比的・比較的タスクにおける性能：構造化と比較の深化単一の文書を要約するだけでなく、複数の文書（例：競合製品のレビュー、異なる政治的見解）を比較し、差異を浮き彫りにする「対比的要約」は、LLM の新たなフロンティアである。5.1 STRUM-LLM：構造化による対比 Google Research らが開発した STRUM-LLM は、対比的要約における主要な課題（入力長の制限、非構造化データの比較困難性）を解決するフレームワークである 29。5.1.1 パイプラインアプローチ STRUM-LLM は、以下の高度なパイプラインを採用している。Tiling（タイリング）: 入力文書を LLM のコンテキストウィンドウに収まるサイズに分割し、無限長の入力を処理可能にする。Extraction（属性抽出）: 各チャンクから、比較対象となる実体の「属性（例：バッテリー、価格）」と「値」を抽出する。Contrast Identification（対比特定）: 抽出された情報を統合し、製品 A と製品 B の間で顕著な差異（Helpful Contrast）がある属性を特定する。Structured Output: 最終的に、属性ごとに行を分けた構造化されたテーブル形式で要約を出力する。このアプローチにより、LLM は「何となく似ている」といった曖昧な比較ではなく、ユーザーの意思決定に直結する具体的かつ証拠（Attribution）に基づいた比較を提供できる。5.2 比較意見マイニング（Comparative Opinion Mining）OOMB (Online Opinion Mining Benchmark) などの研究では、LLM を用いて Web 上の議論やレビューから「実体-特徴-意見」のタプルを抽出し、比較洞察を生成する能力が検証されている 31。従来の感情分析が単にポジティブ/ネガティブの分類に留まっていたのに対し、LLM は「A は B に比べて価格は高いが、サポートの質で勝る」といった、トレードオフを含む複雑な関係性を記述できる点が革新的である。6. XAI と概念キュレーションの文脈での適用：ブラックボックスからガラスボックスへ LLM は「説明される対象」であると同時に、「説明を生成する主体（概念キュレーター）」としても機能する。6.1 自動ニューロンラベリング OpenAI の研究チームは、GPT-4 を用いて他の言語モデル（GPT-2 等）のニューロンの挙動を自然言語で説明する自動化パイプラインを構築した 33。手法: 対象ニューロンが活性化するテキストを GPT-4 に見せ、「このニューロンは何に反応しているか？」という説明を生成させる。その後、その説明に基づいて別のテキストでの活性化を予測させ、説明の正確さをスコアリングする。成果と限界: これにより、数万のニューロンに対して人手を介さずにラベル付けが可能になった。しかし、前述の「多義性」の問題や、GPT-4 がもっともらしいが不正確な説明（幻覚的説明）を生成するリスクも明らかになっており、解釈の信頼性検証が新たな課題となっている。6.2 概念ボトルネックモデル（CBM）の拡張 Concept Bottleneck Models (CBM) は、ニューラルネットワークの予測プロセスを「入力 → 概念 → 出力」という段階に分けることで透明性を高める手法である。CB-LLM: 最新の研究では、LLM を用いて中間層の「概念」を自動的に発見・拡張する試みが行われている 35。Steerability（操舵性）: さらに、特定の概念（例：「攻撃性」）に対応するニューロンの活性値を人為的に操作することで、モデルの生成内容を制御する「Steerability」の実証も進んでいる 37。これは、LLM の安全性を高めるための有望なアプローチである。6.3 デジタル・ヒューマニティーズと社会科学における応用人文学や社会科学の分野では、LLM を質的データの分析補助、すなわち「概念キュレーター」として活用する動きが活発化している。認識論的課題: LLM は大量のテキストを高速にコーディング・分類できるが、その出力は学習データに含まれるバイアス（例：西洋中心主義的な概念枠組み）を反映しやすい 38。人間参加型アプローチ: そのため、LLM を単独で使用するのではなく、研究者が LLM の提案する概念を批判的に検証し、対話的に修正する「Human-in-the-Loop」アプローチが推奨されている。これにより、LLM は研究者の「対話パートナー」として、新たな視点やパターン発見を支援するツールとなり得る。7. 結論本調査により、2020 年から 2025 年にかけての LLM 研究は、単なる性能向上から、**「理解の質」と「内部メカニズムの解明」**へとその重心を移していることが確認された。文脈理解: LLM は表面的な流暢さにおいて人間を凌駕するが、皮肉や含意といった語用論的領域や、小説全体のような長文脈の大域的整合性においては、依然として人間との間に越えがたいギャップが存在する。意味抽出と要約: ROUGE のような古典的指標は役割を終え、LLM-as-a-Judge による意味的評価が主流となった。また、STRUM-LLM や UIEFID に見られるように、構造化処理を取り入れることで、情報の比較や事実性の担保が強化されている。XAI と内部表現: H-Neurons の発見や PRISM による多義性の解明は、LLM をブラックボックスから「ガラスボックス」へと変えるための重要な一歩である。これらは、モデルがなぜ幻覚を見るのか、どうすれば制御できるのかという問いに対する物理的な解を与えつつある。今後の展望とギャップ:XAI 概念キュレーションタスクへの適用において、最大のギャップは**「Ground Truth（正解データ）の不在」**である。LLM が生成した「ニューロンの説明」や「抽出された社会的概念」が真に妥当であるかを検証する客観的な尺度が不足している。今後は、自動生成された説明の「もっともらしさ（Plausibility）」と「忠実性（Faithfulness）」を厳密に区別し、検証するための人間中心の評価プロトコル確立が急務である。表 1: 主要な要約評価指標と LLM 時代の課題指標カテゴリ具体例測定対象 LLM 要約における課題 n-gram ベース ROUGE, BLEU 参照要約との単語重複率抽象的要約（言い換え）を不当に低く評価する。GPT-4 等の流暢さを反映できない。埋め込みベース BERTScore ベクトル空間での意味的類似度 n-gram よりは改善したが、微細な事実誤認や論理的矛盾を捉えきれない。モデルベース (LLM-as-a-Judge)G-Eval, GPTScore 一貫性、整合性、流暢さ、関連性人間評価と高い相関を持つが、計算コストが高く、バイアス（自己選好など）のリスクがある。事実性特化 UIEFID, FIB 情報の正確性、原文との整合性推論と抽出を分離することで精度向上。構造化データの活用が鍵。表 2: PUB ベンチマークにおける主要タスクと LLM の課題カテゴリタスク例 LLM の現状と課題含意 (Implicature)間接的応答の解釈字義通りの意味に固執し、拒絶や肯定のニュアンスを読み違えることが多い。皮肉 (Sarcasm)皮肉の検出文脈、常識、話者の性格を統合する「心の理論」が不足しており、検出精度が低い。直示 (Deixis)指示語の特定局所的な参照関係の解決は比較的得意だが、文脈が長いと混乱が生じる。前提 (Presupposition)前提知識の理解一般常識に基づく前提は理解するが、特定の文脈依存的な前提には弱い。

文脈理解能力に関する研究

近年、LLM の高度な言語理解能力が注目されているものの、複雑な文脈理解には限界も指摘されている。Zhu ら(2024)はコア参照解決や対話状態追跡、暗黙の談話関係推定などのタスクで LLM を検証し、事前学習のみの LLM はファインチューニングモデルに比べて微妙な文脈特徴の理解に苦戦することを報告している ￼。Dentella ら(2024)も複数の文章読解課題で LLM はほぼチャンスレベルの成績であり、人間より劣ると指摘し、人間同等の意味理解には至っていないと結論付けている ￼。さらに、LLM は間接的含意（manner implicatures）のような暗黙の意味を捉える能力も限定的で、Cong ら(2024)は諸タスクで LLM の予測性能がほぼ確率的（チャンス）に近いことを明らかにしている ￼。Ma ら(2025)もシナリオ認知課題で「LLM は表面的な丸暗記に頼り、真の意味的シナリオ理解には失敗する」と報告しており、暗黙的意味把握に課題が残る ￼。一方、レビュー論文では LLM の Transformer 構造が長距離依存や語間の意味的連関を捉える点で優れており、「文脈理解能力において著しい利点がある」と評価されている ￼。
• Zhu et al. (2024) は、文脈理解評価ベンチマークを導入し、事前学習モデルはファインチューン済モデルに比べて微妙な文脈特徴の理解が劣ると報告 ￼。
• Dentella et al. (2024) は、言語理解タスクで LLM の回答は人間評価から大幅に劣ることを示し、「LLM は根本的な言語理解に達していない」と結論づけた ￼。
• Cong et al. (2024) は、社会言語学的含意（manner implicature）の理解では LLM の性能が確率的でしかなく、最新モデルでもほとんど改善が見られなかったと報告 ￼。
• Ma et al. (2025) はシナリオ認知タスクで LLM が表面的なパターンに頼ることを示し、「堅牢な意味的認知能力は得られていない」と指摘している ￼。
• Lin et al. (2025) のレビューでは、LLM（Transformer）は長距離依存関係や意味連関を優れた精度で捉え「文脈理解能力において顕著な利点」を示すと論じている ￼。

主要な発見: LLM は多くの文脈依存タスクで高い性能を示す一方で、コア参照や談話解析のような微妙な文脈関係や間接含意の把握には苦手意識がある。また、最新研究からは「LLM は表層的な丸暗記に依存しがちで深い意味理解には限界がある」という指摘が相次いでいる ￼ ￼。

意味パターン抽出に関する研究

LLM による意味パターン抽出研究は、複数文章から共通概念やパターンを自動的に抽出する試みが中心である。Lam ら(2024)は LLM を活用した概念帰納手法(LLooM)を提案し、従来のトピックモデルでは出力されにくい高次の意味概念（例：「伝統的性役割への批判」）を生成した ￼。実験では、LLooM で得られた概念は従来のモデルを上回る品質とデータカバレッジを示しており ￼、複数文書に共通する深い意味パターンを捉える効果が示された。
• Lam et al. (2024, CHI) は LLM を用いて生データから高レベル概念を帰納し、従来のトピックモデルでは得られない「女性に対する攻撃的言説」といった概念を抽出できた。評価では LLooM の概念が従来モデルより品質・カバレッジ共に優れると報告されている ￼ ￼。
• （関連分野）Greenwald et al. (2024) の AutoMolCo では、LLM を用いて分子の予測に必要な概念・ラベルを自動生成し、単純モデルで GNN を上回る性能を達成した ￼。これは自然言語ではないが、LLM によるパターン（概念）抽出の一例である。

主要な発見: LLM は複数文書から抽象度の高い共通概念を抽出する能力を示しており、従来手法より解釈可能な概念群を得られる可能性がある。例えば、LLooM により得られた概念は人間の解釈に近いものであり、従来モデルと比較して質的に優れていた ￼ ￼。

自然言語要約能力に関する研究

LLM は抽象的要約タスクでも顕著な性能を示すことが報告されている。Parente ら(2024)は ChatGPT による医学論文抄録の要約を評価し、要約文は元抄録の約 70%短縮にもかかわらず、人間評価で「高品質（中央値 90/100）」「高精度（92.5/100）」と非常に高く評価された ￼。対話要約では Zhou ら(2023)が GPT-4 等を用いた実験を行い、生成された要約は人間評価でタスク専用モデルや既存要約より好まれた ￼。一方、Sharma ら(2025)の評価では、オープンソース LLM によるゼロショット要約はドメインによって性能差があり、CNN/DM など新聞記事では専用モデル(Pegasus)にやや劣る結果だったが、追加訓練なしに多領域で統一的に対応可能な利点が指摘された ￼。
• Parente et al. (2024, Ann Fam Med) は ChatGPT が医学論文抄録を 70%短縮して要約し、その品質(90/100)・正確度(92.5/100)が非常に高いことを報告 ￼。
• Zhou et al. (2023, INLG) は ChatGPT/GPT-4 が対話要約を生成し、人間評価で専門システムやリファレンス要約より優先されたことを示した ￼。
• Sharma et al. (2025) は各種 LLM の要約性能を比較し、ニュース記事では Pegasus が LLM よりわずかに高得点だったが、LLM はドメイン横断的に利用可能であると指摘している ￼。

主要な発見: LLM は要約タスクで高い一貫性と意味的正確さを示し、実用水準の要約を生成できる例が多い。特に ChatGPT/GPT-4 は人間評価で高い品質と正確さを得ている ￼ ￼。一方で、専門モデルが最適な場合もあり、領域ごとの性能差が存在することが報告されている ￼。

対比的・比較的タスクにおける性能

LLM は複数文書の比較・対比タスクでも応用が検討されている。Luss ら(2024)は CELL という手法で、LLM 出力に対し「なぜこの応答になったか」を説明する対比的説明を提案した。具体的にはプロンプトをわずかに変更し、異なる応答が得られる理由を示すことで、LLM の振る舞いを説明している ￼。また、Megagon Labs の研究では、LLM に長文・多文書推論能力を要求するベンチマークを提示した。例えば、HoloBench では文書中の情報集約タスクで情報量が増えるにつれ性能が急激に低下することが明らかになった ￼。さらに、MCRank ベンチマークでは「複数の条件で項目をランク付け」する問題で、条件が増えるほど従来 LLM の性能が大幅に低下することが示された。新手法 EXSIR（推論の分解）を用いると、チェーン・オブ・ソート(思考鎖)手法より最大 14.4%の性能向上が得られた ￼。
• Luss et al. (2024) は LLM 出力のコントラスト説明手法を導入し、元プロンプトを少し変更すると LLM が異なる応答を返すことを利用して、「なぜ元の回答になったか」を説明する枠組みを提案した ￼。
• Megagon Labs (2024) は HoloBench/MCRank などの新ベンチマークを通じて、LLM が多数の文書や複雑な条件を扱うと性能が急落することを示し、条件を抽出・分解する EXSIR 法で 14%超の精度向上を報告した ￼ ￼。

主要な発見: 対比的説明や複数文書比較のタスクにおいて、標準 LLM は条件数や情報量の増加に敏感であることが明らかになっている ￼ ￼。一方、プロンプト工夫や推論分解など特化した手法を組み合わせることで、性能向上が可能であることも示された ￼ ￼。

XAI・概念キュレーションへの応用

説明可能 AI（XAI）や概念キュレーションにおいても LLM の利用が増えている。教育領域では Yang ら(2025)が GPT-3.5/GPT-4 を用いて講義情報から概念と関係性を自動抽出し、GPT-3.5 は定量評価で最良、GPT-4 は意味的に優れた概念を生成したが、生成結果は専門家によるチェックが必要であると報告した ￼。画像分類の解釈では Barua ら(2024)が GPT-4 で概念導出を試み、人手生成には及ばないものの従来手法(ECII)より人間に理解しやすい説明を作り出せることを示した ￼。また、Hoang-Xuan ら(2024)はマルチモーダル LLM を用い、畳み込みネットワークのニューロン応答から新規概念を自動発見・検証する手法を提案し、事前定義なしで意味的に解釈可能な概念を得られることを実証した ￼。さらに分子科学では Greenwald ら(2024)が AutoMolCo で LLM に分子概念と定量ラベルを自動生成させ、線形モデルで高性能な説明モデルを構築して GNN を上回る成果を上げている ￼。
• Yang et al. (2025) は GPT-3.5/GPT-4 を活用し、MOOC 講義情報から授業概念と相互関係を生成・抽出する実験を行った。GPT-3.5 は精度 F1 で最高スコアを得た一方、GPT-4 はより意味ある概念を生成したが、生成結果は「専門家の校正が必要」と結論付けられた ￼。
• Barua et al. (2024) はシーン分類タスクで GPT-4 に概念説明を生成させ、ヒューリスティック手法(ECII)より人間評価で優れた説明を得られたが、人手生成には及ばなかった ￼。
• Hoang-Xuan et al. (2024) はマルチモーダル LLM で CNN の個別ニューロンの機能を表す概念を発見する手法を提案し、未知の概念を自動発見・検証するパイプラインを実現した ￼。
• Greenwald et al. (2024) の AutoMolCo は LLM で分子の定量的概念(例:極性表面積など)を自動生成・ラベリングし、単純予測モデルで既存手法を上回る成果を示した ￼。

主要な発見: LLM は XAI 向け概念生成・ラベリングの自動化に有望な成果を示している。各種データ領域で、LLM が高レベルな概念や特徴記述を人手に近い形で生成し、高性能な説明モデルにつなげられる例が報告されている ￼ ￼。一方で、LLM 生成の概念出力には専門家の検証・修正が依然必要であり ￼ ￼、概念品質評価の標準指標や自動化パイプラインの整備が課題として残されている。

ギャップ: 現状の研究では、LLM は概念探索や命名に強みを示すが、完全自動化・高信頼化には課題がある。具体的には、生成された概念や説明は専門家による校正が必要であり ￼ ￼、生成結果と人手基準との乖離も指摘されている。概念の定量的評価指標や LLM 出力の信頼性保証メカニズムは未整備であり、LLM を用いた XAI 概念キュレーションタスクの実用化にはさらなる研究が求められる。

参考文献: 本調査で言及した論文には、Zhu et al. “Can Large Language Models Understand Context?” (2024) ￼、Dentella et al. “Testing AI on language comprehension” (2024) ￼、Cong et al. “Manner implicatures in LLMs” (2024) ￼、Lam et al. “Concept Induction with LLooM” (CHI 2024) ￼ ￼、Parente et al. “Quality, Accuracy, and Bias in ChatGPT Summarization” (2024) ￼、Zhou et al. “GPT vs Summarization Guidelines” (INLG 2025) ￼、Sharma et al. “LLM Summarization Evaluation” (2025) ￼、Barua et al. “Concept Induction using LLMs” (2024) ￼、Hoang-Xuan et al. “LLM-assisted Concept Discovery” (2024) ￼、Greenwald et al. “AutoMolCo” (2024) ￼、Yang et al. “LLM for Course Concept Extraction” (2025) ￼ などが含まれる。各論文の詳細は引用箇所をご参照いただきたい。
