概念特徴の自動命名に関する先行研究

非教師あり概念発見における自動命名
• Discover-then-Name CBM (DN-CBM) – Zhang ら（2024）が提案した手法では、CLIP の特徴空間に対して Sparse Autoencoder (SAE) を適用し、学習した辞書ベクトルを CLIP のテキスト埋め込み空間で最も近い語にマッチングすることで、抽出された概念を自動命名する ￼。例えば図示例では “colorful”, “spheres”, “fence” といった語が付与されており、概念名は入力画像の特徴と概ね一致している ￼ ￼。この手法では語彙として単語リストを用いるため、辞書にない微細な概念を表現できない制限があることも指摘されている ￼。
• 階層型 SAE による CLIP 解釈 – Zaigrajew ら（2025）は CLIP について階層的 SAE を学習し、高活性化画像を用いて概念を可視化した上で、同じく CLIP のテキスト空間を使ってそれらを自動命名している ￼。これにより顔や色、物体など 120 以上の概念が抽出・命名可能となり、言語的な説明にも活用している。
• UCBM（Unsupervised Concept Bottleneck Models） – Oikarinen ら（2024）は事前概念なしに NMF 等で概念ベクトルを抽出し概念ボトルネック層を構築する手法を提案したが、論文中で「発見された概念を命名するには手作業で最も活性化する画像を見て解釈しなければならず、解釈性の工程が遅くなる」としている ￼。したがって現状では人手による名称付けがボトルネックであり、多モーダルな大規模言語モデル（LLM）などによる自動命名の可能性が今後の課題とされている ￼。

メカニズム解釈における自動命名
• NeurFlow（Cao ら, 2025） – ニューラルネットの回路解析手法であり、各層の「コア概念ニューロン」を同一概念群 (neuronal concept group) としてクラスタリングし、これらの概念群間の機能的結合をグラフ化する。論文では 「Multimodal LLM を活用して概念に自動でラベルを付与する」 と明記しており、自動命名の実例として用いられている ￼ ￼。同報告の要約でも「自動概念ラベリングや関係説明にマルチモーダル LLM を利用」とされており ￼、回路上の各概念ノードに GPT-4 などで説明文を付与するアプローチが示されている。
• LLM 活用の概念発見（Hoang-Xuan ら, 2024） – 画像モデル中のニューロンに対して、活性化が高い入力画像群から概念を抽出し、多モーダル LLM（例えば GPT-4V など）でその概念の説明語を生成する手法を提案している ￼ ￼。事前の概念リストを必要とせず、画像の特徴から「open-ended」な概念を自動生成するもので、生成した概念に例/反例を用いた検証も組み込み、自動化の信頼性向上を図っている。
• GPT-4 によるニューロン説明（OpenAI, 2023） – GPT-2 の 30 万個以上のニューロンそれぞれについて、活性化パターンを GPT-4 に入力して自然言語で説明させる試みが行われている ￼。結果として 1,000 個以上のニューロンに対して高スコア（≥0.8）の説明が得られたが、その多くは「特に興味深い動作ではない」と評価されており ￼、LLM による自動説明にも限界があることが示唆された。
• その他の機構的解析手法では、回路探索や Attributions Graphs などで自動命名を謳うものはまだ少ない。多くは発見された特徴を可視化し研究者が名称を付与する流れが主流であり、NeurFlow のような LLM 知能を組み込む試みが先駆的である。

概念ベース XAI における自動命名
• 従来の TCAV/CBM – TCAV や従来型 CBM（Concept Bottleneck Model）は、花や生物部位など事前定義された人間概念を前提とするため、新たにモデル内部から発見した特徴に自動命名する仕組みは組み込まれていない。概念集合やラベルは人手で用意する必要があり、自動的に命名する研究はされていない。
• 最近の概念学習モデル – 一部の研究では LLM や視覚言語モデル（例：CLIP）を用いてタスクごとの概念集合を生成し概念ボトルネックを構築する試みがある ￼ ￼。しかしこれらはあくまでタスクに関連する既知概念を LLM に問い合わせる手法であり、モデルが実際に学習している未知の概念を自動発見して命名するものではない ￼。また ACE（Automatic Concept-based Explanation; Ghorbani ら 2019）など自動概念抽出の手法はあるが、抽出後の名称付けは人手または既存のデータセットに依存している。
• 概念キュレーションの自動化 – 上記の通り、現在のところ概念ベース XAI で概念のキュレーションや命名を完全自動化する研究は確認できていない。データ駆動的に概念を収集・整理する試みはあるものの、最終的な命名には人手や限定された語彙リストに頼る場合が多い。

LLM を用いた概念命名
• Vision 系への応用 – Hoang-Xuan ら（前節）や Zaigrajew ら（2025）の例に見るように、視覚モデルにおいても多モーダル LLM や CLIP のテキスト空間を利用して概念を言語化する研究が進んでいる ￼ ￼。これらは人間が知らない新概念も含めてオープンエンドに命名する方向を示しており、有望な手法といえる。
• 言語モデル内部の概念命名 – OpenAI（2023）の研究では、まさに LLM 自身のニューロンを別の LLM で解釈し説明文を生成した ￼。同様に Bills ら（2023）は GPT-2 ニューロンに対し GPT-4 で説明を生成・評価し、概念的理解を自動化する枠組みを提示している ￼。
• 課題と今後 – 多数の研究で LLM による命名が試されているものの、完全自動化には課題が残る。例えば DN-CBM では単語リストの大きさによって命名精度が左右される ￼、GPT-4 では有用な説明を得られるニューロンは限られる ￼ など、語彙の網羅性・曖昧性や出力信頼度の問題が指摘されている。現状では LLM の導入により概念命名の自動化可能性は広がりつつあるものの、人手による検証や限定された辞書による補強なしには完全な自動化は難しい状況と言える。

代表的な自動命名パイプラインの要約

| 手法                                    | 入力・対象                                         | 自動命名のやり方                                                                       | 自動スコアリング・評価指標                                             | 内部表現アクセス       |
| --------------------------------------- | -------------------------------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------- |
| Network Dissection (Bau et al., 2017)   | CNN の中間層ユニット + Broden データセット         | 各ユニットの活性化マップと概念マスクの IoU を計算し，最大 IoU の概念ラベルを自動付与   | IoU 閾値を満たす「概念検出ユニット」の個数など，解釈可能性スコアを定義 | 要                     |
| CLIP-Dissect (Oikarinen & Weng, 2022)   | 視覚モデルのユニット + CLIP 画像・テキスト埋め込み | 高活性化画像を CLIP で埋め込み，多数のテキスト候補との類似度が最大の語句で命名         | CLIP 類似度スコア，人手評価（説明の妥当性）                            | 要                     |
| Label-Free CBM (Oikarinen et al., 2023) | 画像データセット + 基盤モデル（CLIP 等）           | GPT-3 にタスク説明を与えて候補概念リストを生成し，CLIP でフィルタして概念名とする      | 概念予測精度と下流分類精度（CBM としての性能）                         | 準要（ボトルネック層） |
| Discover-then-Name (Rao et al., 2024)   | 画像エンコーダ特徴（例：CLIP）                     | SAE/NMF で抽出した概念ベクトルに対し，高活性化画像と CLIP/LLM を使って語や句を自動付与 | CBM としての分類性能，再現研究での整合性チェック，一部人手評価         | 要                     |
| OpenAI/Anthropic Auto-Interpretability  | LLM のニューロンや SAE 特徴                        | トップ発火トークン／文を LLM（GPT-4 など）に提示し，自然言語の説明文を生成             | Simulation Score，検出タスクなどで説明の妥当性を自動スコアリング       | 要                     |

本研究とのギャップ（簡約）

- 入力単位・前提の違い
  - 上記の多くは画像モデルや LLM の内部アクティベーションを直接入力とし，ユニット／特徴ごとの命名を行うのに対し，本研究は「概念を持つテキスト集合 vs 持たない集合」というサンプル集合レベルを入力とするため，モデル内部へのアクセスを必須としない。
- 評価軸の違い
  - 既存研究は IoU，CLIP 類似度，Simulation Score など内部挙動ベースの自動指標が中心であるのに対し，本研究は既存アスペクトラベルを外部ゴールドとみなし，BERT スコアや BLEU による意味的一致度で評価する点が異なる。
- タスク設計・用途の違い
  - 既存はニューロン単位の解釈や CBM の構築を主目的とするのに対し，本研究はアスペクト付きレビューという実務寄りデータに対し，「対比因子生成タスク」というベンチマークタスクを定義し，LLM を汎用的な命名モジュールとして評価する点に特徴がある。

まとめ

近年、多くの研究者がモデル内部の概念特徴に自然言語ラベルを与えようと試みている。特に CLIP の埋め込み空間や多モーダル LLM を利用した手法で、抽出した特徴ベクトルに対して自動的に語を当てる試みが報告されている ￼ ￼。しかしこれらはいずれも部分的な自動化に留まっており、語彙制限や生成品質の問題から人手介入が残っている ￼ ￼。したがって、「モデル内概念特徴の命名プロセスを完全自動化した研究はまだない」という主張は正確ではなく、既にいくつかのアプローチが提案されている。ただし実用面では未解決の課題が多く、今後の研究で自動化の限界と実用性がさらに明らかになると考えられる。

参考文献: 最近の論文やオープンソース研究資料を参照し、上述の手法や指摘点をまとめた。例えば、Zhang et al. (2024) ￼ や Cao et al. (2025) ￼、Hoang-Xuan et al. (2024) ￼、OpenAI (2023) ￼ ￼ などが挙げられる。各出典は本文中に示した形式で引用している。
概念特徴の自動命名：ニューラルネットワーク内部表現の解釈可能性における自動化技術の包括的調査と実現可能性の検証エグゼクティブサマリー本報告書は、深層学習モデル内部で発見された概念特徴に対する自然言語での命名プロセス（自動概念命名：Automated Concept Naming）に関する研究の現状を網羅的に調査し、その技術的到達点と限界を体系化したものである。「モデル内部で発見された概念特徴に対する自然言語での命名プロセスの自動化への取り組みはまだなく，どの程度自動化可能であるのかも明らかではない」という検証対象の仮説に対し、本調査は明確な反証を提示する。調査の結果、2020 年以降、特に 2023 年から 2025 年にかけて、概念特徴の自動命名は急速に発展している研究領域であることが確認された。初期の教師ありデータセットを用いた分類的アプローチから、CLIP などのマルチモーダルモデルを用いた検索ベースのアプローチ（Retrieval-Based）、さらには大規模言語モデル（LLM）を用いた生成的アプローチ（Generative Approach）へと技術的なパラダイムシフトが起きている。現在では、OpenAI や Anthropic、EleutherAI などの主要な AI 研究所が、数百万規模の特徴量に対して自動的に説明文を生成・検証するパイプラインを構築しており、一定の精度で自動化が可能であることが実証されている。しかしながら、自動化の「程度」に関しては、依然として重要な課題が残されていることも明らかになった。多義性を持つニューロン（Polysemantic Neurons）の解釈の困難さ、人間が言語化できない「エイリアン概念」の存在、そして LLM によるもっともらしい誤説明（幻覚）のリスクなどである。本報告書では、メカニスティック解釈（Mechanistic Interpretability）、概念ベース XAI（Concept-based XAI）、非教師ありコンセプト発見の各領域における最新手法を詳解し、定量的評価指標を用いた自動化の妥当性検証について論じる。1. 序論：解釈可能性の壁と自動命名の必要性 1.1 背景：ブラックボックス問題とスケーラビリティの壁深層ニューラルネットワーク（DNN）、特に近年の大規模言語モデル（LLM）や Vision Transformer（ViT）は、その卓越した性能と引き換えに、内部動作が人間にとって不可解な「ブラックボックス」であるという問題を抱えている。モデルは高次元のベクトル空間（潜在空間）において情報を処理しており、その内部表現（アクティベーション）は単なる浮動小数点数の羅列に過ぎない。これらの数値がどのような意味的・論理的処理を表しているのかを理解することは、AI の安全性、信頼性、および制御可能性を担保する上で喫緊の課題となっている。従来、モデルの内部表現を解釈するために、研究者は手作業による分析を行ってきた。特定のニューロンが最大発火する入力データ（画像やテキスト）を観察し、人間がその共通項を推論して「このニューロンは犬の顔に反応している」「このニューロンは否定的な感情をコードしている」といったラベル（名前）を付与するプロセスである。しかし、現代の LLM は数千億から数兆のパラメータを持ち、中間層の特徴量も膨大である。これら全てを手作業で分析・命名することは物理的に不可能であり、解釈可能性研究における「スケーラビリティの壁」となっている。1.2 検証対象仮説の意義本調査において検証対象とされた仮説「モデル内部で発見された概念特徴に対する自然言語での命名プロセスの自動化への取り組みはまだなく，どの程度自動化可能であるのかも明らかではない」は、このスケーラビリティの壁を突破するための核心的な問いを含んでいる。もし自動化への取り組みが存在しないのであれば、解釈可能性研究は永遠に小規模なトイモデルの分析に留まることになる。逆に、もし自動化技術が存在し、それが実用レベルに達しつつあるならば、AI の透明性は飛躍的に向上する可能性がある。本報告書では、この仮説を検証するために、以下の 4 つの主要な研究領域を深掘りする。非教師ありコンセプト発見における自動命名：ラベルデータを持たない状況で、モデルが学習した概念をいかに抽出し、名付けるか。メカニスティック解釈における自動命名：モデルの「回路」や「歯車」にあたる微細な構成要素（ニューロンや Attention Head）の機能を、いかに自動的に記述するか。概念ベース XAI における自動命名：Concept Bottleneck Model (CBM) 等のアーキテクチャにおいて、ボトルネック層の概念をいかに自動生成するか。LLM を用いた概念命名：高度な言語能力を持つモデル自身を「解釈者」として利用するメタ的なアプローチ。2. 自動命名技術の系譜と主要パラダイム自動命名技術の進化は、大きく分けて「固定オントロジーへのマッピング（第 1 世代）」、「マルチモーダル検索によるゼロショット命名（第 2 世代）」、「LLM による生成的記述（第 3 世代）」の 3 段階で捉えることができる。本章ではこの技術的変遷を概観し、現在のアプローチがどのように形成されたかを論じる。2.1 第 1 世代：教師ありアライメントと固定語彙 2017 年から 2020 年頃までの初期の研究において、概念の「命名」は実質的に既知の概念セットへの「分類」と同義であった。代表的な手法である Network Dissection (Bau et al., 2017) 1 は、CNN の解釈において画期的な役割を果たした。この手法では、Broden Dataset と呼ばれるテクスチャ、物体、パーツ、シーンなどのラベル付きセグメンテーションデータセットを用意し、個々のニューロンの活性化マップと、データセット内の概念マスクとの IoU（Intersection over Union）を計算する。もしあるニューロンの活性化領域が「犬」のマスクと高い一致率を示せば、そのニューロンには自動的に「犬（dog）」という名前が付与される。また、ACE (Automated Concept-based Explanations) (Ghorbani et al., 2019) 2 は、非教師あり学習的に画像セグメントをクラスタリングすることで概念を「発見」する手法を提案したが、その発見された概念に名前を付ける段階では、依然として類似画像検索や人間による事後的な解釈に依存する側面が強かった。これらの手法の限界は、**「事前に用意されたラベルセット（オントロジー）に含まれない概念は名付けられない」**という点にある。モデルが未知の概念や、より抽象的な特徴（例：「悲しげな雰囲気」や「特定の画風」）を学習していたとしても、それを記述する語彙がデータセットになければ、システムは沈黙するか、誤ったラベルを割り当てることしかできない。2.2 第 2 世代：CLIP を用いた検索ベースのアプローチ（Retrieval-Based）2021 年の CLIP (Contrastive Language-Image Pre-training) の登場は、自動命名に革命をもたらした。画像とテキストを共通の埋め込み空間（Embedding Space）に射影することで、事前のラベル付けなしに、あらゆるテキスト記述との類似度を計算可能になったからである。CLIP-Dissect (Oikarinen & Weng, 2023) 4 はこのパラダイムの代表例である。この手法では、まず分析対象のニューロンが強く反応する画像群を特定する。次に、数万語からなる広範な「概念バンク（Concept Bank）」を用意し、それぞれの単語を CLIP のテキストエンコーダでベクトル化する。ニューロンの反応画像のベクトルと、概念バンク内の単語ベクトルのコサイン類似度を計算し、最も類似度の高い単語をそのニューロンの「名前」として採用する。これにより、固定されたデータセットによる制約から解放され、辞書にある言葉であればどのような概念でも「発見・命名」できるようになった。これを Zero-Shot Concept Naming と呼ぶ。2.3 第 3 世代：LLM による生成的アプローチ（Generative Approach）2023 年以降、GPT-4 や Claude 3 といった高度なマルチモーダル LLM の登場により、自動命名は「検索」から「生成」へと進化した。これは、あらかじめ用意された単語リストから選ぶのではなく、モデルの挙動を観察した LLM が、その機能を説明する自然言語の文をゼロから記述するアプローチである。MILAN (Mutual-Information-guided Linguistic Annotation of Neurons) 6 や、OpenAI による Automated Interpretability 8 がこれに該当する。これらの手法では、解釈対象のニューロンが反応するテキストや画像を LLM に入力し、「このニューロンがどのようなパターンに反応しているか説明せよ」とプロンプトを与える。生成された説明は、単なる名詞（例：「犬」）に留まらず、関係性や文脈（例：「文末における否定的な形容詞」や「水面にある物体の反射」）を含むリッチな記述が可能となる。現在、この生成的アプローチが自動命名研究の最先端（State-of-the-Art）となっており、特にメカニスティック解釈の分野で大規模に展開されている。3. 領域別詳細調査：メカニスティック解釈における自動命名メカニスティック解釈（Mechanistic Interpretability）は、ニューラルネットワークを逆コンパイルし、人間が理解可能なアルゴリズムとして記述することを目的とする分野である。この領域における自動命名は最も活発であり、技術的な洗練度も高い。3.1 課題：多義性（Polysemanticity）と重ね合わせ（Superposition）自動命名における最大の障壁の一つが、**多義性（Polysemanticity）**である。初期の研究において、一つのニューロンは一つの概念に対応している（単義的：Monosemantic）という期待があったが、実際には多くのニューロンが全く無関係な複数の概念に反応することが明らかになった 9。例えば、あるニューロンが「学術論文の引用」と「イングリッシュマフィン」の両方に反応する場合、これに「学術的マフィン」のような単一の名前を付けることは不可能であり、命名システムは混乱する。この現象は 重ね合わせ（Superposition） と呼ばれ、モデルが限られたニューロン数でより多くの特徴を表現するために、特徴量をニューロンの線形結合として圧縮していることに起因すると考えられている。したがって、ニューロンそのものに名前を付けるのではなく、ニューロンの活動を分解し、真の「特徴（Feature）」を取り出してから命名する必要が生じた。3.2 スパースオートエンコーダ（SAE）による特徴分解と命名この問題に対処するために導入されたのが スパースオートエンコーダ（Sparse Autoencoder: SAE） である。Anthropic や EleutherAI、DeepMind などの研究グループは、LLM の中間層の出力（アクティベーション）を入力とし、それをより高次元（例：元の 16 倍〜128 倍）のスパースな潜在空間に写像してから再構成する SAE を学習させた 11。SAE の各潜在変数は、元のニューロンとは異なり、驚くほど単義的（Monosemantic）であることが確認されている。これにより、自動命名の精度は飛躍的に向上した。具体的なプロセスは以下の通りである。特徴抽出: SAE を用いて、LLM の挙動を数百万のスパースな特徴に分解する。最大発火例の特定: 各特徴について、それが強く発火するテキストトークンの列を収集する。LLM による命名（Auto-Interpretability）: GPT-4 や Claude などの「解釈者モデル」に、収集したトークン列を提示し、「これらの例に共通するパターンは何か？」と問いかける。検証: 解釈者モデルが生成した説明（名前）に基づき、新たなテキストに対する発火を予測させ、その精度をスコアリングする。Anthropic の研究報告 11 によれば、この手法を用いて Claude 3 Sonnet 等のモデル内部から「ゴールデンゲートブリッジに関する言及」や「コード内の特定のバグ」といった極めて具体的な概念特徴を抽出し、自動的に命名することに成功している。3.3 自動解釈可能性スコア（Auto-Interpretability Score）命名が「正しい」かどうかを自動的に判定する指標も確立されている。これは、AI が AI を説明し、その説明を AI が採点するという完全な自動ループを構成する。指標名内容提唱者/使用例 Simulation Score 生成された説明文に基づき、LLM がニューロンの挙動（発火/非発火）をシミュレーションする。実際のニューロンの挙動との相関（ピアソン相関など）をスコアとする 8。OpenAI, AnthropicFuzzing / Detection 説明文が正しいならば、LLM はその特徴が発火するテキストとしないテキストを識別できるはずである。発火トークンと非発火トークン（あるいは類義語など）を提示し、正しく分類できるかを F1 スコアで評価する 12。EleutherAI これらのスコアは、自動命名の信頼性を定量的に保証するものであり、「どの程度自動化可能であるか」という問いに対する直接的な回答となっている。現状では、多くの特徴に対して高いスコアが得られる一方、抽象的すぎる特徴や文脈依存性の高い特徴についてはスコアが低くなる傾向があり、これが現在の技術的限界点である。4. 領域別詳細調査：概念ベース XAI における自動命名 Concept Bottleneck Models (CBMs) は、入力（画像など）からまず「概念」を予測し、その概念を用いて最終的なクラス（品種など）を予測する解釈可能なアーキテクチャである。従来、この中間概念は人間が定義し、ラベル付けを行う必要があったが、ここでも自動化が進んでいる。4.1 Label-Free CBM：概念バンクの自動生成 Label-Free CBM (Oikarinen et al., 2023) 18 は、概念ラベル付きデータを一切使用せずに CBM を構築するフレームワークである。ここでの「自動命名」は、概念の候補リスト（概念バンク）を作成するプロセスに適用される。GPT-3 による概念生成: 対象となるデータセット（例：鳥の分類）に対し、GPT-3 に「鳥を見分けるために重要な視覚的特徴を挙げよ」といったプロンプトを入力し、数百〜数千の候補概念（「くちばしの形」「翼の色」など）を生成させる。CLIP によるフィルタリング: 生成された概念リストから、冗長なものや視覚的に識別困難なものを CLIP の埋め込み空間での類似度に基づいて削除する。射影: 画像の特徴量を、残った概念のベクトル空間に射影する。この手法により、人間が概念を設計する必要がなくなり、ドメイン知識がない分野でも解釈可能なモデルを構築できるようになった。ここでの「命名」は、LLM が持つ一般的知識を特定のタスクに転移させるプロセスと言える。4.2 Discover-then-Name (DN-CBM)：発見してから名付けるさらに進んだアプローチとして、Discover-then-Name (DN-CBM) (Rao et al., 2024) 20 がある。Label-Free CBM が「先に名前（概念リスト）を作ってから画像を当てはめる」のに対し、DN-CBM は「まずデータから概念を発見（Discover）し、その後に名前を付ける（Name）」アプローチを採る。Discover: 画像エンコーダ（CLIP Vision Encoder など）の特徴量に対し、スパースオートエンコーダ（SAE）や非負値行列因子分解（NMF）を適用し、データセット内に内在する概念方向（ベクトル）を教師なしで抽出する。Name: 抽出された概念ベクトルに対応する「名前」を自動的に付与する。検索ベース: 概念ベクトルと CLIP のテキストエンコーダの語彙との類似度を計算する。生成ベース: その概念ベクトルが強く反応する画像を複数枚選び、マルチモーダル LLM（GPT-4V など）に入力して「これらの画像に共通する概念を一言で表せ」と指示する。この手法は、モデルが実際に学習している特徴を忠実に反映できる（Fidelity が高い）という利点がある。Label-Free CBM では、LLM が生成した概念リストの中に、モデルが実際に注目している特徴が含まれていない可能性があるが、DN-CBM ではモデル内部の特徴そのものを出発点とするため、その乖離が少ない。4.3 再現性と信頼性の課題しかし、DN-CBM に関する再現研究 22 では、自動付与された名前と、実際のニューロンが反応している視覚的特徴との間に不一致（Misalignment）が見られることが報告されている。例えば、自動命名システムが「タイヤ」と名付けた特徴が、実際には「円形の黒い物体全般」に反応している場合などである。これは、自動命名が「もっともらしい嘘（Hallucination of Interpretability）」をつくリスクを示唆しており、命名結果を鵜呑みにせず、検証することの重要性を浮き彫りにしている。5. 領域別詳細調査：LLM を用いた概念命名メカニズムと応用 5.1 MILAN：視覚的特徴の生成的記述 MILAN 6 は、視覚モデルのニューロンに対して、単語レベルではなく自然言語の「文」による説明を生成する先駆的な研究である。手法: ニューロンが活性化する画像領域（Image Croppings）を入力とし、その領域を説明するテキストを出力する条件付き言語モデルを学習させる。学習には「MILANNOTATIONS」と呼ばれる、人間が作成したニューロン説明データセットを用いる。特徴: 検索ベース（CLIP-Dissect）と異なり、「〜の上にある」「〜のような質感の」といった構成的な（Compositional）記述が可能である。限界: 学習データの質と量に依存するため、学習データに含まれない全く新しいタイプの視覚的抽象概念を記述する能力には限界がある 23。5.2 Concept Layers (2025)：概念による制御と介入最新の研究トレンドとして、2025 年に発表された Concept Layers 24 が挙げられる。これは、LLM の解釈可能性を「観察」から「介入（Intervention）」へと拡張する試みである。概要: 既存の LLM の層に、解釈可能な「概念層」を挿入する。この層は、潜在状態を人間が理解可能な概念（例：「形式ばった表現」「攻撃性」「専門性」）の軸に射影し、また戻す役割を果たす。命名の自動化: どの概念軸が必要かは、タスクに応じて LLM 自身がオントロジー探索を行って決定する。つまり、タスクを与えると、LLM が「このタスクの制御に必要な概念変数は X, Y, Z である」と自動的に定義・命名し、その操作用インターフェースを構築する。意義: これにより、自動命名された概念は単なるラベルではなく、ユーザーがモデルの出力を調整するための「ツマミ（Control Knob）」として機能するようになる。5.3 構成的概念抽出（Compositional Concept Extraction: CCE）CCE 26 は、概念の「構成性（Compositionality）」に着目した研究である。単純な概念（「赤い」「立方体」）の組み合わせとして複雑な概念を表現できるような特徴表現を自動発見することを目指す。GPT-4o による命名: CCE では、発見された概念軸の解釈に GPT-4o を用いている。トップ 20 の活性化画像を含むグリッド画像を生成し、GPT-4o に詳細なキャプション生成と概念名の特定を行わせるという手法を採用しており、視覚的プロンプティングによる自動命名の実例となっている。6. 自動命名の実現可能性と限界の評価これまでの調査に基づき、自動命名の実現可能性（Feasibility）と限界について総合的に評価する。6.1 実現可能性：どの程度自動化可能か？「どの程度自動化可能か」という問いに対しては、**「人間が言語化可能な概念（Human-Aligned Concepts）については、高い精度で完全自動化が可能である」**と結論付けられる。単義的な特徴: 具体的な物体、特定の文法構造、固有名詞などに反応する特徴については、SAE と LLM を組み合わせたパイプラインにより、人間と同等かそれ以上の精度で命名が可能である。スケーラビリティ: Anthropic の事例 11 に見られるように、数百万〜数千万の特徴量に対して並列的に命名を行う計算資源と手法は既に確立されている。6.2 現在の技術的限界一方で、以下の領域においては自動化の限界、あるいは失敗のリスクが確認されている。1. エイリアン概念（Alien Concepts）と記述不可能性モデルは、人間には理解不能な、あるいは自然言語に対応する単語が存在しない統計的特徴（ヒューリスティクス）を学習することがある。これを「エイリアン概念」と呼ぶ。自動命名システム（LLM）は、これらに対しても無理やりもっともらしい名前（幻覚）を付ける傾向がある。結果として、「名前は付いているが、その名前と実際の挙動が一致していない」という状況が生まれ、解釈可能性をミスリードする危険性がある 22。2. 多義性の残留 SAE による分解が進んでいるとはいえ、依然として完全に分離しきれない多義的特徴は存在する。LLM が生成する説明が「A または B、あるいは文脈によっては C」といった曖昧なものになり、概念としての有用性が低下する場合がある。3. 計算コスト数百万の特徴量に対し、高性能な LLM（GPT-4 など）を用いて推論・検証を行うコストは極めて高い。OpenAI もこれを「計算集約的（Compute Intensive）」と認めており 30、実用化に向けたボトルネックの一つとなっている。軽量なモデルでの蒸留や、効率的なスコアリング手法（Intervention Scoring など 11）の研究が進められている。6.3 定量的評価のまとめ各手法の信頼性を担保するための評価指標も整備されている。評価の観点指標内容信頼性予測能力 Simulation Score / Auto-Interp Score 名前から挙動を予測できるか高（因果的検証に近い）識別能力 Detection / Fuzzing Score 名前で発火例を識別できるか中〜高意味的整合性 CLIP Similarity / Soft-WUP ベクトル空間での距離中（荒い相関しか見れない場合あり）人間評価 Human Rating 人間が見て納得できるか高（ただしスケーラビリティなし）7. 結論本調査の結果、「モデル内部で発見された概念特徴に対する自然言語での命名プロセスの自動化への取り組みはまだなく，どの程度自動化可能であるのかも明らかではない」という仮説は、明確に否定される。取り組みの有無: 既に多数の先行研究が存在する。2017 年の Network Dissection に始まり、CLIP を用いた検索ベース手法（CLIP-Dissect, Label-Free CBM）、そして LLM を用いた生成的・検証的手法（OpenAI/Anthropic Auto-Interpretability, Discover-then-Name）へと、技術は着実に進化している。自動化の程度:手法: 教師なしで概念を発見し、LLM を用いて自然言語で命名し、その命名の正確性をシミュレーションによって数値化するという完全自動化されたパイプラインが確立されている。精度: 人間が理解可能な概念については高い精度で命名可能であるが、モデル特有の非人間的な特徴（エイリアン概念）については、LLM が誤った解釈を生成するリスクが残る。規模: 数百万個の特徴量に対して適用可能なスケーラビリティが実証されている。今後の展望:現在、研究の焦点は「命名ができるか」というフェーズを超え、「その名前は信頼できるか（Faithfulness）」、「その名前を使ってモデルを制御できるか（Intervenability）」というフェーズに移行している。2025 年の Concept Layers や、SAE を用いた介入実験は、自動命名された概念が単なる受動的なラベルではなく、AI モデルを人間が操作するためのインターフェース（API）となる未来を示唆している。したがって、今後の研究においては、自動命名そのものの可能性を問うのではなく、「自動生成された説明の因果的な正しさをいかに保証するか」、あるいは**「言語化できない特徴をいかに扱うか」**という点に焦点を当てることが推奨される。

arxiv.org
Deciphering Functions of Neurons in Vision-Language Models - arXiv
新しいウィンドウで開く

arxiv.org
[1902.03129] Towards Automatic Concept-based Explanations - arXiv
新しいウィンドウで開く

ris.utwente.nl
Explainable AI in medical imaging - https ://ris.utwen te.nl
新しいウィンドウで開く

arxiv.org
Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs - arXiv
新しいウィンドウで開く

arxiv.org
Label-free concept bottleneck models. - arXiv
新しいウィンドウで開く

milan.csail.mit.edu
Natural Language Descriptions of Deep Visual Features
新しいウィンドウで開く

openreview.net
NATURAL LANGUAGE DESCRIPTIONS OF DEEP VISUAL FEATURES - OpenReview
新しいウィンドウで開く

github.com
openai/automated-interpretability - GitHub
新しいウィンドウで開く

intuitionlabs.ai
Understanding Mechanistic Interpretability in AI Models - IntuitionLabs
新しいウィンドウで開く

lesswrong.com
Sparse Autoencoders Find Highly Interpretable Directions in Language Models - LessWrong
新しいウィンドウで開く

arxiv.org
Automatically Interpreting Millions of Features in Large Language Models - arXiv
新しいウィンドウで開く

arxiv.org
Sparse Autoencoders Can Interpret Randomly Initialized Transformers - arXiv
新しいウィンドウで開く

galileo.ai
Monosemanticity: How Anthropic Made AI 70% More Interpretable | Galileo
新しいウィンドウで開く

arxiv.org
Automatically Interpreting Millions of Features in Large Language Models - arXiv
新しいウィンドウで開く

transformer-circuits.pub
Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet
新しいウィンドウで開く

pnas.org
Sparse autoencoders uncover biologically interpretable features in protein language model representations | PNAS
新しいウィンドウで開く

blog.eleuther.ai
Open Source Automated Interpretability for Sparse Autoencoder Features | EleutherAI Blog
新しいウィンドウで開く

medium.com
Label-Free Concept Bottleneck Models (ICLR 2023): A New Paradigm for Explainable AI
新しいウィンドウで開く

arxiv.org
arXiv:2304.06129v1 [cs.LG] 12 Apr 2023
新しいウィンドウで開く

arxiv.org
Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery
新しいウィンドウで開く

researchgate.net
(PDF) Cross-Modal Conceptualization in Bottleneck Models - ResearchGate
新しいウィンドウで開く

openreview.net
Revisiting Discover-then-Name Concept Bottleneck Models: A Reproducibility Study
新しいウィンドウで開く

arxiv.org
Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models
新しいウィンドウで開く

arxiv.org
Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization
新しいウィンドウで開く

researchgate.net
Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization
新しいウィンドウで開く

seas.upenn.edu
Towards Compositionality in Concept Learning - Penn Engineering
新しいウィンドウで開く

seas.upenn.edu
Towards Compositionality in Concept Learning - Penn Engineering
新しいウィンドウで開く

arxiv.org
Towards Compositionality in Concept Learning - arXiv
新しいウィンドウで開く

arxiv.org
Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization - arXiv
新しいウィンドウで開く

openai.com
Language models can explain neurons in language models - OpenAI
新しいウィンドウで開く
