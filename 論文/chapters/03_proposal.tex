\chapter{提案手法}

第1章で述べたとおり，本論文の目的は，対比因子ラベル生成タスクを通じて，LLM がテキスト集合差分入力だけから既存のアスペクトスキーマの語彙をどの程度再発見できるかを，実務的データセット上で定量的に評価することである．本章では，この目的を達成するために，LLM を用いて，2 つのテキスト集合 $A$ と $B$ の差分から対比因子ラベルを自動生成する手法を提案する．実験では，正解アスペクトラベルにもとづいて $A/B$ を構成し，生成ラベルと外部アスペクトスキーマとの意味的一致度で評価する．本論文では，テキスト集合差分入力 $(A,B)$ から自然言語ラベル $L$ を生成する写像 $(A,B)\mapsto L$ を対比因子ラベル生成タスクと呼び，その出力ラベル $L$ を一貫して対比因子ラベルと呼ぶ．

\section{対比因子ラベル生成タスクの定式化}

本研究が提案する対比因子ラベル生成タスクは，集合間の差分を自然言語ラベルとして要約するタスクを定式化する．

学習データセット $\mathcal{D}$ から得られる 2 つのテキスト集合 $A$ と $B$ が与えられるとき，それらの差分を簡潔な自然言語ラベル $L$ として生成する．
以下，数学的定式化では集合 $A$，集合 $B$ と表記し，プロンプトや実験手順の記述ではグループA，グループB と表記する．

このタスクの目的は，集合 $A$ に含まれるテキスト群の意味的・内容的な特徴のうち，集合 $B$ には含まれない差分を抽出・推論し，それを簡潔な自然言語ラベル $L$ として自動的に生成することである．

ここで，モデル内部の中間表現への直接アクセスを前提として，モデル内部の特定の内部要素 $N$（または非教師ありに抽出された潜在概念ベクトル $C$）に着目する．本研究では，命名の対象となる中間表現の要素を内部要素と呼び，その具体例としてニューロンのスカラー活性や非教師あり概念抽出による概念ベクトルを想定する．ただし，本研究の下流の命名・評価モジュール自体は中間表現への直接アクセスを必要とせず，内部要素の高活性に対応すると想定されるテキスト集合 $A/B$ のみを入力として動作する．一方で，データセット側の人手ラベルをアスペクトと呼ぶ．

タスクの例としてレビューの分類を行う深層学習モデル $M$ を考え，メカニスティック解釈にもとづいて中間層の内部要素 $N$（または非教師ありに抽出された潜在概念ベクトル $C$）を取り出した状況を想定する．各レビュー $x$ について，内部要素 $N$ の活性化の大きさを $\text{activation}(x, N)$ とおく．例えば，$\text{activation}(x, N)$ が大きいレビューには \texttt{it's a bit expensive.} や \texttt{The price is good.} のような価格への言及が多く，$\text{activation}(x, N)$ が小さいレビューには価格以外の内容が多い状況を考える．このとき，集合差分入力 $(A,B)$ に対して，グループAに特徴的でグループBには見られない差分を表す対比因子ラベル $L$ として，例えば \texttt{mentions of price} のような短い自然言語フレーズが生成されることが望ましい．

$\text{activation}(x, N)$ が大きい上位 $k$ 件のテキスト集合を $A$（発火群），小さい下位 $k$ 件のテキスト集合を $B$（非発火群）と定義する（$k=\texttt{group\_size}$）．記法を簡単にするため，以下では内部要素 $N$ の場合で表記するが，同様の定式化は概念ベクトル $C$ にも拡張できる．$L$ は，この差分を要約する自然言語フレーズ（価格に関する言及など）である．

このタスク全体は，集合入力からラベルへの写像
\[
 (A, B) \mapsto L
\]
として表される．以下では，この写像を便宜的に \texttt{textContrastiveNaming} と呼ぶ．
次節では，この写像を LLM を用いた手続きとして具体化し，(i) グルーピング，(ii) プロンプト設計，(iii) ラベル生成の 3 段階の処理フローとして記述する．

\section{LLM による対比的要約の実行}

本研究では，定式化された対比因子ラベル生成タスクを，LLM の文脈理解能力と自然言語生成能力を活用して解決する．LLM を，集合 $A$ と $B$ の差分を推論し，ラベルを生成する対比因子ラベル生成器として利用する．

提案手法は，以下の 3 段階の処理フローからなる．

\begin{enumerate}
\item \textbf{データ抽出とグルーピング} \\
解釈対象のニューラルネットワーク $M$ と，内部要素として特定の要素 $N$ を選択する．評価データセット $\mathcal{D}$ の各テキスト $x$ を $M$ に入力し，内部要素 $N$ の活性化値 $\text{activation}(x, N)$ を測定する．測定された活性化値に基づき，ハイパーパラメータ \texttt{group\_size} を用いて，活性化値が最も高いテキスト群 $A$ と，活性化値が最も低い（またはランダムな）テキスト群 $B$ を抽出する．すなわち，
 \[
   A = \{ x_1, \dots, x_k \}, \quad
   B = \{ x'_1, \dots, x'_k \}, \quad
   k = \texttt{group\_size}
 \]
 とする．\texttt{group\_size} は，プロンプトのコンテキスト長制限と計算コストのトレードオフを踏まえて決定される．
 \item \textbf{プロンプト設計と差分推論（Prompt Engineering and Contrast Inference）}：
 抽出されたテキスト集合 $A$ と $B$ の内容を，LLM の入力プロンプトに組み込む．プロンプトは，LLM に対し，単なる要約ではなく \texttt{グループ A に特徴的でグループ B には見られない主要な違いを特定し，簡潔に回答する} という対比因子ラベル生成タスクとして明確に指示する．
 特に，本研究で用いるプロンプトは次の要素から構成される．
 \begin{itemize}
  \item \textbf{タスク説明}：まず \texttt{2つのデータグループを比較して，グループAに特徴的でグループBには見られない表現パターンや内容の特徴を特定してください} といった指示文を提示する．
  \item \textbf{Few-shot 例（オプション）}：Few-shot 設定が有効な場合，\texttt{examples\_section} に \texttt{【例題$N$】グループA: [...] グループB: [...] 回答: [正解ラベル]} という形式の例を挿入する．
  \item \textbf{集合 $A$ のテキスト群}：\texttt{【グループA】} の見出しの下に，各テキストを \texttt{- [テキスト内容]} の形式で最大 \texttt{group\_size} 件列挙する．
  \item \textbf{集合 $B$ のテキスト群}：\texttt{【グループB】} の見出しの下に，同様の形式でテキストを列挙する．
  \item \textbf{出力制約}：プロンプト末尾で \texttt{英語で短いフレーズとして，グループAに特徴的でグループBには見られない主要な違いを簡潔に回答してください} と指示し，簡潔な対比因子ラベルの生成を求める．
 \end{itemize}
 
 \item \textbf{対比因子ラベルの生成} \\
 LLM に対して，推論結果にもとづき，集合 $A$ の意味的特性を簡潔に表現した自然言語ラベル $L$ を生成するよう求める．例えば，集合 $A$ が（価格が高すぎる）といったレビューを含み，集合 $B$ がレビューを含むが価格には言及しない場合，\texttt{mentions of price} のようなラベルが得られる可能性がある．
\end{enumerate}

\section{Few-shot ICL の設計}

本研究では，Few-shot インコンテキスト・ラーニング（ICL）を，対比因子ラベル生成の補助的な設定として導入する．具体的には，プロンプトに入出力例を含めることで，出力形式や語彙選択の傾向を誘導することを狙う．Few-shot の有無や例題数は，プロンプト設計上の設定項目として扱う．

Few-shot 設定が有効な場合，タスク説明の直後に \texttt{examples\_section} を挿入する．例の形式は \texttt{【例題$N$】グループA: [...] グループB: [...] 回答: [正解ラベル]} とする．shot数は0，1，3で行う。
