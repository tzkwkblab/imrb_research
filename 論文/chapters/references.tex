% 参考文献（References）
\newpage
\addcontentsline{toc}{chapter}{参考文献}
\renewcommand{\bibname}{参考文献}

%% 参考文献に bibtex を使う場合
%\bibliographystyle{junsrt}
%\bibliography{hoge}

%% 参考文献を直接ファイルに含めて書く場合
\begin{thebibliography}{99}

\bibitem{pontiki-EtAl:2014:SemEval2014Task4}
M.~Pontiki, D.~Galanis, J.~Pavlopoulos, H.~Papageorgiou, I.~Androutsopoulos, and S.~Manandhar:
``SemEval-2014 Task 4: Aspect Based Sentiment Analysis,''
in \textit{Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)}, pp.~27--35, 2014.

\bibitem{ribeiro2016should}
M.~T. Ribeiro, S.~Singh, and C.~Guestrin:
``Why should I trust you?: Explaining the predictions of any classifier,''
in \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pp.~1135--1144, 2016.

\bibitem{lundberg2017unified}
S.~M. Lundberg and S.-I. Lee:
``A unified approach to interpreting model predictions,''
in \textit{Advances in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{wachter2017counterfactual}
S.~Wachter, B.~Mittelstadt, and C.~Russell:
``Counterfactual explanations without opening the black box: Automated decisions and the GDPR,''
\textit{Harvard Journal of Law \& Technology}, vol.~31, no.~2, p.~841, 2017.

\bibitem{kim2018interpretability}
B.~Kim, M.~Wattenberg, J.~Gilmer, C.~Cai, J.~Wexler, F.~Viegas, et al.:
``Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV),''
in \textit{Proceedings of the 35th International Conference on Machine Learning}, pp.~2673--2682, 2018.

\bibitem{luss2024cell}
R.~Luss:
``CELL your Model: Contrastive Explanations for Large Language Models,''
arXiv:2406.11785, 2024.

\bibitem{bucinca2024contrastive}
Z.~Bu{\c{c}}inca:
``Contrastive Explanations That Anticipate Human Misconceptions Can Improve Human Decision-Making Skills,''
arXiv:2410.04253, 2024.

\bibitem{xu2024concept}
Y.~Xu et al.:
``Concept Bottleneck Models Without Predefined Concepts,''
arXiv:2407.03921, 2024.

\bibitem{anthropic2025biology}
Anthropic:
``On the Biology of a Large Language Model,''
Transformer Circuits, 2025. Available at \url{https://transformer-circuits.pub/2025/attribution-graphs/biology.html}.

\bibitem{alghamdi2024dynamic}
M.~Alghamdi et al.:
``Dynamic Sentiment Analysis with Local Large Language Models using Majority Voting,''
arXiv:2407.13069, 2024.

\bibitem{demszky2020goemotions}
D.~Demszky, D.~Movshovitz-Attias, J.~Ko, A.~Cowen, G.~Nemade, and S.~Ravi:
``GoEmotions: A Dataset of Fine-Grained Emotions,''
in \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, 2020.

\bibitem{srec:steam-review-aspect-dataset}
S.~Khosasi:
``Steam review aspect dataset,''
2024. Available at \url{https://srec.ai/blog/steam-review-aspect-dataset}.

\bibitem{papineni-etal-2002-bleu}
K.~Papineni, S.~Roukos, T.~Ward, and W.-J. Zhu:
``BLEU: a Method for Automatic Evaluation of Machine Translation,''
in \textit{Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics}, pp.~311--318, 2002.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova:
``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,''
arXiv:1810.04805, 2018.

\bibitem{schrodi2024unsupervised}
S.~Schrodi, M.~R{\"u}ckl, T.~Wirth, M.~B{\"o}hm, and D.~R{\"u}gamer:
``Concept Bottleneck Models Without Predefined Concepts,''
arXiv:2407.03921, 2024.

\bibitem{ameisen2025attribution}
E.~Ameisen, J.~Lindsey, A.~Pearce, W.~Gurnee, N.~L. Turner, B.~Chen, C.~Citro, D.~Abrahams, S.~Carter, B.~Hosmer, J.~Marcus, M.~Sklar, A.~Templeton, T.~Bricken, C.~McDougall, H.~Cunningham, T.~Henighan, A.~Jermyn, A.~Jones, A.~Persic, Z.~Qi, T.~B. Thompson, S.~Zimmerman, K.~Rivoire, T.~Conerly, C.~Olah, and J.~Batson:
``Circuit Tracing: Revealing Computational Graphs in Language Models,''
Transformer Circuits, 2025. Available at \url{https://transformer-circuits.pub/2025/attribution-graphs/methods.html}.

\bibitem{bordt2022posthoc}
S.~Bordt, M.~Finck, E.~Raidl, and U.~von Luxburg:
``Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts,''
in \textit{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22)}, pp.~1495--1515, 2022.

\bibitem{kardale2023contrastive}
A.~Kardale:
``Contrastive text summarization: a survey,''
\textit{International Journal of Information and Computation}, vol.~12, no.~3, pp.~1--10, 2023.

\bibitem{saha2024strumllm}
A.~Saha, B.~P. Majumder, H.~Jhamtani, S.~Subramanian, S.~Sreedhar, S.~Chakrabarti, and P.~Kankar:
``STRUM-LLM: Attributed and Structured Contrastive Summarization for User-Oriented Comparison,''
arXiv:2403.19710, 2024.

\bibitem{luo2024chatabsa}
Z.~Luo, Z.~Feng, Y.~Zhang, and H.~Liu:
``ChatABSA: A Novel Framework for Aspect-based Sentiment Analysis using Large Language Models,''
arXiv:2401.08226, 2024.

\bibitem{wang2024llmcluster}
J.~Wang, J.~Song, X.~Sun, C.~Chen, W.~Liu, and Y.~Liu:
``Improving Clustering Performance by Leveraging Large Language Models,''
arXiv:2410.00927, 2024.

\bibitem{sellam-etal-2020-bleurt}
T.~Sellam, D.~Das, and A.~Parikh:
``BLEURT: Learning Robust Metrics for Text Generation,''
in \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.~7881--7892, 2020.

\bibitem{yuan2021bartscore}
W.~Yuan, G.~Neubig, and P.~Liu:
``BARTScore: Evaluating Generated Text as Text Generation,''
in \textit{Advances in Neural Information Processing Systems}, vol.~34, pp.~27263--27277, 2021.

\bibitem{reiter2018structured}
E.~Reiter:
``A Structured Review of the Validity of BLEU,''
\textit{Computational Linguistics}, vol.~44, no.~3, pp.~393--401, 2018.

\bibitem{holtzman2020curious}
A.~Holtzman, J.~Buys, L.~Du, M.~Forbes, and Y.~Choi:
``The Curious Case of Neural Text Degeneration,''
in \textit{International Conference on Learning Representations (ICLR)}, 2020.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin:
``Attention is All you Need,''
in \textit{Advances in Neural Information Processing Systems (NIPS)}, 2017.

\bibitem{zhang2019bertscore}
T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi:
``BERTScore: Evaluating Text Generation with BERT,''
arXiv:1904.09675, 2019.

\bibitem{stein2024towards}
A.~Stein, A.~Naik, Y.~Wu, M.~Naik, and E.~Wong:
``Towards Compositionality in Concept Learning,''
in \textit{Proceedings of the International Conference on Machine Learning (ICML)}, 2024.

\end{thebibliography}

