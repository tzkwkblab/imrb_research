\chapter{まとめ}
\section{結論}

本研究は，大規模言語モデル（LLM）を用いて，2 つのテキスト集合 $A$ と $B$ の意味的差分を自然言語ラベルとして記述する対比因子命名タスクを定式化し，このタスクに対して LLM による自動命名がどの程度可能かをデータセットを用いた実験により検証した．実験では，SemEval，GoEmotions，Steam については人手アスペクトラベルにもとづくテキスト集合，COCO Retrieved Concepts については Top/Bottom 画像群に付与されたキャプション集合から構成したグループA/Bを用いることで，タスク定義における集合 $A$ と $B$ を具体化した．
LLM によるコントラスティブ要約を対比因子生成器として利用することで，コンセプトベース XAI が抱えてきた発見された内部構造への命名の自動化欠如という課題に対して，一つの実装可能なアプローチを示した．

SemEval-2014，GoEmotions，Steam，COCO Retrieved Concepts という性質の異なる 4 データセットに対する評価の結果，全 36 実験での SBERT類似度 平均は 0.6980 であった．
0-shot・temperature=0 の条件下でも多くの設定で人手ラベルと一定程度の意味的整合が見られた一方で，データセット間（例：SemEval と Steam）やアスペクト間にはばらつきや低スコアの事例も存在した．
一方で，BLEU スコアはほぼ 0 に張り付いており，本タスクでは語彙一致ではなく意味類似度に基づく評価が必要であることが明らかになった．

Few-shot ICL，group\_size，モデル種別，アスペクト説明文の有無を変化させた追加実験により，対比因子ラベリング性能が，概念の具体性，データセット設計，プロンプト設計の影響を受ける傾向が示唆された．
特に，SemEval のようにアスペクトとテキスト分布の対応が明瞭なデータセットでは相対的に高いスコアが得られた一方，Steam のようにマルチアスペクトでノイズの大きいドメインではスコアが相対的に低くなる傾向が観測された．

COCO Retrieved Concepts 実験では，少数のコンセプトに対するケーススタディとして，LLM がテキスト集合差分からフォーマルイベント，家族活動，電子機器といった視覚概念を指し示すラベルを生成できる例と，キャプションの頻度バイアスに引きずられて本質的な視覚特徴を十分に捉えられない例の両方が確認された．
これらの観察から，提案手法が視覚モデルのニューロン解釈に拡張し得る可能性と，同時にテキスト表現に依存する限界があることが示唆された．

以上を総合すると，LLM を用いた集合差分ベースの対比因子命名は，具体的アスペクトや整備されたデータセットに対しては，今回扱った条件の範囲内で実用的に機能しうることが示唆された．
また，非教師ありコンセプト抽出や Attribution Graphs が発見する内部構造に自然言語ラベルを付与するモジュールとして，限定的ながら選択肢の一つとなり得ることが示唆された．

\section{本研究の貢献}

本研究の主な貢献は以下の 3 点である．

第一に，2 つのテキスト集合 $(A,B)$ の差分から自然言語ラベル $L$ を生成する写像 \texttt{textContrastiveNaming}$(A,B) \to L$ を定式化し，コントラスティブ要約の枠組みを XAI 文脈の概念命名評価へ接続可能なタスクとして位置付けた点である．
これにより，非教師ありコンセプト抽出（UCBM, CCE）とメカニスティック解釈（Attribution Graphs）で分離していた概念の発見と命名を統一的に扱うためのタスク設定の一例を提示した．

第二に，SemEval-2014，GoEmotions，Steam，COCO Retrieved Concepts という 4 つの異なるドメインに対して一貫した実験パイプラインを構築し，対比因子命名タスクの実現可能性と限界を 4 データセット・全 36 実験の範囲で定量的に検討した点である．
特に，具体的アスペクトと抽象的感情カテゴリの双方に対して多くの条件で SBERT類似度 0.6〜0.8 台のスコアが得られた一方で，Steam の一部アスペクトなど相対的に低スコアとなるケースも確認され，概念の具体性とデータセット特性が性能に与える影響を示唆する結果が得られた．

第三に，Few-shot ICL，group\_size，モデル比較，アスペクト説明文比較といった追加実験を通じて，プロンプト設計とサンプリング戦略が出力スタイルと性能指標に与える影響について，統計的には有意差が得られなかったものの，いくつかの傾向を探索的に整理した点である．
たとえば，1-shot ICL による語彙的安定化の傾向，group\_size 50〜300 の範囲でスコアの大きな変動が見られなかったこと，新旧モデル間で顕著な差が見られなかったこと，説明文の有無が一部アスペクトでスコアの改善に結び付く例があったことなどである．
また，SBERT類似度 を主要指標，LLM 評価を補助指標とし，BLEU を参考値とする評価枠組みを用い，BLEU のような n-gram ベースの語彙一致指標が本タスクの主評価指標としては適さないことを確認した点である．
これにより，対比因子ラベリングのような概念的・説明的タスクに対して，意味類似度指標と LLM ベース評価を組み合わせる必要があることを示唆した．

\section{今後の課題}

本研究の問題設定は，第1章で述べたように，C-XAI やメカニスティック解釈における高コストな概念キュレーションの負担に関わる命名作業について，LLM がテキスト集合差分入力から既存スキーマの語彙をどの程度再発見できるかを，ゴールドラベル付きデータセット上で定量的に検証することであった．
本章で示した結果は，この目的に対して一定の示唆を与える一方で，依然として解決されていない論点も多い．
本節では，これらの未解決点と今後の方向性を整理する．

第一に，抽象的概念やノイズの大きいドメインに対する対比因子命名の挙動の理解と，必要に応じた性能改善である．
Steam レビューや一部の GoEmotions カテゴリでは，グループAとグループBの差分が単一アスペクトに集約されず，対比因子が曖昧になりやすい．
こうした設定に対しては，段階的な推論を促すプロンプティングを導入し，LLM に集合差分の解釈プロセスを段階的に明示させることで，抽象概念の命名がどの程度安定化するかを検討する価値がある．
また，Bu\c{c}inca et al. (2024)~\cite{bucinca2024contrastive} は，人間の誤解を見越した対比説明が意思決定スキルの向上に寄与することを示しており，この知見を踏まえて本研究で生成した対比因子ラベルが研究者や実務者のモデル理解・診断にどの程度貢献しうるかを，小規模なユーザスタディ等を通じて評価することも今後の課題である．

第二に，評価指標の高度化である．
本研究では SBERT類似度 と LLM 評価を中心に用いたが，人間評価との整合性が高いとされる BLEURT~\cite{sellam-etal-2020-bleurt} や BARTScore~\cite{yuan2021bartscore} などの学習ベース指標を導入し，単一指標に依存しない多面的な評価設計へ拡張することが考えられる．
特に，SBERT類似度 が近い設定間での微細な差異や，LLM スコアが粗い離散値に留まる問題に対して，連続値かつ高感度な指標を組み合わせることにより，解釈の精度と安定性を検証しやすくなると期待される．

第三に，視覚概念に対する対比因子命名の精緻化である．
COCO Retrieved Concepts 実験では，テキストキャプションのみを入力としたため，キャプションの頻度バイアスにより本質的な視覚特徴を取り逃がすケースが観測された．
今後は，画像から抽出した物体ラベルやシーン属性，CLIP ベースの画像埋め込みなどを中間表現として併用し，テキストと視覚特徴の双方に整合する対比因子を生成する枠組みを検討することが望ましい．

第四に，大規模ニューロン集合への適用と運用上のスケーラビリティである．
本研究ではデータセット単位のアスペクトを対象としたが，実際のモデル解釈では多数のニューロンや特徴量に対して一括でラベルを付与する必要がある．
このため，group\_size や Few-shot 設定を含むパイプライン全体の計算効率の最適化と，ラベルの一貫性・冗長性を制御するメタレベルの集約手法を設計し，解釈可能性と運用コストのトレードオフを評価することが重要な検討課題となる．

また，UCBM や Attribution Graphs などの手法によって発見された内部構造に対して，内部要素にもとづいて構成した集合 $A/B$ を入力として対比因子ラベルを付与し，専門家が妥当性を評価する枠組みへの接続も残された課題である．
本研究で得られた知見を踏まえ，実際の内部表現を対象とした小規模なケーススタディから段階的に検証を進めることが，概念キュレーションの負担軽減という当初の動機に沿った評価につながると考えられる．

最後に，本研究で提案した対比因子命名は，集合 $A$ と集合 $B$ の差分を，LLM を用いて人間に分かりやすい自然言語ラベルとして自動付与する枠組みである．
第1章で述べたように，本手法は規制対応や説明責任を直接的に満たすことを目的とするものではなく，既存モデルの内部状態を科学的に理解するための分析的な透明性を補助するモジュールとして位置付けられる．
本研究で扱った評価設定と条件の範囲に限れば，この枠組みはモデル内部の構造を把握するうえで一定の手がかりを与えうることが示唆された．
今後，ここで示した課題に対する改良と，より現実的な設定での検証を進めることで，XAI 研究および一部の実運用環境において，本手法がニューロン解釈を補助するモジュールとしてどの程度有用かをより慎重に評価していくことが今後の課題となる．


