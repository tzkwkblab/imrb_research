\chapter{結果と考察}

第4章で述べた各実験について，得られた結果を示し，次いで実験ごとに個別の考察を行う．その上で，章末において全ての実験カテゴリを横断して総括的に考察する．

本章では (i) メイン実験（複数データセットの結果比較），(ii) 入力やプロンプト条件別の実験（Few-shot 設定，グループサイズ，使用モデル，アスペクト説明文の有無），(iii) 外部の正解ラベルを前提としないデータセットを用いた検証，(iv) 補足的な分析（概念の具体性，エラー分析，統計的分析）のカテゴリ別に，結果と考察を述べる構成で記述する．

\section{メイン実験}
\noindent
本節では，SemEval-2014，GoEmotions，Steam の 3 データセットに対してメイン実験を行った．以降では，データセット別の集計結果，主要アスペクト別の結果，全体統計の順に結果を示し，最後にこれらの結果にもとづく考察を述べる．
\subsection{実験結果}
実験設定の詳細（パラメータ一覧）として，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0），アスペクト記述 = 無効とした．


データセット別の結果を表~\ref{tab:main_dataset_results}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：データセット別評価スコア}
  \label{tab:main_dataset_results}
  \begin{tabular}{lccc}
    \toprule
    データセット & SBERT類似度 & BLEU & LLM \\
    \midrule
    SemEval-2014 & & & \\
    \quad 平均 & 0.7531 & 0.0220 & 0.5500 \\
    \quad 最小 & 0.7181 & 0.0123 & 0.4000 \\
    \quad 最大 & 0.8012 & 0.0278 & 0.6000 \\
    \midrule
    GoEmotions & & & \\
    \quad 平均 & 0.7127 & 0.0073 & 0.4714 \\
    \quad 最小 & 0.5437 & 0.0 & 0.2 \\
    \quad 最大 & 0.8941 & 0.0408 & 0.8 \\
    \midrule
    Steam & & & \\
    \quad 平均 & 0.5403 & 0.0 & 0.3 \\
    \quad 最小 & 0.5164 & 0.0 & 0.2 \\
    \quad 最大 & 0.5612 & 0.0 & 0.6 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:main_dataset_results}より，SBERT類似度および LLM スコアは SemEval-2014 が最も高く，次いで GoEmotions，Steam の順に低下した．一方で BLEU は全データセットで低く，特に Steam では 0.0 に張り付いている．

アスペクト別の結果（主要なアスペクト）を表~\ref{tab:main_aspect_results}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：主要アスペクト別評価スコア}
  \label{tab:main_aspect_results}
  \begin{tabular}{lcccc}
    \toprule
    データセット & アスペクト & SBERT類似度 & BLEU & LLM \\
    \midrule
    \multirow{4}{*}{SemEval-2014} & Food & 0.7286 & 0.0123 & 0.4000 \\
    & Service & 0.7181 & 0.0240 & 0.6000 \\
    & Battery & 0.7646 & 0.0278 & 0.6000 \\
    & Screen & 0.8012 & 0.0240 & 0.6000 \\
    \midrule
    \multirow{4}{*}{GoEmotions} & Joy & 0.8192 & 0.0000 & 0.8000 \\
    & Anger & 0.7214 & 0.0000 & 0.6000 \\
    & Disgust & 0.8316 & 0.0240 & 0.8000 \\
    & Embarrassment & 0.8941 & 0.0408 & 0.8000 \\
    \midrule
    \multirow{4}{*}{Steam} & Gameplay & 0.5612 & 0.0000 & 0.2000 \\
    & Visual & 0.5164 & 0.0000 & 0.2000 \\
    & Story & 0.5383 & 0.0000 & 0.6000 \\
    & Audio & 0.5452 & 0.0000 & 0.2000 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:main_aspect_results}より，SemEval-2014 では \textit{Screen} が最も高い SBERT類似度（0.8012）を示し，GoEmotions では \textit{Embarrassment} が最も高い（0.8941）．Steam では SBERT類似度が全体として低い一方，LLM スコアは \textit{Story} のみ高く（0.6000），他のアスペクトは 0.2000 に留まった．

全体の統計を表~\ref{tab:main_overall_stats}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験：全体統計}
  \label{tab:main_overall_stats}
  \begin{tabular}{lccc}
    \toprule
    評価指標 & 平均 & 最小 & 最大 \\
    \midrule
    SBERT類似度 & 0.6980 & 0.5164 & 0.8941 \\
    BLEU & 0.0082 & 0.0000 & 0.0408 \\
    LLM & 0.4611 & 0.2000 & 0.8000 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:main_overall_stats}より，SBERT類似度は 0.5164--0.8941 とばらつきがある一方，BLEU は平均 0.0082 と極めて低い．また，LLM スコアは 0.2000--0.8000 の範囲にあり，条件によって厳しめの評価になっている．

\subsection{考察}
\label{subsec:main_experiment_discussion}
メイン実験の結果から，対比因子タスクにおいて LLM は 一定の性能を示すことが確認できる．全 36 実験の SBERT類似度は平均 0.6980（最小 0.5164，最大 0.8941）であり，データセット別には SemEval-2014（平均 0.7531）と GoEmotions（0.7127）が高く，Steam（0.5403）が大きく低下した（表~\ref{tab:main_dataset_results}）．LLM による 5 段階評価でも同様に，SemEval（0.5500）および GoEmotions（0.4714）が Steam（0.3000）を上回っており，データセット間の序列は概ね一致している（表~\ref{tab:main_dataset_results}）．

この序列は，データセット特性の違いとして解釈できる．SemEval-2014 や GoEmotions では，アスペクト／感情カテゴリに対応する語彙や典型表現が比較的一貫しており，グループ $A/B$ の差分が特定の意味領域に集中しやすい．その結果，LLM は差分の中心となる要因を短いラベルとして抽出しやすく，安定した命名につながる．一方で Steam はレビューが長く雑多で（平均 251 語，中央値 163 語，最大 1,521 語；語彙タイプ数 15,740），皮肉や複合評価などのノイズも大きいため，アスペクトがレビューの文脈に依存しやすい．このとき $A/B$ の差分は複数の要素に分散しやすく，単一の要因を短いラベルに圧縮することが難しくなるため，平均スコアの低下として現れると考えられる．

評価指標間の関係として，BLEU は平均 0.0082 と極めて低く，0.0 となる条件も多い（表~\ref{tab:main_overall_stats}）．この結果は，生成ラベルが正解ラベルと語彙的にはほとんど一致していないことを示している．一方で，SBERT類似度は平均 0.6980 と中程度の値を示しており（表~\ref{tab:main_overall_stats}），語彙が一致しなくても意味的には近い表現へ言い換えられているケースが多いと解釈できる．参考として，メイン実験再実行ログ（36 条件）の語数統計では，正解ラベルは平均 1.0 語，生成ラベルは平均 9.7 語であり，短い正解ラベルに対して生成ラベルが説明的フレーズになりやすいという性質が確認できる．したがって，BLEU の低さは，単純な失敗を意味するというより，本タスクでは言い換えが頻繁に生じるために n-gram 一致に基づく評価が低く出やすいことを反映している．

SBERT類似度と LLM 評価は，スコアの水準は異なるものの，全 36 条件における順位には強い正の相関が確認され（Spearman順位相関 $\rho = 0.73$, $p < 0.001$），どの条件が相対的に良いかという傾向を概ね共有している．ただし，SemEval-2014 と Steam は条件数が少ないため，データセット内での相関推定は不安定である．一方で，LLM 評価の平均は 0.4611 と低めであり，単なる意味的近さだけでなく，対比因子として焦点が合っているか，アスペクト名として簡潔に読めるかといった観点で，より厳しく妥当性を判定していると解釈できる．したがって，SBERT類似度は意味的一致の程度を広く把握する指標，LLM 評価は対比因子ラベルとしての品質を保守的に確認する指標として位置付けられ，両者は互いに補完関係にある．

さらにアスペクト別に見ると，高スコアになりやすい条件は，(1) 語彙的一貫性，(2) グループ $A/B$ の統計的差分の大きさ，(3) 抽象度の低さ（具象性）に依存する．例えば SemEval-2014 の \textit{Screen} は SBERT類似度 0.8012，GoEmotions の \textit{Embarrassment} は 0.8941 と高い値を示しており（表~\ref{tab:main_aspect_results}），これらは典型的表現が比較的明確である．一方で，Steam のようにアスペクト境界が曖昧でノイズが大きい条件では，$A/B$ 差分が単一アスペクトに対応しにくく，性能が低下しやすい．

以上より，本研究目的である対比因子タスクにおける LLM の性能は，条件が整ったデータセット／アスペクトでは 0-shot でも中程度以上の意味的一致を達成する一方，ドメインノイズや多義性が大きい条件では性能が低下しやすい，という形で整理できる．

\section{Few-shot設定による性能比較}
\subsection{実験結果}
実験設定として，Steam データセットを用いて，Few-shot 設定（0-shot，1-shot，3-shot）による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 100，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 12 実験（4 アスペクト × 3 Few-shot 設定）であり，全実験が成功した．

Few-shot 別の平均スコアを表~\ref{tab:fewshot_summary}に，アスペクト別の Few-shot 効果を表~\ref{tab:fewshot_aspect}に示す．
\begin{table}[htbp]
  \centering
  \caption{Few-shot設定による性能比較：平均スコア}
  \label{tab:fewshot_summary}
  \begin{tabular}{lccc}
    \toprule
    Few-shot設定 & SBERT類似度 & BLEU & LLM \\
    \midrule
    0-shot & & & \\
    \quad 平均 & 0.5526 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5462 & 0.0 & 0.2 \\
    \quad 最大 & 0.5596 & 0.0 & 0.4 \\
    \midrule
    1-shot & & & \\
    \quad 平均 & 0.6530 & 0.0 & 0.3500 \\
    \quad 最小 & 0.5111 & 0.0 & 0.2 \\
    \quad 最大 & 0.8356 & 0.0 & 0.8 \\
    \midrule
    3-shot & & & \\
    \quad 平均 & 0.5754 & 0.0 & 0.4000 \\
    \quad 最小 & 0.5416 & 0.0 & 0.2 \\
    \quad 最大 & 0.6449 & 0.0 & 0.6 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:fewshot_summary}より，SBERT類似度は 1-shot が平均 0.6530 と最も高く，0-shot（0.5526）および 3-shot（0.5754）を上回った．一方で LLM スコアは 0-shot から 3-shot にかけて増加し，BLEU は全設定で 0.0 であった．

SBERT類似度についてFriedman検定を実施し，$p=0.4724$で0/1/3-shot間に有意差は確認されなかった（Holm補正付き事後比較も全て非有意）。

\begin{table}[htbp]
  \centering
  \caption{Few-shot設定による性能比較：アスペクト別SBERT類似度}
  \label{tab:fewshot_aspect}
  \begin{tabular}{lccc}
    \toprule
    アスペクト & 0-shot & 1-shot & 3-shot \\
    \midrule
    Gameplay & 0.5462 & 0.6802 & 0.5644 \\
    Visual & 0.5562 & 0.5111 & 0.6449 \\
    Story & 0.5483 & 0.8356 & 0.5416 \\
    Audio & 0.5596 & 0.5850 & 0.5505 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:fewshot_aspect}より，アスペクト別には 1-shot で \textit{Story} が 0.8356 と大きく上昇する一方，\textit{Visual} では 1-shot が 0.5111 と低下しており，Few-shot の効果は一様ではない．

\subsection{考察}
Few-shot 実験では，0-shot，1-shot，3-shot の 3 設定における性能の違いを検証した．
SBERT類似度 の平均は 0-shot で 0.5526，1-shot で 0.6530，3-shot で 0.5754 であり，1-shot で最も高い値を示したが，Friedman 検定の結果（$p=0.4724$），統計的有意差は確認できなかった．
一方，LLM スコアは 0-shot で 0.3000，1-shot で 0.3500，3-shot で 0.4000 と単調増加しており，こちらも同様に統計的には有意な差は得られていない．

1-shot 設定では，プロンプト中に 1 つの例題 $(A_{\mathrm{ex}},B_{\mathrm{ex}},L_{\mathrm{ex}})$ を与えることで，出力が正解アスペクト名に近い語彙へと寄りやすくなる傾向が見られた．
この結果，SemEval 型のラベルに対して語彙的に近い短いラベルが生成されやすくなり，Few-shot 実験における SBERT類似度 が平均値として 0.5526（0-shot）から 0.6530（1-shot）へと上昇するなどの傾向が確認されたと解釈できる（表\ref{tab:fewshot_summary}参照）．
他方で，3-shot 設定では，複数例のスタイルを平均したより説明的なラベル（複数側面をまとめたフレーズ）が出力されやすくなり，正解ラベルとの埋め込み類似度はやや低下するものの，LLM による「説明としての自然さ」「対比としての一貫性」は向上したと考えられる．

アスペクト別に見ると，\textit{Story} では 1-shot で SBERT類似度 0.8356，LLM スコア 0.8000 と特に高い値を示し，物語的要素を含むレビューに対して Few-shot ICL が有効であることが分かる．
一方，\textit{Visual} や \textit{Audio} では，SBERT類似度 は 0.5700 前後であるにもかかわらず LLM スコアは 0.2000 に留まり，表層的な類似度があっても「対比因子ラベルとして有用か」という観点では厳しく評価されている．
これは，これらのアスペクトでは，対比因子ラベルが「グラフィックス」「サウンド」という表層カテゴリに留まり，具体的な差分（解像度，アートスタイル，音の存在感など）を十分に表現できていないことを反映している．

これらの結果は，Few-shot ICL が単純に性能を単調改善させる仕組みではなく，ラベルのスタイル（キーワード指向か，説明指向か）と評価指標との間にトレードオフを生じさせることを示している．
統計的な有意差は得られていないものの，0-shot でも SBERT類似度 0.5500 程度，LLM スコア 0.3000 程度を達成していることから，Few-shot なしでも本タスクは一定程度機能し，Few-shot の設計により「正解ラベルに近い語彙を出させるか」「説明として自然なフレーズを出させるか」をある程度制御できることが示唆された．
したがって，本タスクにおいては，定量的一致（SBERT類似度）の最大化を重視する場合には 1-shot 程度の例示を，説明としての自然さや対比の一貫性を重視する場合には 3-shot によるスタイル誘導を選択するといったように，傾向レベルの結果を踏まえつつ最適化したい観点に応じた Few-shot 設計が求められる．

\section{グループサイズの影響分析}
\subsection{実験結果}
実験設定として，Steam データセットを用いて，group\_size（50，100，150，200，300）による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，GPT モデル = gpt-4o-mini，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 20 実験（4 アスペクト × 5 group\_size）であり，全実験が成功した．

グループサイズ別の性能を表~\ref{tab:groupsize_results}に示す．
Friedman 検定では p=0.3309 となり，Holm 補正後の全ペア比較も含め有意差は確認されなかった．
\begin{table}[htbp]
  \centering
  \caption{グループサイズによる性能比較（各group\_sizeでの全アスペクト平均値）}
  \label{tab:groupsize_results}
  \begin{tabular}{lccc}
    \toprule
    group\_size & SBERT類似度 & BLEU & LLM \\
    \midrule
    50 & 0.5455 & 0.0 & 0.3000 \\
    100 & 0.5436 & 0.0 & 0.3500 \\
    150 & 0.5321 & 0.0 & 0.2500 \\
    200 & 0.5396 & 0.0 & 0.3000 \\
    300 & 0.5375 & 0.0 & 0.2000 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:groupsize_results}より，SBERT類似度は 0.5321--0.5455 の範囲に収まり，group\_size に対する単調な改善は確認できない．また BLEU は全条件で 0.0 であり，LLM スコアも 0.2000--0.3500 の範囲で大きな差は見られない．

全体の統計を表~\ref{tab:groupsize_overall_stats}に示す．
\begin{table}[htbp]
  \centering
  \caption{グループサイズ比較実験：全体統計}
  \label{tab:groupsize_overall_stats}
  \begin{tabular}{lccc}
    \toprule
    評価指標 & 平均 & 最小 & 最大 \\
    \midrule
    SBERT類似度 & 0.5396 & 0.5019 & 0.5636 \\
    BLEU & 0.0000 & 0.0000 & 0.0000 \\
    LLM & 0.2800 & 0.2000 & 0.6000 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:groupsize_overall_stats}より，全条件を通した SBERT類似度の範囲は 0.5019--0.5636 であり，group\_size の変更は平均値レベルでは限定的な影響に留まった．

コンテキスト長との関係として，group\_size が大きくなるほど，プロンプトのコンテキスト長が増加する．
メイン実験では group\_size = 100 を用い，コンテキスト長制限を考慮して各グループから最大 100 件のテキストを抽出した．

\subsection{考察}
Group Size 比較実験では，Steam データセットに対して group\_size を 50, 100, 150, 200, 300 と変化させた．
SBERT類似度 の平均は，それぞれ 0.5455, 0.5436, 0.5321, 0.5396, 0.5375 であり，全体として 0.53--0.55 の範囲に収まった．Friedman 検定の結果（$p=0.3309$）および Holm 補正付きの事後比較では，いずれの group\_size 間にも統計的に有意な差は検出されておらず，これらの違いはあくまで平均値レベルの変動にとどまる．
一方，LLM スコアの平均は 0.2000--0.3500 の範囲にあり，group\_size 間で顕著な差は確認されなかった．
また，本評価で 5 段階評価を 0.0--1.0 に線形変換した際に 1 点が 0.2000 に対応するため，多くの条件で 0.2000 付近に集中しやすく，サンプル数の変更だけでは人手評価相当の水準で明確な改善が得られにくかったと解釈できる．

この結果は，50〜300 の範囲では，サンプル数の増加が対比因子ラベルの意味的類似度に与える影響は限定的であり，特定の group\_size が極端に有利または不利になることはないことを，統計的検定でも有意差が検出されなかったという事実とあわせて，平均値レベルの傾向として示唆しているにとどまる．
group\_size を増やすことでノイズの平均化が期待される一方で，本実験では SBERT類似度 の単調な向上は観測されず，差分は最大でも 0.01 程度に留まった．

アスペクト別に見ると，group\_size の変化よりも，\textit{Gameplay} や \textit{Story} といったアスペクト間の難易度差の方が SBERT類似度・LLM スコアの変動に大きく寄与している．
したがって，本タスクにおける主なボトルネックはサンプル数よりも，データセット特性やアスペクト定義の明瞭さであると解釈できる．
実運用上は，プロンプトのコンテキスト長や計算コストを考慮しつつ，本実験で観測された傾向の範囲では 100〜200 程度の group\_size を用いることが，性能とコストのバランスをとるうえで一つの妥当な目安になると考えられる．

\section{モデル比較実験}
\subsection{実験結果}
GPT-4o-mini vs GPT-5.1 の比較として，Steam データセットを用いて，2 モデルによる性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 100，few\_shot = 0，group\_size = 100，LLM 評価 = 有効（gpt-4o，temperature = 0.0）とした．
総実験数は 8 実験（4 アスペクト × 2 モデル）であり，全実験が成功した．

モデル別の性能差を表~\ref{tab:model_comparison_summary}に，アスペクト別の性能差を表~\ref{tab:model_comparison_aspect}に示す．
対応のある Wilcoxon 検定では p=0.8750（中央値差 -0.0158，Holm 補正後も非有意）となり，統計的には差は確認されなかった．
\begin{table}[htbp]
  \centering
  \caption{モデル比較実験：平均スコア}
  \label{tab:model_comparison_summary}
  \begin{tabular}{lccc}
    \toprule
    モデル & SBERT類似度 & BLEU & LLM \\
    \midrule
    GPT-4o-mini & & & \\
    \quad 平均 & 0.5453 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5214 & 0.0 & 0.2 \\
    \quad 最大 & 0.5600 & 0.0 & 0.4 \\
    \midrule
    GPT-5.1 & & & \\
    \quad 平均 & 0.5375 & 0.0 & 0.2500 \\
    \quad 最小 & 0.5167 & 0.0 & 0.2 \\
    \quad 最大 & 0.5621 & 0.0 & 0.4 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:model_comparison_summary}より，平均 SBERT類似度は GPT-4o-mini が 0.5453，GPT-5.1 が 0.5375 と近い値であり，LLM スコアも 0.3000 と 0.2500 で大差はない．

SBERT類似度に対する対応のあるWilcoxon検定ではp=0.8750で、GPT-4o-miniとGPT-5.1の差は有意ではなかった（中央値差 5.1−4o-mini = -0.0158）。

\begin{table}[htbp]
  \centering
  \caption{モデル比較実験：アスペクト別SBERT類似度}
  \label{tab:model_comparison_aspect}
  \begin{tabular}{lcc}
    \toprule
    アスペクト & GPT-4o-mini & GPT-5.1 \\
    \midrule
    Gameplay & 0.5600 & 0.5423 \\
    Visual & 0.5425 & 0.5287 \\
    Story & 0.5573 & 0.5167 \\
    Audio & 0.5214 & 0.5621 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:model_comparison_aspect}より，\textit{Gameplay}，\textit{Visual}，\textit{Story} では GPT-4o-mini が上回る一方，\textit{Audio} は GPT-5.1 が上回っており，優位性はアスペクトに依存している．

\subsection{考察}
モデル比較実験では，Steam データセットに対して gpt-4o-mini と gpt-5.1 を比較した．
SBERT類似度 の平均は gpt-4o-mini が 0.5453，gpt-5.1 が 0.5375，LLM スコアはそれぞれ 0.3000 と 0.2500 であり，新しいモデルである gpt-5.1 が一貫して優位とはならなかった．対応のある Wilcoxon 検定の結果（$n=4, p=0.8750$），これらの差はいずれも有意水準 $5\%$ では統計的に有意とはいえない．
特に \textit{Gameplay}, \textit{Story}, \textit{Visual} のアスペクトでは gpt-4o-mini が SBERT類似度・LLM スコアともに高く，\textit{Audio} のみ gpt-5.1 が SBERT類似度 で上回った．

この結果は，0-shot・temperature=0 という決定論的条件では，モデルの世代よりもタスクとの整合性や事前学習分布が性能を左右しうる一方で，本実験の小規模データにおいては統計的に有意な優劣が確認できないことを示している．
すなわち，本実験の条件とサンプル数の範囲に限れば，「より新しいモデルを使えば常に良い対比因子が得られる」という前提を支持する明確な証拠は得られておらず，平均値レベルの傾向と統計的検定結果の両方を踏まえたうえで，対象ドメインとアスペクト特性に対する実測に基づくモデル選択が必要である．
同時に，ここでの知見はあくまでこの決定論的プロンプト条件におけるものであり，温度や Few-shot 例の設計を変えた場合にも同様の傾向が維持されるとは限らない点に注意が必要である．

\section{アスペクト説明文の効果検証}
\subsection{実験結果}
実験設定として，Steam データセットを用いて，アスペクト説明文の有無による性能差を検証した．
実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o，LLM 評価 = 有効（gpt-4o-mini，temperature = 0.0）とした．
総実験数は 8 実験（4 アスペクト × 2 条件（説明文あり/なし））であり，全実験が成功した．

説明文の有無による性能差を表~\ref{tab:aspect_desc_summary}に，アスペクト別の効果を表~\ref{tab:aspect_desc_aspect}に示す．
対応のある Wilcoxon 検定では p=0.3750（Holm 補正後も非有意）となり，性能差は傾向レベルであった．
\begin{table}[htbp]
  \centering
  \caption{アスペクト説明文の効果検証：平均スコア}
  \label{tab:aspect_desc_summary}
  \begin{tabular}{lccc}
    \toprule
    条件 & SBERT類似度 & BLEU & LLM \\
    \midrule
    説明文なし & & & \\
    \quad 平均 & 0.5395 & 0.0 & 0.2500 \\
    \quad 最小 & 0.5311 & 0.0 & 0.2 \\
    \quad 最大 & 0.5544 & 0.0 & 0.4 \\
    \midrule
    説明文あり & & & \\
    \quad 平均 & 0.5496 & 0.0 & 0.3000 \\
    \quad 最小 & 0.5186 & 0.0 & 0.2 \\
    \quad 最大 & 0.5810 & 0.0 & 0.4 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:aspect_desc_summary}より，説明文あり条件は説明文なし条件に比べて，SBERT類似度が 0.5395 から 0.5496 に増加し，LLM スコアも 0.2500 から 0.3000 に増加した．一方で BLEU は両条件で 0.0 であった．

SBERT類似度で対応のあるWilcoxon検定を行い，$p=0.3750$で説明文あり/なしの差は有意ではなかった（中央値差 $with\_desc - no\_desc = 0.0131$）。

\begin{table}[htbp]
  \centering
  \caption{アスペクト説明文の効果検証：アスペクト別SBERT類似度}
  \label{tab:aspect_desc_aspect}
  \begin{tabular}{lcc}
    \toprule
    アスペクト & 説明文なし & 説明文あり \\
    \midrule
    Gameplay & 0.5335 & 0.5523 \\
    Visual & 0.5311 & 0.5186 \\
    Story & 0.5392 & 0.5467 \\
    Audio & 0.5544 & 0.5810 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:aspect_desc_aspect}より，説明文の効果はアスペクトで異なり，\textit{Audio} は 0.5544 から 0.5810，\textit{Gameplay} は 0.5335 から 0.5523 へと増加する一方，\textit{Visual} は 0.5311 から 0.5186 に低下した．

\subsection{考察}
アスペクト説明文比較実験では，同じ Steam データセットに対し，アスペクトの説明文をプロンプト先頭に付加する条件としない条件を比較した．
全体平均では，説明文なしで SBERT類似度 0.5395，LLM スコア 0.2500，説明文ありで SBERT類似度 0.5496，LLM スコア 0.3000 と，平均値レベルでは改善傾向が見られた．対応のある Wilcoxon 検定の結果（$n=4, p=0.3750$），これらの差も統計的には有意ではなく，少数サンプルで観測された傾向として解釈するのが妥当である．
特に \textit{Audio} と \textit{Gameplay} では SBERT類似度 の上昇幅が大きく，LLM スコアも 0.2000 から 0.4000 へと改善しているが，これらも平均値レベルの改善として位置付けられる．
これは，アスペクトの意味領域を明示することで，LLM の注意がレビュー全般のスタイルではなく，アスペクト固有の差分（音響面，ゲーム性）に向かいやすくなった結果と解釈できる．

一方，\textit{Visual} では説明文あり条件で SBERT類似度 が 0.5311 から 0.5186 に低下しており，汎用的すぎる説明文が「見た目」以外の要素（レビューの詳細さや長さなど）への注意を誘導した可能性がある．
このように，説明文は一般に安定化に寄与する一方で，粒度が広すぎたり抽象的すぎたりすると LLM の注意がアスペクト固有の視覚的特徴からレビュー全体のスタイルなどへと拡散し，かえってアスペクト固有性を弱めるリスクもある．

評価指標間の関係に関しては，BLEU が全条件で 0.0 付近に張り付いているのに対し，SBERT類似度 と LLM スコアはアスペクトや設定の違いを敏感に反映している．
とりわけ，SBERT類似度 が近い場合でも LLM スコアに差が出るケースがあり，SBERT類似度 が「意味空間における距離」を測るのに対し，LLM スコアは「対比因子ラベルとしての焦点の合致」や「説明の妥当性」をより厳しく評価しているといえる．
この構造から，対比因子ラベリングの評価には，SBERT類似度 を主要指標としつつ，LLM 評価を補助指標として併用する多面的な評価設計が必要であることが確認された．

\section{COCO Retrieved Concepts実験}
\label{sec:coco_experiment}
\subsection{実験結果}
実験設定として，COCO Retrieved Concepts データセットを用いて，正解ラベルがない画像キャプションデータセットに対する対比因子生成を検証した．本データセットは，各画像に対して COCO データセット由来の複数のキャプションが付与されている一方で，画像をどのような基準でクラスタリングすべきかという正解となるクラスタラベルは与えられていない．本データセットは，300個の潜在概念（concept embeddings）が学習されたモデルから生成されており，各概念について，CLIP（ViT-B/32）を用いて計算した画像埋め込みとのコサイン類似度に基づいて，MS-COCO 2017 train splitから画像を取得している．具体的には，各概念に対して最も類似度が高い100枚の画像を Top-100，最も類似度が低い100枚の画像を Bottom-100 として抽出している．このとき，Top-100 と Bottom-100 の分割基準は潜在概念と CLIP 類似度により定まるが，それがどのような視覚的側面に対応しているかは外部からは明示されない，という状況を想定している．

この分割結果に対して，Top-100 をグループA，Bottom-100 をグループBとして抽出した．各グループについて，画像ごとに付与された COCO キャプション群を用いて対比因子生成を行うことで，各概念がどのような視覚的側面によって Top-100 と Bottom-100 に分割されているのかを，自然言語の対比因子として言語化できるかを検証することを狙いとした．

実験パラメータは，temperature = 0.0，max\_tokens = 2000，few\_shot = 0，group\_size = 100，GPT モデル = gpt-4o-mini，LLM 評価 = 無効とした．総実験数は 5 実験（5 コンセプト）であり，全実験が成功した．正解ラベルがないため，SBERT類似度 と BLEU スコアは参考値として記録するにとどめ，評価指標としては用いなかった．代わりに，生成された対比因子と，各コンセプトに対応する画像群とを見比べることで，対比因子が画像の視覚的特徴を適切に記述しているか，すなわち各概念に対する類似度ランキングの分割基準を妥当な形で言語化できているかを確認した．

視覚的概念記述の生成能力の検証結果を表~\ref{tab:coco_results}に示す．
各コンセプトについて，生成された対比因子と画像との整合性を確認するため，代表的な画像を図~\ref{fig:coco_concept0}--\ref{fig:coco_concept50}に示す．

\begin{table}[htbp]
  \centering
  \caption{COCO Retrieved Concepts実験：評価スコア}
  \label{tab:coco_results}
  \begin{tabular}{lcc}
    \toprule
    評価指標 & 平均 & 範囲 \\
    \midrule
    SBERT類似度 & 0.6173 & 0.5714--0.6537 \\
    BLEU & 0.0000 & 0.0000--0.0000 \\
    LLM & --- & （無効） \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:coco_results}に示す SBERT類似度と BLEU は，正解ラベルが存在しないため参考値であるが，少なくとも 5 コンセプトの範囲では SBERT類似度が 0.57--0.65 の範囲に収まり，BLEU は 0.0 であった．

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 500 332]{image/coco/concept_0_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 415]{image/coco/concept_0_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_0の代表画像例（生成対比因子：「Group A features everyday scenes and objects, while Group B focuses on events and people in formal settings.」）}
  \label{fig:coco_concept0}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 427]{image/coco/concept_1_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 425]{image/coco/concept_1_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_1の代表画像例}
  \label{fig:coco_concept1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 500 332]{image/coco/concept_2_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 612 612]{image/coco/concept_2_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_2の代表画像例}
  \label{fig:coco_concept2}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 427]{image/coco/concept_10_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 480 480]{image/coco/concept_10_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_10の代表画像例}
  \label{fig:coco_concept10}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 565 640]{image/coco/concept_50_group_a.jpg}
    \caption{Group A (Top-100)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio,bb=0 0 640 456]{image/coco/concept_50_group_b.jpg}
    \caption{Group B (Bottom-100)}
  \end{subfigure}
  \caption{concept\_50の代表画像例（生成対比因子：「Group A focuses on electronics and mobile devices.」）}
  \label{fig:coco_concept50}
\end{figure}

\subsection{考察}
COCO Retrieved Concepts 実験では，非教師ありに学習された潜在コンセプトと CLIP 類似度に基づいて構成された Top-100/Bottom-100 画像群に付与されたキャプションを用い，5 つの概念に対して対比因子ラベルを生成した．
正解ラベルが存在しないため，テキストベースの SBERT類似度（平均 0.6173）と，実画像との整合性に基づく定性的評価を組み合わせて妥当性を検証した．
ここでの SBERT類似度 はあくまでキャプション間の意味類似度を測る参考指標であり，生成ラベルと画像との対応そのものを評価するものではない点に留意する必要がある．

concept\_0，concept\_10，concept\_50 では，「日常的な場面と物 vs フォーマルなイベントと人々」「子ども・家族・動物を含む活動」「電子機器・モバイル端末」といった対比因子ラベルが生成され，少なくともこれらのコンセプトに関しては，対応する画像群の視覚的特徴と整合的と解釈できる例が観察された．
これらのケースでは，LLM はキャプション差分から「イベント性」「家庭的活動」「電子機器」といった高レベル概念を抽出し，それが実際の画像に現れる被写体カテゴリや構図とおおむね一致していると評価された．
この結果は，キャプション集合の差分から視覚的概念を言語化し得る可能性を一部ケースで示唆するものであるが，正解ラベルが存在しないという制約の下での定性的評価に基づくものであり，より体系的な視覚整合性評価は今後の課題として残る．

一方，concept\_1 や concept\_2 では，「スポーツと屋外活動」「動物と自然風景」といった対比因子ラベルが生成されたものの，Top 側の画像群には時計や信号機などスポーツとは無関係な画像が多数含まれており，対比因子ラベルは集合の一部の特徴を過度に代表させたものになっていた．
これは，LLM がキャプション中で頻出する語（例：\textit{sports}, \textit{outdoor}）に引きずられ，本質的な視覚パターン（時計，時間）を取り逃がすバイアスを持つことを示している．

これらの観察から，テキスト集合差分のみに基づく対比因子ラベル生成は，視覚的概念に対して一定の成功例を持つ一方で，キャプション側の偏りやノイズに敏感であり，画像の本質的特徴を必ずしも忠実に反映しない場合があることが分かる．
視覚ドメインへ応用する際には，画像から抽出した物体ラベルやシーン属性などの情報も併用し，テキストと視覚特徴の両方に整合する形で対比因子ラベルを検証する必要があることが示唆される．

\section{概念の具体性による性能比較}
\label{sec:concreteness}
\subsection{実験結果}
LLM による対比因子ラベル生成の性能は，対象となる概念の具体性（Concrete vs.\ Abstract）によって差異が観測された．
データセット別・概念タイプ別の性能比較を表~\ref{tab:concreteness_comparison}に，主要アスペクト別の詳細を表~\ref{tab:concreteness_aspects}に示す．

\begin{table}[htbp]
  \centering
  \caption{概念の具体性による性能比較：データセット別SBERT類似度}
  \label{tab:concreteness_comparison}
  \begin{tabular}{lcc}
    \toprule
    データセット & 概念タイプ & SBERT類似度平均 \\
    \midrule
    SemEval-2014 & 具体的アスペクト & 0.7531 \\
    GoEmotions & 抽象的概念 & 0.7127 \\
    Steam & ドメイン固有で抽象度の高いアスペクト & 0.5403 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:concreteness_comparison}より，SBERT類似度の平均は SemEval-2014（0.7531）と GoEmotions（0.7127）で高く，Steam（0.5403）で低い．すなわち，概念の具体性や語彙的一貫性が高い設定ほど，高い類似度が得られる傾向が見られる．

\begin{table}[htbp]
  \centering
  \caption{概念の具体性による性能比較：主要アスペクト別SBERT類似度}
  \label{tab:concreteness_aspects}
  \begin{tabular}{lcc}
    \toprule
    データセット & アスペクト & SBERT類似度 \\
    \midrule
    \multirow{4}{*}{SemEval-2014（具体的）} & Food & 0.7286 \\
    & Service & 0.7181 \\
    & Battery & 0.7646 \\
    & Screen & 0.8012 \\
    \midrule
    \multirow{4}{*}{Steam（ドメイン固有・抽象度高）} & Gameplay & 0.5612 \\
    & Visual & 0.5164 \\
    & Story & 0.5383 \\
    & Audio & 0.5452 \\
    \midrule
    \multirow{4}{*}{GoEmotions（抽象的）} & Joy & 0.8192 \\
    & Disgust & 0.8316 \\
    & Embarrassment & 0.8941 \\
    & Neutral & 0.5437 \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:concreteness_aspects}より，SemEval-2014 では \textit{Screen} が 0.8012 と高い一方，Steam では \textit{Visual} が 0.5164 と低い．また GoEmotions では \textit{Embarrassment} が 0.8941 と高いが，\textit{Neutral} は 0.5437 と低く，カテゴリ間のばらつきが確認できる．

SemEval-2014 の具体的なアスペクト（SBERT類似度 平均 0.7531）は，Steam のアスペクト（SBERT類似度 平均 0.5403）と比較して高い値を記録した．
GoEmotions の感情カテゴリ（SBERT類似度 平均 0.7127）は，Steam のアスペクトと比較して高い値を記録したが，SemEval-2014 の具体的なアスペクトと比較して低い値を記録した．
GoEmotions の一部の感情カテゴリ（Disgust 0.8316，Embarrassment 0.8941，Joy 0.8192）では，高い SBERT類似度 を記録した一方，neutral（0.5437）では低い値を記録した．

\subsection{考察}
データセット別の性能比較から，概念の具体性とデータセット設計が対比因子ラベルの性能に強く影響することが分かる．
SemEval-2014 では SBERT類似度 平均 0.7531，GoEmotions では 0.7127，Steam では 0.5403 と，明確な性能差が観測された．

SemEval-2014 で扱う \textit{Food}, \textit{Service}, \textit{Battery}, \textit{Screen} などのアスペクトは，具体的な物理属性や機能に結びついた概念であり，レビュー中での語彙が比較的一貫している．
このため，グループAとグループBの差分には「食べ物の品質」「価格」「バッテリーの持続時間」「画面の解像度」といった安定した統計的パターンが現れやすく，LLM はそれらを単語レベルまたは短い説明フレーズとして抽出しやすい．
結果として，SemEval では SBERT類似度 が高く，LLM スコアも 0.5500 と比較的高い．

GoEmotions の 28 感情カテゴリは，物理的対象ではなく内在状態を表す抽象的概念であるが，データセット設計上，各感情に対応する典型的トリガー表現や感情語彙が豊富に含まれている．
そのため，\textit{disgust}, \textit{embarrassment}, \textit{joy} など一部の感情カテゴリでは SBERT類似度 が 0.8300 以上に達し，LLM スコアも 0.8000 と高い値を記録している．
一方で，\textit{neutral} のように境界が曖昧なカテゴリでは 0.5437 まで低下しており，概念の抽象度だけでなく，アノテーション方針と語彙的一貫性が性能に影響していることが分かる．

Steam Review Aspect Dataset では，\textit{Gameplay}, \textit{Visual}, \textit{Story}, \textit{Audio} といったアスペクトの SBERT類似度 平均が 0.5403 に留まり，LLM スコアも 0.3000 前後と低い．
Steam レビューは長文で雑多な記述が多く，複数アスペクトへの同時言及や皮肉的表現も頻出する．
その結果，グループAとグループBの差分が単一アスペクトにきれいに対応せず，「ゲームプレイ」「ストーリー」「雰囲気」「技術的問題」といった複数の要素が混在した対比因子が生成される傾向がある．
この構造的ノイズが，SemEval や GoEmotions と比較した際の性能低下の主因である．

これらの結果から，対比因子ラベリング性能は，(i) アスペクトの具体性，(ii) レビュー中の語彙的一貫性，(iii) グループA/Bの統計的差分の大きさ，の三要因と一定の関係を持つことが示唆された．
特に，SemEval のように人手ラベルとテキスト分布が整合的なベンチマークでは，集合差分からの自動命名が比較的容易であり，Steam のようにドメイン特有の多義性とノイズが大きい場合には難易度が高い傾向がある．
要するに，対比因子ラベリングの成否はアスペクト概念の明瞭さとレビュー言語の一貫性に大きく左右され，ノイズや多義性の大きいドメインでは人手アスペクトと整合したラベルを得ることが難しい．
次節では，これらのデータセット特性に対して Few-shot ICL による出力スタイルの制御がどの程度補償的に働くかを検証する．

\section{エラー分析と限界}
本実験では，全ての実験カテゴリにおいて実行上の失敗は発生せず，予定した 89 実験を完了した．
一方で，コンテキスト長制限を考慮し，各グループから最大 100 件のテキストを抽出する制約を設けており，大規模なテキスト集合に対する性能は十分に評価できていないという限界がある．
また，具体的なアスペクト（SemEval-2014 の Food，Service，Battery，Screen）では高い SBERT類似度 を記録した一方で，抽象度の高いアスペクト（Steam の Gameplay，Visual，Story，Audio）や一部の感情カテゴリ（GoEmotions の neutral など）ではスコアが低く，概念の抽象性が命名性能に影響することが示唆された．

\section{補足分析}
Few-shot ICL では，少数の例題をプロンプトの \texttt{examples\_section} に挿入し，モデルが出力形式や語彙選択を安定させることを狙った．
Steam データセットを用いた Few-shot 実験では，1-shot 設定が 0-shot および 3-shot と比較して最も高い SBERT類似度 を示し，過度な例示よりも代表的な例を少数与える方が有効である可能性が示された．
さらに，SemEval-2014，GoEmotions，Steam の結果を比較すると，具体的アスペクトを扱う SemEval-2014 で最も高いスコアが得られ，抽象度が高い Steam のアスペクトでスコアが低下するなど，概念の具体性が対比因子ラベル生成の難易度を規定する一因であることがうかがえた．

\section{統計的分析}
\label{sec:statistical_analysis}
Few-shot（0/1/3-shot）とグループサイズ比較について Holm 補正済み Friedman 検定を実施し，それぞれ p=0.4724，p=0.3309 でいずれも有意差は確認されなかった．
モデル比較（n=4）とアスペクト説明文有無（n=4）については，対応のある Wilcoxon 検定を実施し，p=0.8750（中央値差 -0.0158），p=0.3750 といずれも非有意であった（Holm 補正後も非有意）．
詳細な統計量と検定結果は \texttt{stat\_tests.md} を参照されたい．

外れ値の分析を表~\ref{tab:outliers}に示す．
\begin{table}[htbp]
  \centering
  \caption{メイン実験における外れ値分析}
  \label{tab:outliers}
  \begin{tabular}{lcc}
    \toprule
    指標 & 最小値 & 最大値 \\
    \midrule
    SBERT類似度 & 0.5164（Steam Visual） & 0.8941（GoEmotions Embarrassment） \\
    BLEU & 0.0000 & 0.0408（GoEmotions Embarrassment） \\
    LLM & 0.2000 & 0.8000（GoEmotions Joy, Disgust, Embarrassment） \\
    \bottomrule
  \end{tabular}
\end{table}

表~\ref{tab:outliers}より，SBERT類似度と LLM スコアはいずれも条件によるばらつきが大きく，最良ケース（GoEmotions \textit{Embarrassment}）と最悪ケース（Steam \textit{Visual}）で大きな差がある．また BLEU は最大でも 0.0408 に留まり，多くの条件で 0.0 となる．

GoEmotions の感情カテゴリでは，neutral アスペクトで SBERT類似度 0.5437 と低い値を記録したが，これは他の感情カテゴリと比較して低い値であった．

実験間の一貫性の確認として，全実験カテゴリにおいて実験間の一貫性が確認された．
