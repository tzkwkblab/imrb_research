\chapter{考察}

本章では、第4章で示した実験結果に基づき、対比因子ラベル自動生成手法の妥当性、性能の決定要因、および限界について整理して考察する。

\section{対比因子命名タスクの実現可能性}

まず、SemEval-2014、GoEmotions、Steam の 3 データセットに対するメイン実験の結果から、対比因子命名タスクの実現可能性を評価する。
全 36 実験における BERTScore の平均は 0.6980、最小 0.5164、最大 0.8941 であり、特に SemEval-2014 に対しては平均 0.7531 と高い値を示した。
これは、グループAとグループBの集合差分のみを与えたにもかかわらず、LLM が人手アスペクトラベルと中程度以上に意味的に整合するラベルを安定して生成できていることを示す。

一方で、BLEU スコアは全体平均 0.0082 とほぼ 0 に近く、Few-shot 実験や追加実験でも一貫して 0.0 付近であった。
これは第4章で述べたように、正解ラベルが短いアスペクト名であるのに対し生成ラベルが説明的フレーズとなるというタスク特性に起因しており、BLEU は本タスクでは参考指標にとどまる。

LLM による 5 段階評価を用いた意味的類似度スコアは、全体平均 0.4611 と BERTScore よりも厳しい値を示したが、SemEval では 0.5500、GoEmotions では 0.4714 と、BERTScore と整合した順位付けになっている。
このことから、BERTScore（定義は第4章を参照）は生成ラベルと正解ラベルの意味空間における距離を連続値として測定し、LLM 評価は「対比因子ラベルとして妥当か」「アスペクト名として読めるか」をより保守的に判定する補完的な指標として機能していると解釈できる。

以上の結果は、集合差分入力 $(A,B)$ からの自然言語ラベル生成というタスクに対して、LLM が 0-shot 設定でも意味的に妥当なラベルを多数のケースで生成可能であること、すなわち提案タスクが実現可能な水準にあることを示している。

\section{概念の具体性とデータセット特性の影響}

データセット別の性能比較から、概念の具体性とデータセット設計が対比因子ラベルの性能に強く影響することが分かる。
SemEval-2014 では BERTScore 平均 0.7531、GoEmotions では 0.7127、Steam では 0.5403 と、明確な性能差が観測された。

SemEval-2014 で扱う \textit{Food}, \textit{Service}, \textit{Battery}, \textit{Screen} などのアスペクトは、具体的な物理属性や機能に結びついた概念であり、レビュー中での語彙が比較的一貫している。
このため、グループAとグループBの差分には「食べ物の品質」「価格」「バッテリーの持続時間」「画面の解像度」といった安定した統計的パターンが現れやすく、LLM はそれらを単語レベルまたは短い説明フレーズとして抽出しやすい。
結果として、SemEval では BERTScore が高く、LLM スコアも 0.5500 と比較的高い。

GoEmotions の 28 感情カテゴリは、物理的対象ではなく内在状態を表す抽象的概念であるが、データセット設計上、各感情に対応する典型的トリガー表現や感情語彙が豊富に含まれている。
そのため、\textit{disgust}, \textit{embarrassment}, \textit{joy} など一部の感情カテゴリでは BERTScore が 0.8300 以上に達し、LLM スコアも 0.8000 と高い値を記録している。
一方で、\textit{neutral} のように境界が曖昧なカテゴリでは 0.5437 まで低下しており、概念の抽象度だけでなく、アノテーション方針と語彙的一貫性が性能に影響していることが分かる。

Steam Review Aspect Dataset では、\textit{Gameplay}, \textit{Visual}, \textit{Story}, \textit{Audio} といったアスペクトの BERTScore 平均が 0.5403 に留まり、LLM スコアも 0.3000 前後と低い。
Steam レビューは長文で雑多な記述が多く、複数アスペクトへの同時言及や皮肉的表現も頻出する。
その結果、グループAとグループBの差分が単一アスペクトにきれいに対応せず、「ゲームプレイ」「ストーリー」「雰囲気」「技術的問題」といった複数の要素が混在した対比因子が生成される傾向がある。
この構造的ノイズが、SemEval や GoEmotions と比較した際の性能低下の主因である。

これらの結果から、対比因子ラベリング性能は、(i) アスペクトの具体性、(ii) レビュー中の語彙的一貫性、(iii) グループA/Bの統計的差分の大きさ、の三要因に強く依存することが示された。
特に、SemEval のように人手ラベルとテキスト分布が整合的なベンチマークでは、集合差分からの自動命名が比較的容易であり、Steam のようにドメイン特有の多義性とノイズが大きい場合には難易度が高い。

\section{Few-shot ICL と出力スタイルのトレードオフ}

Few-shot 実験では、0-shot、1-shot、3-shot の 3 設定における性能の違いを検証した。
BERTScore の平均は 0-shot で 0.5526、1-shot で 0.6530、3-shot で 0.5754 であり、1-shot で明確なピークを示した。
一方、LLM スコアは 0-shot で 0.3000、1-shot で 0.3500、3-shot で 0.4000 と単調増加しており、両指標で最適な Few-shot 数が一致していない。

1-shot 設定では、プロンプト中に 1 つの例題 $(A_{\mathrm{ex}},B_{\mathrm{ex}},L_{\mathrm{ex}})$ を与えることで、出力が正解アスペクト名に近い語彙へと強く誘導される。
この結果、SemEval 型のラベルに対して語彙的に近い短いラベルが生成されやすくなり、BERTScore が大きく向上したと解釈できる。
他方で、3-shot 設定では、複数例のスタイルを平均したより説明的なラベル（複数側面をまとめたフレーズ）が出力されやすくなり、正解ラベルとの埋め込み類似度はやや低下するものの、LLM による「説明としての自然さ」「対比としての一貫性」は向上したと考えられる。

アスペクト別に見ると、\textit{Story} では 1-shot で BERTScore 0.8356、LLM スコア 0.8000 と特に高い値を示し、物語的要素を含むレビューに対して Few-shot ICL が有効であることが分かる。
一方、\textit{Visual} や \textit{Audio} では、BERTScore は 0.5700 前後であるにもかかわらず LLM スコアは 0.2000 に留まり、表層的な類似度があっても「対比因子ラベルとして有用か」という観点では厳しく評価されている。
これは、これらのアスペクトでは、対比因子ラベルが「グラフィックス」「サウンド」という表層カテゴリに留まり、具体的な差分（解像度、アートスタイル、音の存在感など）を十分に表現できていないことを反映している。

これらの結果は、Few-shot ICL が単純に性能を単調改善させる仕組みではなく、ラベルのスタイル（キーワード指向か、説明指向か）と評価指標との間にトレードオフを生じさせることを示している。
0-shot でも BERTScore 0.5500 程度、LLM スコア 0.3000 程度を達成していることから、Few-shot なしでも本タスクは一定程度機能するが、Few-shot の設計により「正解ラベルに近い語彙を出させるか」「説明として自然なフレーズを出させるか」を制御できることが確認された。

\section{グループサイズとサンプリング戦略の影響}

Group Size 比較実験では、Steam データセットに対して group\_size を 50, 100, 150, 200, 300 と変化させた。
BERTScore の平均は、それぞれ 0.5489, 0.5514, 0.5406, 0.5487, 0.5603 であり、全体として 0.54 前後に収まりつつ、300 でわずかな上昇が見られた。
一方、LLM スコアの平均は全ての group\_size で 0.2000 に固定されており、人手評価相当の観点からは顕著な差は検出されなかった。

この結果は、50〜300 の範囲では、サンプル数の増加が対比因子ラベルの意味的類似度に与える影響は限定的であり、特定の group\_size が極端に有利または不利になることはないことを示している。
group\_size を増やすことで、グループAとグループBのノイズが平均化され、わずかながら BERTScore が向上する傾向はあるが、その効果は 0.01〜0.02 程度に留まる。

アスペクト別に見ると、group\_size の変化よりも、\textit{Gameplay} や \textit{Story} といったアスペクト間の難易度差の方が BERTScore・LLM スコアの変動に大きく寄与している。
したがって、本タスクにおける主なボトルネックはサンプル数よりも、データセット特性やアスペクト定義の明瞭さであると解釈できる。
実運用上は、プロンプトのコンテキスト長や計算コストを考慮しつつ、100〜200 程度の group\_size を採用すれば十分な性能が得られると考えられる。

\section{モデル選択・プロンプト設計と評価指標の関係}

モデル比較実験では、Steam データセットに対して gpt-4o-mini と gpt-5.1 を比較した。
BERTScore の平均は gpt-4o-mini が 0.5453、gpt-5.1 が 0.5375、LLM スコアはそれぞれ 0.3000 と 0.2500 であり、新しいモデルである gpt-5.1 が一貫して優位とはならなかった。
特に \textit{Gameplay}, \textit{Story}, \textit{Visual} のアスペクトでは gpt-4o-mini が BERTScore・LLM スコアともに高く、\textit{Audio} のみ gpt-5.1 が BERTScore で上回った。

この結果は、0-shot・temperature=0 という決定論的条件では、モデルの世代よりもタスクとの整合性や事前学習分布が性能を左右することを示している。
すなわち、「より新しいモデルを使えば常に良い対比因子が得られる」という前提は成立せず、対象ドメインとアスペクト特性に対する実測に基づくモデル選択が必要である。

アスペクト説明文比較実験では、同じ Steam データセットに対し、アスペクトの説明文をプロンプト先頭に付加する条件としない条件を比較した。
全体平均では、説明文なしで BERTScore 0.5395、LLM スコア 0.2500、説明文ありで BERTScore 0.5496、LLM スコア 0.3000 と、一貫して改善が見られた。
特に \textit{Audio} と \textit{Gameplay} では BERTScore の上昇幅が大きく、LLM スコアも 0.2000 から 0.4000 へと改善している。
これは、アスペクトの意味領域を明示することで、LLM の注意がレビュー全般のスタイルではなく、アスペクト固有の差分（音響面、ゲーム性）に向かいやすくなった結果と解釈できる。

一方、\textit{Visual} では説明文あり条件で BERTScore が 0.5311 から 0.5186 に低下しており、汎用的すぎる説明文が「見た目」以外の要素（レビューの詳細さや長さなど）への注意を誘導した可能性がある。
このように、説明文は一般に安定化に寄与するが、その内容次第ではアスペクト固有性を弱めるリスクもある。

評価指標間の関係に関しては、BLEU が全条件で 0.0 付近に張り付いているのに対し、BERTScore と LLM スコアはアスペクトや設定の違いを敏感に反映している。
とりわけ、BERTScore が近い場合でも LLM スコアに差が出るケースがあり、BERTScore が「意味空間における距離」を測るのに対し、LLM スコアは「対比因子ラベルとしての焦点の合致」や「説明の妥当性」をより厳しく評価しているといえる。
この構造から、対比因子ラベリングの評価には、BERTScore を主要指標としつつ、LLM 評価を補助指標として併用する多面的な評価設計が必要であることが確認された。

\section{COCO Retrieved Concepts による視覚概念への拡張性}

COCO Retrieved Concepts 実験では、CLIP 類似度に基づいて抽出したキャプションの Top-100/Bottom-100 を用い、5 つの概念に対して対比因子ラベルを生成した。
正解ラベルが存在しないため、テキストベースの BERTScore（平均 0.6173）と、実画像との整合性に基づく定性的評価を組み合わせて妥当性を検証した。

concept\_0、concept\_10、concept\_50 では、「日常的な場面と物 vs フォーマルなイベントと人々」「子ども・家族・動物を含む活動」「電子機器・モバイル端末」といった対比因子ラベルが生成され、対応する画像群の視覚的特徴とよく整合していた。
これらのケースでは、LLM はキャプション差分から「イベント性」「家庭的活動」「電子機器」といった高レベル概念を抽出し、それが実際の画像に現れる被写体カテゴリや構図と一致している。
この結果は、提案手法が視覚モデルのニューロン解釈に応用可能な概念レベルの対比因子ラベルを生成し得ることを示唆している。

一方、concept\_1 や concept\_2 では、「スポーツと屋外活動」「動物と自然風景」といった対比因子ラベルが生成されたものの、Top 側の画像群には時計や信号機などスポーツとは無関係な画像が多数含まれており、対比因子ラベルは集合の一部の特徴を過度に代表させたものになっていた。
これは、LLM がキャプション中で頻出する語（例：\textit{sports}, \textit{outdoor}）に引きずられ、本質的な視覚パターン（時計、時間）を取り逃がすバイアスを持つことを示している。

これらの観察から、テキスト集合差分のみに基づく対比因子ラベル生成は、視覚的概念に対して一定の成功例を持つ一方で、キャプション側の偏りやノイズに敏感であり、画像の本質的特徴を必ずしも忠実に反映しない場合があることが分かる。
視覚モデルのニューロン解釈に応用する際には、画像から抽出した物体ラベルやシーン属性などを中間表現として併用し、テキストと視覚特徴の両方に整合する対比因子ラベルを生成する必要があることが示唆される。

\section{本手法の限界と課題}

実験結果全体から、本手法にはいくつかの限界が明確になった。
第一に、対比因子ラベルは具体的アスペクトや語彙的一貫性が高いデータセットでは高い BERTScore を達成する一方で、Steam のようなノイズが大きくマルチアスペクトなドメインでは性能が低下する。
これは、単一ニューロンの発火パターンが、必ずしも既存の人手アスペクト体系と一対一に対応しないこと、そしてレビュー言語の多義性により「何についての差分か」が曖昧になることを反映している。

第二に、BLEU のような語彙一致指標は、本タスクでは一貫して有用性を示さなかった。
正解ラベルが短いアスペクト名であり、生成ラベルが説明的フレーズであるという構造的なギャップが存在する以上、n-gram 重複に基づく指標は性能評価に適さない。
これは、対比因子ラベリングの評価設計において、意味分布ベースおよび学習ベースの指標を組み合わせる必要があることを意味する。

第三に、Few-shot ICL やアスペクト説明文は出力の安定化と性能向上に寄与するが、その効果は一様ではない。
特に \textit{Visual} のように説明文が抽象的で他アスペクトと重なりやすい場合、説明文がかえってアスペクト固有性を弱めることがある。
したがって、Few-shot 例や説明文は、対象アスペクトの境界を明確に切り出すように慎重に設計する必要がある。

第四に、COCO 実験で明らかになったように、テキストキャプションに依存した対比因子生成は、キャプション側の頻度バイアスや記述の抜け漏れに影響を受け、視覚的現実とずれたラベルを生成するリスクを持つ。
画像との整合性評価を組み込むこと、および画像特徴量と生成ラベルとの対応関係を明示的に検証することが不可欠である。

これらの限界は、提案手法が「ニューロン発火条件に対する対比因子ラベルを自動生成する」という目標に対して、どの条件で有効に機能し、どの条件で性能が制約されるかを具体的に示している。
特に、概念の具体性、データセット設計、Few-shot 設定、評価指標設計、テキストと視覚のギャップといった要因が、今後の改良において重点的に扱うべき課題であることが明らかになった。


