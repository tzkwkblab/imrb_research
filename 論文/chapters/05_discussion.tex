\chapter{考察}

本章では，第4章で示した実験結果に基づき，対比因子ラベル自動生成手法の妥当性，性能の決定要因，および限界について整理して考察する．

\section{対比因子命名タスクの実現可能性}

まず，SemEval-2014，GoEmotions，Steam の 3 データセットに対するメイン実験の結果から，対比因子命名タスクの実現可能性を評価する．
全 36 実験における BERTScore の平均は 0.6980，最小 0.5164，最大 0.8941 であり，特に SemEval-2014 に対しては平均 0.7531 と高い値を示した．
これは，グループAとグループBの集合差分のみを与えたにもかかわらず，LLM が人手アスペクトラベルと中程度以上に意味的に整合するラベルを多くのケースで生成できていることを示す結果であり，個々の実験間にはばらつきや例外も存在する．

一方で，BLEU スコアは全体平均 0.0082 とほぼ 0 に近く，Few-shot 実験や追加実験でも一貫して 0.0 付近であった．
これは第4章で述べたように，正解ラベルが短いアスペクト名であるのに対し生成ラベルが説明的フレーズとなるというタスク特性に起因しており，BLEU は本タスクでは参考指標にとどまる．

LLM による 5 段階評価を用いた意味的類似度スコアは，全体平均 0.4611 と BERTScore よりも厳しい値を示したが，SemEval では 0.5500，GoEmotions では 0.4714 と，BERTScore と整合した順位付けになっている．
このことから，BERTScore（定義は第4章を参照）は生成ラベルと正解ラベルの意味空間における距離を連続値として測定し，LLM 評価は「対比因子ラベルとして妥当か」「アスペクト名として読めるか」をより保守的に判定する補完的な指標として機能していると解釈できる．

以上の結果は，集合差分入力 $(A,B)$ からの自然言語ラベル生成というタスクに対して，LLM が 0-shot 設定でも意味的に妥当なラベルを多数のケースで生成可能であること，すなわち提案タスクが実現可能な水準にあることを示している．
なお，後続の Few-shot 設定やグループサイズ，モデル種別，アスペクト説明文有無といった追加実験に対して実施した統計的有意性検定（Friedman 検定および対応のある Wilcoxon 検定）では，いずれの条件間差も有意水準 $5\%$ では有意とはならず，本章で議論する差は平均値レベルの傾向に基づくものである．

\section{概念の具体性とデータセット特性の影響}

データセット別の性能比較から，概念の具体性とデータセット設計が対比因子ラベルの性能に強く影響することが分かる．
SemEval-2014 では BERTScore 平均 0.7531，GoEmotions では 0.7127，Steam では 0.5403 と，明確な性能差が観測された．

SemEval-2014 で扱う \textit{Food}, \textit{Service}, \textit{Battery}, \textit{Screen} などのアスペクトは，具体的な物理属性や機能に結びついた概念であり，レビュー中での語彙が比較的一貫している．
このため，グループAとグループBの差分には「食べ物の品質」「価格」「バッテリーの持続時間」「画面の解像度」といった安定した統計的パターンが現れやすく，LLM はそれらを単語レベルまたは短い説明フレーズとして抽出しやすい．
結果として，SemEval では BERTScore が高く，LLM スコアも 0.5500 と比較的高い．

GoEmotions の 28 感情カテゴリは，物理的対象ではなく内在状態を表す抽象的概念であるが，データセット設計上，各感情に対応する典型的トリガー表現や感情語彙が豊富に含まれている．
そのため，\textit{disgust}, \textit{embarrassment}, \textit{joy} など一部の感情カテゴリでは BERTScore が 0.8300 以上に達し，LLM スコアも 0.8000 と高い値を記録している．
一方で，\textit{neutral} のように境界が曖昧なカテゴリでは 0.5437 まで低下しており，概念の抽象度だけでなく，アノテーション方針と語彙的一貫性が性能に影響していることが分かる．

Steam Review Aspect Dataset では，\textit{Gameplay}, \textit{Visual}, \textit{Story}, \textit{Audio} といったアスペクトの BERTScore 平均が 0.5403 に留まり，LLM スコアも 0.3000 前後と低い．
Steam レビューは長文で雑多な記述が多く，複数アスペクトへの同時言及や皮肉的表現も頻出する．
その結果，グループAとグループBの差分が単一アスペクトにきれいに対応せず，「ゲームプレイ」「ストーリー」「雰囲気」「技術的問題」といった複数の要素が混在した対比因子が生成される傾向がある．
この構造的ノイズが，SemEval や GoEmotions と比較した際の性能低下の主因である．

これらの結果から，対比因子ラベリング性能は，(i) アスペクトの具体性，(ii) レビュー中の語彙的一貫性，(iii) グループA/Bの統計的差分の大きさ，の三要因と一定の関係を持つことが示唆された．
特に，SemEval のように人手ラベルとテキスト分布が整合的なベンチマークでは，集合差分からの自動命名が比較的容易であり，Steam のようにドメイン特有の多義性とノイズが大きい場合には難易度が高い傾向がある．
要するに，対比因子ラベリングの成否はアスペクト概念の明瞭さとレビュー言語の一貫性に大きく左右され，ノイズや多義性の大きいドメインでは人手アスペクトと整合したラベルを得ることが難しい．
次節では，これらのデータセット特性に対して Few-shot ICL による出力スタイルの制御がどの程度補償的に働くかを検証する．

\section{Few-shot ICL と出力スタイルのトレードオフ}

Few-shot 実験では，0-shot，1-shot，3-shot の 3 設定における性能の違いを検証した．
BERTScore の平均は 0-shot で 0.5526，1-shot で 0.6530，3-shot で 0.5754 であり，1-shot で最も高い値を示したが，Friedman 検定の結果（$p=0.4724$），統計的有意差は確認できなかった．
一方，LLM スコアは 0-shot で 0.3000，1-shot で 0.3500，3-shot で 0.4000 と単調増加しており，こちらも同様に統計的には有意な差は得られていない．

1-shot 設定では，プロンプト中に 1 つの例題 $(A_{\mathrm{ex}},B_{\mathrm{ex}},L_{\mathrm{ex}})$ を与えることで，出力が正解アスペクト名に近い語彙へと寄りやすくなる傾向が見られた．
この結果，SemEval 型のラベルに対して語彙的に近い短いラベルが生成されやすくなり，Few-shot 実験における BERTScore が平均値として 0.5526（0-shot）から 0.6530（1-shot）へと上昇するなどの傾向が確認されたと解釈できる（表\ref{tab:fewshot_summary}参照）．
他方で，3-shot 設定では，複数例のスタイルを平均したより説明的なラベル（複数側面をまとめたフレーズ）が出力されやすくなり，正解ラベルとの埋め込み類似度はやや低下するものの，LLM による「説明としての自然さ」「対比としての一貫性」は向上したと考えられる．

アスペクト別に見ると，\textit{Story} では 1-shot で BERTScore 0.8356，LLM スコア 0.8000 と特に高い値を示し，物語的要素を含むレビューに対して Few-shot ICL が有効であることが分かる．
一方，\textit{Visual} や \textit{Audio} では，BERTScore は 0.5700 前後であるにもかかわらず LLM スコアは 0.2000 に留まり，表層的な類似度があっても「対比因子ラベルとして有用か」という観点では厳しく評価されている．
これは，これらのアスペクトでは，対比因子ラベルが「グラフィックス」「サウンド」という表層カテゴリに留まり，具体的な差分（解像度，アートスタイル，音の存在感など）を十分に表現できていないことを反映している．

これらの結果は，Few-shot ICL が単純に性能を単調改善させる仕組みではなく，ラベルのスタイル（キーワード指向か，説明指向か）と評価指標との間にトレードオフを生じさせることを示している．
統計的な有意差は得られていないものの，0-shot でも BERTScore 0.5500 程度，LLM スコア 0.3000 程度を達成していることから，Few-shot なしでも本タスクは一定程度機能し，Few-shot の設計により「正解ラベルに近い語彙を出させるか」「説明として自然なフレーズを出させるか」をある程度制御できることが示唆された．
したがって，本タスクにおいては，定量的一致（BERTScore）の最大化を重視する場合には 1-shot 程度の例示を，説明としての自然さや対比の一貫性を重視する場合には 3-shot によるスタイル誘導を選択するといったように，傾向レベルの結果を踏まえつつ最適化したい観点に応じた Few-shot 設計が求められる．

\section{グループサイズとサンプリング戦略の影響}

Group Size 比較実験では，Steam データセットに対して group\_size を 50, 100, 150, 200, 300 と変化させた．
BERTScore の平均は，それぞれ 0.5489, 0.5514, 0.5406, 0.5487, 0.5603 であり，全体として 0.54 前後に収まりつつ，300 でわずかな上昇が見られた．Friedman 検定の結果（$p=0.3309$）および Holm 補正付きの事後比較では，いずれの group\_size 間にも統計的に有意な差は検出されておらず，これらの違いはあくまで平均値レベルの変動にとどまる．
一方，LLM スコアの平均は全ての group\_size で 0.2000 に固定されており，人手評価相当の観点からは顕著な差は検出されなかった．
これは，本評価で 5 段階評価を 0.0–1.0 に線形変換した際に 1 点が 0.2000 に対応し，今回の条件ではいずれの group\_size においてもほとんどのラベルが最低評価に集中したためであり，サンプル数の変更だけでは人手評価相当の水準で明確な改善が得られなかったことを意味する．

この結果は，50〜300 の範囲では，サンプル数の増加が対比因子ラベルの意味的類似度に与える影響は限定的であり，特定の group\_size が極端に有利または不利になることはないことを，統計的検定でも有意差が検出されなかったという事実とあわせて，平均値レベルの傾向として示唆しているにとどまる．
group\_size を増やすことで，グループAとグループBのノイズが平均化され，わずかながら BERTScore が向上する傾向はあるが，その効果は 0.01〜0.02 程度に留まる．

アスペクト別に見ると，group\_size の変化よりも，\textit{Gameplay} や \textit{Story} といったアスペクト間の難易度差の方が BERTScore・LLM スコアの変動に大きく寄与している．
したがって，本タスクにおける主なボトルネックはサンプル数よりも，データセット特性やアスペクト定義の明瞭さであると解釈できる．
実運用上は，プロンプトのコンテキスト長や計算コストを考慮しつつ，本実験で観測された傾向の範囲では 100〜200 程度の group\_size を採用することが，性能とコストのバランスをとるうえで一つの妥当な目安になると考えられる．

\section{モデル選択・プロンプト設計と評価指標の関係}

モデル比較実験では，Steam データセットに対して gpt-4o-mini と gpt-5.1 を比較した．
BERTScore の平均は gpt-4o-mini が 0.5453，gpt-5.1 が 0.5375，LLM スコアはそれぞれ 0.3000 と 0.2500 であり，新しいモデルである gpt-5.1 が一貫して優位とはならなかった．対応のある Wilcoxon 検定の結果（$n=4, p=0.8750$），これらの差はいずれも有意水準 $5\%$ では統計的に有意とはいえない．
特に \textit{Gameplay}, \textit{Story}, \textit{Visual} のアスペクトでは gpt-4o-mini が BERTScore・LLM スコアともに高く，\textit{Audio} のみ gpt-5.1 が BERTScore で上回った．

この結果は，0-shot・temperature=0 という決定論的条件では，モデルの世代よりもタスクとの整合性や事前学習分布が性能を左右しうる一方で，本実験の小規模データにおいては統計的に有意な優劣が確認できないことを示している．
すなわち，本実験の条件とサンプル数の範囲に限れば，「より新しいモデルを使えば常に良い対比因子が得られる」という前提を支持する明確な証拠は得られておらず，平均値レベルの傾向と統計的検定結果の両方を踏まえたうえで，対象ドメインとアスペクト特性に対する実測に基づくモデル選択が必要である．
同時に，ここでの知見はあくまでこの決定論的プロンプト条件におけるものであり，温度や Few-shot 例の設計を変えた場合にも同様の傾向が維持されるとは限らない点に注意が必要である．

アスペクト説明文比較実験では，同じ Steam データセットに対し，アスペクトの説明文をプロンプト先頭に付加する条件としない条件を比較した．
全体平均では，説明文なしで BERTScore 0.5395，LLM スコア 0.2500，説明文ありで BERTScore 0.5496，LLM スコア 0.3000 と，平均値レベルでは改善傾向が見られた．対応のある Wilcoxon 検定の結果（$n=4, p=0.3750$），これらの差も統計的には有意ではなく，少数サンプルで観測された傾向として解釈するのが妥当である．
特に \textit{Audio} と \textit{Gameplay} では BERTScore の上昇幅が大きく，LLM スコアも 0.2000 から 0.4000 へと改善しているが，これらも平均値レベルの改善として位置付けられる．
これは，アスペクトの意味領域を明示することで，LLM の注意がレビュー全般のスタイルではなく，アスペクト固有の差分（音響面，ゲーム性）に向かいやすくなった結果と解釈できる．

一方，\textit{Visual} では説明文あり条件で BERTScore が 0.5311 から 0.5186 に低下しており，汎用的すぎる説明文が「見た目」以外の要素（レビューの詳細さや長さなど）への注意を誘導した可能性がある．
このように，説明文は一般に安定化に寄与する一方で，粒度が広すぎたり抽象的すぎたりすると LLM の注意がアスペクト固有の視覚的特徴からレビュー全体のスタイルなどへと拡散し，かえってアスペクト固有性を弱めるリスクもある．

評価指標間の関係に関しては，BLEU が全条件で 0.0 付近に張り付いているのに対し，BERTScore と LLM スコアはアスペクトや設定の違いを敏感に反映している．
とりわけ，BERTScore が近い場合でも LLM スコアに差が出るケースがあり，BERTScore が「意味空間における距離」を測るのに対し，LLM スコアは「対比因子ラベルとしての焦点の合致」や「説明の妥当性」をより厳しく評価しているといえる．
この構造から，対比因子ラベリングの評価には，BERTScore を主要指標としつつ，LLM 評価を補助指標として併用する多面的な評価設計が必要であることが確認された．

\section{COCO Retrieved Concepts による視覚概念への拡張性}

COCO Retrieved Concepts 実験では，非教師ありに学習された潜在コンセプトと CLIP 類似度に基づいて構成された Top-100/Bottom-100 画像群に付与されたキャプションを用い，5 つの概念に対して対比因子ラベルを生成した．
正解ラベルが存在しないため，テキストベースの BERTScore（平均 0.6173）と，実画像との整合性に基づく定性的評価を組み合わせて妥当性を検証した．
ここでの BERTScore はあくまでキャプション間の意味類似度を測る参考指標であり，生成ラベルと画像との対応そのものを評価するものではない点に留意する必要がある．

concept\_0，concept\_10，concept\_50 では，「日常的な場面と物 vs フォーマルなイベントと人々」「子ども・家族・動物を含む活動」「電子機器・モバイル端末」といった対比因子ラベルが生成され，少なくともこれらのコンセプトに関しては，対応する画像群の視覚的特徴と整合的と解釈できる例が観察された．
これらのケースでは，LLM はキャプション差分から「イベント性」「家庭的活動」「電子機器」といった高レベル概念を抽出し，それが実際の画像に現れる被写体カテゴリや構図とおおむね一致していると評価された．
この結果は，提案手法が視覚モデルのニューロン解釈に応用可能な概念レベルの対比因子ラベルを生成し得る可能性を一部ケースで示唆するものであるが，正解ラベルが存在しないという制約の下での定性的評価に基づくものであり，より体系的な視覚整合性評価は今後の課題として残る．

一方，concept\_1 や concept\_2 では，「スポーツと屋外活動」「動物と自然風景」といった対比因子ラベルが生成されたものの，Top 側の画像群には時計や信号機などスポーツとは無関係な画像が多数含まれており，対比因子ラベルは集合の一部の特徴を過度に代表させたものになっていた．
これは，LLM がキャプション中で頻出する語（例：\textit{sports}, \textit{outdoor}）に引きずられ，本質的な視覚パターン（時計，時間）を取り逃がすバイアスを持つことを示している．

これらの観察から，テキスト集合差分のみに基づく対比因子ラベル生成は，視覚的概念に対して一定の成功例を持つ一方で，キャプション側の偏りやノイズに敏感であり，画像の本質的特徴を必ずしも忠実に反映しない場合があることが分かる．
視覚モデルのニューロン解釈に応用する際には，画像から抽出した物体ラベルやシーン属性などを中間表現として併用し，テキストと視覚特徴の両方に整合する対比因子ラベルを生成する必要があることが示唆される．

\section{本手法の限界と課題}

実験結果全体から，本手法にはいくつかの限界が明確になった．
第一に，対比因子ラベルは具体的アスペクトや語彙的一貫性が高いデータセットでは高い BERTScore を達成する一方で，Steam のようなノイズが大きくマルチアスペクトなドメインでは性能が低下する．
これは，単一ニューロンの発火パターンが，必ずしも既存の人手アスペクト体系と一対一に対応しないこと，そしてレビュー言語の多義性により「何についての差分か」が曖昧になることを反映している．

第二に，BLEU のような語彙一致指標は，本タスクでは一貫して有用性を示さなかった．
正解ラベルが短いアスペクト名であり，生成ラベルが説明的フレーズであるという構造的なギャップが存在する以上，n-gram 重複に基づく指標は性能評価に適さない．
これは，対比因子ラベリングの評価設計において，意味分布ベースおよび学習ベースの指標を組み合わせる必要があることを意味する．

第三に，Few-shot ICL やアスペクト説明文は出力の安定化と性能向上に寄与するが，その効果は一様ではない．
特に \textit{Visual} のように説明文が抽象的で他アスペクトと重なりやすい場合，説明文がかえってアスペクト固有性を弱めることがある．
したがって，Few-shot 例や説明文は，対象アスペクトの境界を明確に切り出すように慎重に設計する必要がある．

第四に，COCO 実験で明らかになったように，テキストキャプションに依存した対比因子生成は，キャプション側の頻度バイアスや記述の抜け漏れに影響を受け，視覚的現実とずれたラベルを生成するリスクを持つ．
画像との整合性評価を組み込むこと，および画像特徴量と生成ラベルとの対応関係を明示的に検証することは，精度向上と信頼性確保に向けて望ましい改善方策であると考えられる．

これらの限界は，提案手法が「ニューロン発火条件に対する対比因子ラベルを自動生成する」という目標に対して，どの条件で有効に機能し，どの条件で性能が制約されるかを具体的に示している．
特に，概念の具体性，データセット設計，Few-shot 設定，評価指標設計，テキストと視覚のギャップといった要因が，今後の改良において重点的に扱うべき課題であることが明らかになった．
総じて，本手法は概念が具体的でテキスト分布とラベル体系が整合的な条件では有効に機能する一方，ノイズの大きいマルチアスペクト環境やテキストと視覚表現の乖離が大きい条件では，対比因子ラベルが人間の直観とずれやすいという制約を持つ．


