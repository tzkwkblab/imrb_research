\chapter{関連研究}

本章では，第一章で述べた問題設定と貢献を踏まえ，本研究が位置づけられる枠組みと残されたギャップを整理する．
特に，(1) 個別インスタンス中心の事後説明，(2) 概念ベース XAI・非教師ありコンセプト発見・メカニスティック解釈，(3) LLM による自動ラベル生成とコントラスティブ要約，の三領域を取り上げ，それぞれがどこまで集合レベル／概念レベルの説明と命名を実現しているかを概観する．

\section{従来の事後説明手法とその限界}

深層学習モデルのブラックボックス性に対処するため，初期の XAI 研究は主に，特定の予測に対する入力特徴量の寄与度を事後的に可視化する手法に焦点を当ててきた．

\subsection{特徴量帰属手法の限界}

最も広く用いられてきた手法として，LIME（Local Interpretable Model-agnostic Explanations, Ribeiro et al., 2016）~\cite{ribeiro2016should} や SHAP（SHapley Additive exPlanations, Lundberg \& Lee, 2017）~\cite{lundberg2017unified} が挙げられる．LIME はターゲットとする予測の周囲で入力を摂動し，局所的に解釈可能な代理モデル（サロゲートモデル）を構築することで個々のインスタンスの予測根拠を可視化する．SHAP はゲーム理論に基づく Shapley 値を用いて特徴量の貢献度を統一的な枠組みで定量化する．

しかし，これらの事後説明手法には，少なくとも三つの観点から根本的な課題が指摘されている．第一に，説明の忠実性・ロバストネスの問題である．Slack らは，実データ上では差別的な決定境界を持つモデルと，摂動サンプル上でのみ用いられる見かけ上公平なモデルを組み合わせることで，LIME や SHAP が後者の挙動のみを説明し，真の判断根拠を偽装できることを示した~\cite{slack2020fooling}．また，Alvarez-Melis と Jaakkola は，距離空間上で十分近い入力ペアに対して説明もまた十分近くあるべきだとするロバストネス基準を定式化し，多くの特徴量帰属手法がこの条件を満たさないことを理論解析と実験の両面から示している~\cite{alvarez2018robustness}．第二に，倫理的・規制的な透明性の観点からの問題である．Bordt らは，事後説明アルゴリズムが本質的な曖昧さを持ち，敵対的な状況では容易に都合の良い説明だけを提示できてしまうため，GDPR における説明責任などの目的とは両立しにくいと批判する~\cite{bordt2022posthoc}．Rudin も，高責任ドメインにおいてブラックボックスモデルに事後説明を付与するよりも，当初から解釈可能なモデルクラスを用いるべきだと主張している~\cite{rudin2019stop}．

\subsection{集合レベル説明の欠如}

以上のように，従来の事後説明手法は，個々の予測に対する特徴量寄与の可視化という観点からは一定の有用性を持つ一方で，(1) 真の判断根拠の再現性や (2) 倫理的操作可能性に関する問題を抱えている．本研究にとって特に重要なのは，(3) モデルが特定のデータ集合全体に対してどのような意味的性質を学習しているのかを，集合レベルのパターンとして直接記述する手段が乏しいという点である．

特徴量帰属手法の出力は，画像における重要なピクセルやテキストにおける重要な単語といった低レベルな特徴量の寄与スコアにとどまり，例えば高価格だがサービスが良いレビュー群のように，集合レベルの意味的なまとまりを直接記述することはできない．Haedecke らは，多数の事例から類似サンプルをクラスタリングし，各クラスタの共有する特徴を自然言語で記述することでグループレベルのパターンを説明する手法を提案しており，既存のローカル説明では集合レベルの振る舞いを十分に捉えられないという問題意識を共有している~\cite{haedecke2025conceptClusters}．Hu らも，クラスタレベルの説明とインスタンスレベルの説明を概念的に区別し，グループ全体のパターン理解と個別インスタンスの所属理由との間にギャップがあることを指摘する~\cite{hu2024interpretableClustering}．言い換えれば，個々のサンプルがなぜその予測になったのかを説明する枠組みは整いつつある一方で，あるグループ（クラスタや中間表現の高活性群）全体にどのような意味的性質が共有されているかを直接記述するための枠組みは依然として不十分であり，この集合レベルの説明ギャップが残されている．本研究は，この集合レベル説明ギャップに焦点を当て，特定の概念を持つテキスト集合 $A$ と持たない集合 $B$ の間の意味的な差分を抽出し，このグループレベルのコントラストを自然言語ラベルとして記述することを目指す．

\subsection{反事実的説明}

反事実的説明もまた，対比的な要素を持つ XAI 手法として注目を集めてきた．Wachter らは，ある入力 $x$ に対して予測結果 $y$ を所望の値 $y'$ に変えるための距離制約付き最小変化を求めることで，もし年収がこれだけ高ければローンは承認された，といった行動可能な助言を与える枠組みを示した~\cite{wachter2017counterfactual}．Dhurandhar らの CEM も，予測を維持するのに十分な特徴と，除去されれば予測が変わる特徴を同時に同定することで，「なぜこのクラスであり，他ではないのか」という対比的説明を与える~\cite{dhurandhar2018cem}．
一方で，Wachter らは，反事実的説明はモデル内部構造や統計的性質を包括的に明らかにするものではなく，ブラックボックスを開くことなく個々の判断に対して行動可能な情報を与える実務的な第一歩として位置づけている．CEM もまた，個々のインスタンスに対する最小変更に基づく局所的な対比的説明を目的としており，多数のサンプルに共通する概念レベルの差分やグループ全体の振る舞いを直接扱う枠組みではない．

\section{非教師ありコンセプト発見と命名の課題}

従来の特徴量帰属手法がピクセルやトークンといった低レベル特徴の寄与度にとどまり，人間にとって直感的に理解しやすい単位でモデルの振る舞いを説明しにくいという問題意識から，「このレビューは価格に言及している」などの概念に基づいた説明を目指す研究が行なわれてきた~\cite{kim2018interpretability}．

コンセプトベース XAI（C-XAI）の代表例である TCAV（Kim et al., 2018）は，概念活性化ベクトルを用いてモデルが特定の概念にどれだけ敏感であるかを，事後説明によって定量化する~\cite{kim2018interpretability}．TCAV は既存のブラックボックスモデルを固定したまま内部表現空間における「価格」や「サービス」といった概念方向を事後的に学習し，その概念が予測スコアに与える影響を測ることで，説明の単位をピクセルやトークンといった低レベル特徴から，人間が理解しやすい概念へと引き上げる．一方で，TCAV が与えるのはあくまで入出力関係に基づく感度の解析結果，つまり事後説明であり，モデル内部でどのような機構がどの概念を計算しているのかを直接的に特定するものではない．

\subsection{人手ラベリングへの依存}

事後説明とは別の方向性として，そもそもモデル自体を概念レベルで解釈しやすい構造に設計するアプローチがある．コンセプトボトルネックモデル（Concept Bottleneck Model, CBM）は，入力から概念ベクトルを経由して最終予測を行うモデル構造を直接学習し，中間層の各次元を人間が定義した概念と一対一に対応付ける~\cite{koh2020concept}．これにより，モデルは入力ごとに各概念の有無や強さを中間層で明示的に表現し，最終予測がどの概念にもとづいて行われたのかを，中間表現から直接読み取ることが可能になる．
しかし CBM を含む従来の C-XAI は，解釈の基礎となる概念の定義（ラベリング）を人間に依存しているという構造的な問題を抱えてきた~\cite{kim2018interpretability,koh2020concept}．中間層が各概念の有無を正しく表現するように訓練するためには，どのデータにどの概念が含まれているかを示すラベルを大量に用意しなければならない．このプロセスは，高コストな概念キュレーションと呼ばれ，特に医療や科学といった専門知識が必要なドメインでの XAI 導入の最大の障壁となっている．例えばデジタル病理の WSI 診断では，ピクセル単位の手動アノテーションが病理医の大きな負担となることが指摘されている~\cite{nan2025deep}．また，SemEval-2014 のような既存の ABSA ベンチマークにおいてもアスペクトラベルは人手によるアノテーションに依存している~\cite{pontiki-EtAl:2014:SemEval2014Task4}．

\subsection{非教師あり概念抽出とメカニスティック解釈の命名問題}

この人手依存の課題を克服するため，非教師ありでモデル内部から概念を抽出する手法の研究が行われてきた．Unsupervised CBM（UCBM） は，CBM のボトルネック構造を引き継ぎつつ，人間による事前定義なしにモデルの内部表現から概念（潜在ベクトル）を自動抽出することを可能にした~\cite{schrodi2024unsupervised}．また，Compositional Concept Extraction（CCE）は，より構成的な概念の表現を抽出する~\cite{stein2024towards}．
しかしこれらの手法が発見する潜在的なベクトル表現に関しても，そのベクトルが人間にとって何を意味するのかという自然言語でのラベリングは，いずれも人手に強く依存している．

さらに，モデル内部の計算過程を直接解析するメカニスティック解釈（Mechanistic Interpretability, MI）分野においても，同様の課題が存在する．Anthropic による Attribution Graphsは，LLM の内部計算プロセスをトレースし，特徴量間の相互作用をグラフとして可視化することで回路を発見する一連のケーススタディを報告している~\cite{anthropic2025biology} ．一方で，同シリーズの手法論をまとめた Circuit Tracing では，グラフのノードとして現れる特徴量が必ずしも自然言語的に意味のある概念とは限らず，各ノードが具体的に何を検出しているかを自然言語で特定するプロセスが自動化されていない~\cite{ameisen2025attribution} ．著者ら自身も，解釈を容易にするために関連する意味を持つ特徴量を手動でスーパーノードとしてグループ化しており，この手動ステップが労働集約的であり情報の欠損を引き起こしうると議論している．

このように，C-XAI，非教師ありコンセプト発見，メカニスティック解釈のいずれにおいても，概念や特徴量に対する自然言語での命名は，高コストな人手作業として残されている．

\subsection{自動概念命名と自動解釈パイプラインの系譜}

上記の課題に対し，近年はモデル内部の概念特徴に対して自然言語ラベルを半自動的に付与するパイプラインが多数提案されている．Network Dissection は，CNN の中間ユニットの活性化マップと Broden データセットの概念マスクとの IoU を用いて，あらかじめ定義された概念集合から各ユニットに最も適合するラベルを自動で割り当てる~\cite{bau2017networkdissection}．CLIP-Dissect は，CLIP の画像・テキスト埋め込み空間を利用し，ユニットが強く反応する画像群と多数のテキスト候補との類似度にもとづいて，よりオープンボキャブラリに近い形で命名を行う~\cite{oikarinen2022clipdissect}．Label-free CBM~\cite{oikarinen2023labelfree} や Discover-then-Name CBM~\cite{rao2024discoverthenname} は，タスクに関連する概念バンク自体を GPT 系 LLM や CLIP から自動生成し，発見された潜在ベクトルに対して CLIP 類似度や LLM 出力を用いて名前を付与する枠組みを示している．

言語モデルに対しても，OpenAI による自動解釈パイプライン~\cite{openai2023neurons} や Anthropic の Auto-Interpretability 系列~\cite{anthropic2024monosemantic} のように，トップ発火トークン列を上位の LLM に入力して説明文を生成し，Simulation Score などの自動指標で説明の妥当性を評価する大規模パイプラインが構築されつつある．これらの手法は，モデル内部への直接アクセスを前提としたパイプラインであるという点で共通している．

このようにモデルの内部表現に対して自然言語ラベルを付与する手法が研究される一方で，アスペクト付きレビューのような実務的データ上で，既存のラベル体系との整合性を外部ゴールドラベルに対する意味的一致度として評価する枠組みは十分に整っていない．

本研究の主眼は，概念特徴を持つテキスト集合と持たないテキスト集合の差分を入力として LLM に命名を行わせ，外部アスペクトスキーマとの意味的一致度にもとづいて自動概念命名の達成度を定量的に検証することである．
この枠組みにより，中間表現への直接アクセスを前提とする設定で発展してきた発見・命名・内部指標による評価の系譜と，外部アスペクトスキーマを用いた対比因子ラベリングの観点を接続する．

\section{LLM を用いた自動ラベル生成とコントラスティブ要約}

前節までに見たように，モデルの中間表現への直接アクセスを前提とする系譜では，内部表現から概念を発見し，その解釈や命名までを含む自動化パイプラインが構築されつつある．一方で，本研究が扱うのは，ラベルが付与されていない状態のテキスト集合に対して，集合間コントラストのみを手がかりに LLM でラベルを生成し，外部アスペクトスキーマとの意味的一致度で評価するという問題設定である．本節では，この問題設定に関係する先行研究として，LLM を用いた自動ラベル生成とコントラスティブ要約の系譜を整理し，本研究の位置づけを明確にする．

\subsection{LLM による自動ラベル生成}

大規模言語モデル（LLM）は，その強力な文脈理解能力と自然言語生成能力により，様々なラベリングや要約タスクに応用されている．例えば，Huang と He は，テキストクラスタリングを LLM による分類問題として捉え直し，（i）データセットにもとづいて候補ラベル集合を生成し意味的に近いラベルを統合したうえで，（ii）各テキストに最も適切なラベルを割り当てる，という 2 段階の枠組みを提案している~\cite{huang2024textclustering}．このように，既存の LLM を用いた命名の多くは，単一の集合（クラスタ）を対象に，その集合を代表するラベルを与える設定を扱う．また，アスペクトベース感情分析（ABSA）においても，LLM をゼロショット／少数例プロンプト，あるいは微調整で用い，アスペクト項抽出や極性分類など，所与のタスク定義にもとづく予測を行う試みが報告されている~\cite{simmering2023llmabsa}．これらは，本研究の概念命名と問題意識を共有しつつも，命名対象が集合内の共通点や所与のラベル集合への対応づけに寄りやすい点に特徴がある．

しかし，本研究が重視するのは，単一集合の要約的命名ではなく，ある概念を持つテキスト集合 $A$ と持たないテキスト集合 $B$ の差分，すなわち集合間コントラストを自然言語ラベルとして記述することである．したがって，命名対象が集合内の共通点ではなく集合間の差分である点に本質的な違いがある．加えて，ABSA における LLM の多くの利用は，アスペクトや極性など，あらかじめ定義されたラベル体系にもとづく予測を主眼とするため~\cite{simmering2023llmabsa}，非教師ありのコントラスティブ設定で未知概念を自動発見・命名する本研究とはタスク設計が異なる．

\subsection{コントラスティブ要約とテキスト分布差分の記述}

本研究のタスク設定に概念的に最も近いのは，自然言語処理（NLP）分野における対比的要約（contrastive summarization）またはグループ差分要約（group-difference summarization）の系譜である．Str{\"o}hle らは，2 つ以上の文書群を比較し，そのうちの 1 つの集合に特有で，かつ関連性の高い差異をハイライトする要約を生成することを目的とする研究動向を整理している~\cite{strohle2024contrastive}．STRUM-LLM は，この枠組みを用いて，2 つの比較対象（例：製品 A vs 製品 B）の差分を，LLM を用いた多段階パイプラインで属性付きの構造化要約として生成する~\cite{saha2024strumllm}．また，CELL（Contrastive Explanations for Large Language Models）は，LLM の出力に対する対比的な説明（なぜその出力が選ばれ，他の出力が選ばれなかったのか）を生成するという点で，本研究と問題意識を共有しているが，単一インスタンスの出力に対する説明に焦点を当てている~\cite{luss2024cell}．

Zhong らは，2 つのテキスト分布の違いを自然言語で説明するタスクを明示的に定式化し，本研究と最も近い技術フレームを与えている~\cite{zhong2022describing}．彼らは，バイナリ分類における正例クラスと負例クラスをそれぞれ分布 $D_1, D_0$ とみなし，$D_1$ に特徴的で $D_0$ には見られない差分を，自然言語仮説（例：is about sports）として生成する．この枠組みは，データセット解析，ショートカット検出，クラスタラベリングなどに応用されている．評価は，各タスクに対して人手で用意された正例クラスの説明文との類似度（BERTScore および人手評価）にもとづく．ただし，Zhong らの主目的はテキスト分類・トピックラベリング・データセット解析であり，XAI の観点から，モデルや概念抽出器が持つ概念に外部アスペクトスキーマにもとづく名前をどこまで妥当につけられるかを系統的に検証する枠組みを主眼としてはいない．

\subsection{本研究の位置づけと新規性}

以上を踏まえると，(1) LLM によるクラスタラベリングや ChatABSA は単一集合内の共通テーマ命名や教師あり ABSA の補助に焦点を当てており，(2) 対比的要約や Group-Difference Summarization は集合間の差分を自然言語で要約する枠組みを与えており，(3) Zhong らはテキスト分布差分の自然言語仮説学習という最も近いフレームを提示している，という 3 系統が存在する．しかし，これらはいずれも，XAI の観点から LLM を概念命名モジュールとみなし，ABSA や感情・ゲームレビューといった実務的アスペクトスキーマを外部基準として，LLM がテキスト集合差分から既存アスペクト名をどこまで再発見できるかを体系的に評価することを主目的としていない．

本研究は，Zhong らのテキスト分布差分説明フレームやコントラスティブ要約の知見を踏まえ，2 つのテキスト集合 $A$ と $B$ の差分から対比因子ラベルを生成し，外部アスペクトスキーマとの意味的一致度で評価するベンチマークを構築する．SemEval-2014 ABSA，Steam Review Aspect Dataset，GoEmotions などのゴールドラベル付きデータセットを用い，アスペクトごとにテキスト集合 $A/B$ を定義する．そのうえで，LLM に集合差分のみを提示し，生成された対比因子ラベルと正解アスペクト名の意味的一致度（SBERT類似度，LLM 評価など）を測ることで，LLM の概念命名性能を XAI の観点から定量化する．

\section{本章のまとめと本研究の位置づけ}

本章では，事後説明・反事実的説明，概念ベース XAI・非教師ありコンセプト発見・メカニスティック解釈，LLM による自動ラベル生成とコントラスティブ要約の三系統の関連研究を概観した．事後説明および反事実的説明は，いずれも単一インスタンスの局所的な挙動に焦点を当てており，モデルが内部にもつ概念構造や集合レベルの差分を直接的に捉えることはできない．一方，C-XAI，非教師ありコンセプト発見，メカニスティック解釈，自動命名パイプラインは，内部表現へのアクセスを前提として概念の発見・命名・内部指標による評価という流れを確立しつつあるが，ABSA などの外部アスペクトスキーマに対する概念命名性能を横断的に評価する枠組みは未整備である．さらに，Zhong らの分布差分説明や対比的要約の枠組みは，集合差分を自然言語で要約する強力な技術フレームを提供しているものの，XAI の観点から LLM の概念命名能力を実務的スキーマにもとづいて評価することは主目的としていない．

このギャップを埋める具体的タスクとして，本研究は，概念特徴を持つテキスト集合 $A$ と持たない集合 $B$ の差分から対比因子ラベルを生成し，外部アスペクトスキーマとの意味的一致度を評価する対比因子生成タスクとベンチマークを第三章で定式化する．
