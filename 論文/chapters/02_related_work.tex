\chapter{関連研究}

本研究は，深層学習モデルの解釈可能性（XAI）において，ニューロンの発火条件を自然言語で自動命名するという，従来の手法が満たせていなかったギャップを埋めることを目的としている．
本章では，関連する先行研究を体系的に整理し，それぞれの限界を明確にすることで，本研究が提案する「集合差分による自動命名」の新規性を位置づける．
本研究は，(1) 個別インスタンス中心の事後説明手法，(2) 概念の発見に留まる非教師ありコンセプト抽出，(3) 一般的な NLP タスクに特化していたコントラスティブ要約，という三つの主要な研究領域の交差点に位置づけられる．

\section{従来の XAI 手法：個別インスタンス中心の解釈}

深層学習モデルのブラックボックス性に対処するため，初期の XAI 研究は主に，特定の予測に対する入力特徴量の寄与度を事後的に（Post-hoc）説明する手法に焦点を当ててきた．

\subsection{特徴量帰属手法の個別性}

最も広く採用されてきた手法として，LIME（Local Interpretable Model-agnostic Explanations, Ribeiro et al., 2016）~\cite{ribeiro2016should}や SHAP（SHapley Additive exPlanations, Lundberg \& Lee, 2017）~\cite{lundberg2017unified}が挙げられる．
LIME は，ターゲットとする予測の周囲でデータを摂動させ，局所的に解釈可能な代理モデル（サロゲートモデル）を構築することで，個々のインスタンスの予測根拠を可視化する．
SHAP は，ゲーム理論に基づく Shapley 値を用いて，特徴量の貢献度を統一的な枠組みで定量化する．

これらの特徴量帰属手法は，その性質上，個別インスタンスの局所的な説明に特化している．
LIME や SHAP の出力は，画像における「重要なピクセル」やテキストにおける「重要な単語」といった低レベルな特徴量の寄与スコアであり，モデルが特定のデータ集合に対して系統的に何を学習しているかを，人間が理解できる自然言語の概念（コンセプト）として説明する能力を欠いている．
本研究の目的は，特定のニューロンが発火する集合 A と発火しない集合 B の間の意味的な差分を抽出することであり，個別予測の寄与度を問う従来の XAI 手法では，このグループレベルの差異の説明というタスクを満たすことができない．

さらに，事後説明手法は信頼性の問題も指摘されている．
特に Bordt et al. (2022)~\cite{bordt2022posthoc}は，事後説明が本質的な曖昧さを持ち，敵対的な文脈において容易に操作可能であるため，法的・倫理的な透明性の目的を達成する上で「不適切 (unsuitable)」であると結論付けている．
本研究がモデルの内部状態（ニューロン発火）に直接着目する動機の一つは，この事後説明の曖昧さを回避し，より忠実な説明を提供することにある．

\subsection{反事実的説明の限界}

反事実的説明（Counterfactual Explanation）もまた，対比的（Contrastive）な要素を持つ XAI 手法として注目を集めてきた~\cite{wachter2017counterfactual}．
CEM（Contrastive Explanations Method, Dhurandhar et al., 2018）や Wachter et al. (2017) の手法は，「もし入力 $x$ の一部が $x'$ に変わったら，予測 $y$ はどう変わるか」という，単一事例に対する最小限の入力変更を特定する．
これにより，ユーザーは「この予測を覆すために何をすべきか」という行動可能な洞察を得る．

反事実的説明は，その定義上，単一インスタンスの局所的な反転に限定される．
本研究が対象とするのは，特定のニューロンの発火パターンが，データ集合全体でどのような意味的な特性を持つかを記述すること，すなわち集合間の一般的・代表的な差分を自然言語で要約するタスクである．
反事実的説明は，この集合レベルでのコントラスト記述という目標を達成できない．

\section{非教師ありコンセプト発見と命名の課題}

事後説明の限界を受け，XAI 研究の焦点は，低レベルな特徴量から，人間が理解できる高レベルの「コンセプト（概念）」に基づいた説明へと移行した~\cite{kim2018interpretability}．
コンセプトベース XAI（C-XAI）の代表例である TCAV（Kim et al., 2018）は，概念活性化ベクトルを用いてモデルが特定の概念にどれだけ敏感であるかを定量化する．

\subsection{高コストな人手ラベリング依存}

従来の C-XAI は，解釈の基礎となる概念の定義（ラベリング）を人間に依存しているという構造的な問題を抱えてきた~\cite{kim2018interpretability}．
このプロセスは，同じく Kim らによって「高コストな概念キュレーション (expensive concept curation)」と呼ばれ，特に医療や科学といった専門知識が必要なドメインでの XAI 導入の最大の障壁となっている．
SemEval-2014 のような既存の ABSA ベンチマークでさえ，アスペクト（概念）ラベルは人手によるアノテーションに依存している~\cite{pontiki-EtAl:2014:SemEval2014Task4}．

\subsection{非教師あり概念抽出の進歩と「命名」の欠如}

近年，この人手依存の課題を克服するため，非教師ありでモデル内部から概念を抽出する手法が大きく進展している~\cite{schrodi2024unsupervised,stein2024towards}．
Unsupervised CBM (UCBM, Schrodi et al., 2024) は，人間による事前定義なしに，モデルの内部表現から概念（潜在ベクトル）を自動抽出することを可能にした．
また，Compositional Concept Extraction (CCE, Stein et al., 2024)~\cite{stein2024towards}は，より構成的な概念の表現を抽出する．

UCBM や CCE の進歩にもかかわらず，これらの手法が「発見」するのは，依然として潜在的なベクトル表現である．
そのベクトルが人間にとって何を意味するのか（例：「コンセプト X」が「縞模様」を意味すること）という自然言語での「命名（ラベリング）」機能は，これらの研究には含まれていない．
命名プロセスは，発見された概念が強く発火するサンプルを研究者が手動で分析し，ラベルを付与するという，非教師あり C-XAI の「最後のワンマイル問題」として残されていた~\cite{schrodi2024unsupervised}．

本研究は，この手動命名というボトルネックに対し，UCBM や CCE が発見した概念（に対応する発火/非発火サンプル群）を LLM に入力し，その意味を直接「対比因子ラベル」として生成するスケーラブルな自動命名モジュールとして機能する．

\subsection{メカニスティック解釈における手動作業}

最先端のメカニスティック解釈（Mechanistic Interpretability, MI）分野においても，同様の課題が存在する．
Anthropic による Attribution Graphs（2025）~\cite{anthropic2025biology}は，LLM（例：Claude 3.5 Haiku）の内部計算プロセスをトレースし，特徴量間の相互作用をグラフとして可視化することで回路を発見する．
しかし，同研究においても，グラフのノードとして発見される個々の「特徴量」の意味付け，すなわちノードが具体的に何を検出しているかを自然言語で特定するプロセスは自動化されていない．

同論文の著者ら自身も，解釈を容易にするために，関連する意味を持つ特徴量を手動で「スーパーノード」としてグループ化しており，この手動ステップが「労働集約的 (labor-intensive)」であり，情報の欠損を引き起こすと認めている~\cite{ameisen2025attribution}．
本研究の提案手法は，この MI によって発見された特徴量や回路に対し，その発火条件の差分に基づき，人間が理解できる対比因子ラベルを自動で付与する手段を提供し，MI の実用化と統合的 XAI の実現に寄与する．

\section{LLM を用いた自動ラベル生成とコントラスティブ要約}

大規模言語モデル（LLM）は，その強力な文脈理解能力と自然言語生成能力により，様々なラベリングや要約タスクに応用されている．

\subsection{LLM による命名と正規化}

LLM は，テキストクラスタリングの結果に対して，クラスタ内のサンプル群の共通するテーマを要約し，クラスタラベルを自動生成するために活用されている~\cite{wang2024llmcluster}．
これにより，Embedding ベースのクラスタリングを Few-shot 学習を用いた分類タスクに変換する新たなパラダイムが提案されている．
また，アスペクトベースド感情分析（ABSA）の領域においても，LLM は Few-shot プロンプトを用いてアスペクトの抽出や，ノイズの多い合成ラベルの正規化（Normalization）に利用されている~\cite{luo2024chatabsa}．

\subsection{本研究のタスクとの乖離}

LLM による従来の命名研究の多くは，単一のデータ集合（クラスタ）内の共通点を記述することに焦点を当てている~\cite{wang2024llmcluster}．
一方，本研究は，ニューロンの発火群 A と非発火群 B の間の差分，すなわちコントラスト（対比）を記述することを目的としている．
また，ABSA における LLM の適用例（例：ChatABSA）は，主に教師ありの環境下で特定のアスペクトを抽出するタスクであり，非教師ありのコントラスティブな設定で未知の概念を自動発見・命名する本研究とはタスク設定が異なる~\cite{luo2024chatabsa}．

\subsection{コントラスティブ要約との関係}

本研究のタスク設定に概念的に最も近いのは，自然言語処理（NLP）分野における「コントラスティブ要約 (Contrastive Summarization)」または「グループ差分要約 (Group-Difference Summarization)」の系譜である~\cite{kardale2023contrastive}．
このタスクは，2 つ以上の文書群を比較し，そのうちの 1 つの集合に特有で，かつ関連性の高い差異をハイライトする要約を生成することを目的とする．

LLM を用いた先行研究として，STRUM-LLM (Saha et al., 2024)~\cite{saha2024strumllm}が挙げられる．
これは，2 つの比較対象（例：製品 A vs 製品 B）の差分を，LLM を用いた多段階パイプラインで属性付きの構造化要約として生成する．

また，Luss et al. (2024)~\cite{luss2024cell}は，CELL（Contrastive Explanations for Large Language Models）を提案し，LLM の出力に対する対比的な説明（なぜその出力が選ばれ，他の出力が選ばれなかったのか）を生成するという点で，本研究と問題意識を共有している．
一方で，CELL はあくまで個々の出力インスタンスに対する説明に焦点を当てており，本研究が対象とするような，ニューロン発火群 $A$ と非発火群 $B$ の\textbf{集合レベルの一般的・代表的な差分}を自然言語ラベルとして要約するタスクとは目的と対象が異なる．

\subsection{本研究の新規性}

STRUM-LLM は，Web 検索を含む複雑なパイプラインを構築し，一般的な製品比較タスクに特化している．
対照的に，本研究は，この「コントラスティブ要約」のフレームワークを XAI ドメイン（モデル内部状態）に初めて適用する点に新規性がある．
本研究は，ニューロンの発火群と非発火群という，より抽象的なテキスト集合の差分抽出に対し，Few-shot プロンプティングという，より簡潔なアプローチで，スケーラブルな自然言語ラベリングの実現可能性を検証するものである．
これは，LLM 命名の知見と，コントラスティブ要約のタスク定義を，XAI の文脈で融合した新規な交差点に位置づけられる．

本研究は，個別インスタンスの説明に留まる LIME/SHAP の限界，命名の課題を残す UCBM/CCE のボトルネック，そして一般 NLP タスクに留まっていたコントラスティブ要約の知見を統合することで，スケーラブルな内部状態の説明を実現する新たな手法を提案するものである．

