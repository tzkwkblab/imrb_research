\chapter{関連研究}

本章では，第一章で述べた問題設定と貢献を踏まえ，本研究が位置づけられる枠組みと残されたギャップを整理する．
具体的には，(1) 個別インスタンス中心の事後説明，(2) 概念ベース XAI・非教師ありコンセプト発見・メカニスティック解釈，(3) LLM による自動ラベル生成とコントラスティブ要約，の三領域を取り上げる．

\section{従来の事後説明手法とその限界}

深層学習モデルのブラックボックス性に対処するため，初期の XAI 研究は主に，特定の予測に対する入力特徴量の寄与度を事後説明する手法に焦点を当ててきた．

\subsection{特徴量帰属手法の限界}

最も広く採用されてきた手法として，LIME（Local Interpretable Model-agnostic Explanations, Ribeiro et al., 2016）~\cite{ribeiro2016should}や SHAP（SHapley Additive exPlanations, Lundberg \& Lee, 2017）~\cite{lundberg2017unified}が挙げられる．LIME は，ターゲットとする予測の周囲でデータを摂動させ，局所的に解釈可能な代理モデル（サロゲートモデル）を構築することで，個々のインスタンスの予測根拠を可視化する．SHAP は，ゲーム理論に基づく Shapley 値を用いて，特徴量の貢献度を統一的な枠組みで定量化する．

しかし，これらの事後説明手法には，(1) 説明の忠実性・ロバストネス，(2) 倫理的・規制的な透明性，(3) 集合レベルの意味構造の説明能力という少なくとも三つの観点から根本的な課題が指摘されている．第一に，これらの説明は入力摂動に対する出力変化から重要特徴を外挿的に推定したものであり，モデル内部で実際に用いられた判断根拠をそのまま再現しているとは限らない点である．Slack et al. (2020)~\cite{slack2020fooling}は，実データ上では差別的な決定境界を持つモデルと，摂動されたサンプル上でのみ用いられる，見かけ上公平なモデルを組み合わせることで，LIME や SHAP が観測するのは後者の挙動のみとなり，元のモデルが少数派属性に依存していても，あたかも無害な特徴に基づいて予測しているかのような説明を意図的に生成できることを示した．また，Alvarez\mbox{-}Melis と Jaakkola~\cite{alvarez2018robustness}は，距離空間上で十分近い入力ペアに対して説明もまた十分近くあるべきだとする定量的なロバストネス基準を定式化し，LIME や SHAP を含む多くの特徴量帰属手法がこの条件を満たさず，ごく小さな入力摂動に対しても重要度ベクトルが大きく変化しうることを理論解析と実験の両面から示している．

第二に，説明提供者と説明の受け手の利害が対立する文脈では，事後説明が倫理的に操作可能であるという問題がある．Bordt et al. (2022)~\cite{bordt2022posthoc}は，事後説明アルゴリズムが本質的な曖昧さを持ち，敵対的な状況では容易に都合の良い説明だけを提示できてしまうため，規制や倫理の文脈で求められる真正な透明性を達成できないと批判している．具体的には，同一のモデルと予測に対しても，特徴空間の分割方法や摂動サンプリング分布，正則化項の設定など，アルゴリズム設計上の選択に応じて複数の候補説明が生成されうること，そしてどの説明アルゴリズムを用いるかという選択自体が説明提供者の裁量に委ねられていることを指摘する．このような「説明の設計自由度」によって，企業や開発者は不利な特徴依存を隠し，有利に見える説明のみを提示する戦略を取りうるため，GDPR における説明責任の要請など，法的・倫理的な透明性の目的とは本質的に両立しにくいと論じている．さらに Rudin~\cite{rudin2019stop}は，医療や司法などの高責任ドメインにおいては，ブラックボックスモデルに事後説明を付与するという枠組み自体が不適切であり，はじめから解釈可能なモデルを採用すべきだと主張している．彼女は，スコアカードや浅い決定木など，人間が直接理解できるモデルであっても多くの実問題でブラックボックスと同等の性能が得られる事例を示し，そのような状況でなお事後説明を選択することは，Bordt らが指摘するような曖昧で操作可能な説明に依存する不要なリスクを上乗せしているに過ぎないと批判する．

\subsection{集合レベル説明の欠如}

以上のように，従来の事後説明手法は，個々の予測に対する特徴量寄与の可視化という観点からは一定の有用性を持つ一方で，(1) 説明の忠実性・ロバストネスの欠如や，(2) 規制・倫理的な文脈での説明の操作可能性といった問題を抱えている．加えて，本研究にとって特に重要なのは，(3) モデルが特定のデータ集合全体に対してどのような意味的性質を学習しているのかを，集合レベルのパターンとして記述する手段が乏しいという点である．

特徴量帰属手法の出力は，画像における「重要なピクセル」やテキストにおける「重要な単語」といった低レベルな特徴量の寄与スコアにとどまり，例えば「高価格だがサービスが良いレビュー群」といった集合レベルの意味的なまとまりを直接記述することはできない．Haedecke ら~\cite{haedecke2025conceptClusters}は，多数の事例から類似サンプルをクラスタリングし，各クラスタの共有する特徴を自然言語で記述することでグループレベルのパターンを説明する手法を提案しており，既存のローカル説明では集合レベルの振る舞いを十分に捉えられないという問題意識を共有している．また Hu ら~\cite{hu2024interpretableClustering}は，クラスタレベルの説明とインスタンスレベルの説明を概念的に区別し，グループ全体のパターン理解と個別インスタンスの所属理由との間にギャップがあることを指摘している．言い換えれば，「このサンプルがなぜこの予測になったか」を説明する仕組みは整いつつある一方で，「あるグループ（クラスタやニューロン発火群）全体にどのような意味的性質が共有されているか」を直接記述するための枠組みは依然として不十分であり，この集合レベルの説明ギャップが残されている．

本研究は，この集合レベル説明ギャップに焦点を当てる．特定のニューロンが発火するテキスト集合 $A$ と発火しないテキスト集合 $B$ の間の意味的な差分を抽出し，このグループレベルのコントラストを自然言語ラベルとして記述することを目的とする．個別予測の寄与度を問う従来の事後説明手法だけでは，この集合レベルの差異の説明というタスクを十分に満たすことができないため，本研究ではモデルの内部状態（ニューロン発火）に直接着目し，$A$ と $B$ の集合差分を対比因子ラベルとして要約する手法を検討する．

\subsection{反事実的説明の限界}

反事実的説明（Counterfactual Explanation）もまた，対比的（Contrastive）な要素を持つ XAI 手法として注目を集めてきた~\cite{wachter2017counterfactual}．
CEM（Contrastive Explanations Method, Dhurandhar et al., 2018）や Wachter et al. (2017) の手法は，「もし入力 $x$ の一部が $x'$ に変わったら，予測 $y$ はどう変わるか」という，単一事例に対する最小限の入力変更を特定する．
これにより，ユーザーは「この予測を覆すために何をすべきか」という行動可能な洞察を得る．

反事実的説明は，その定義上，単一インスタンスの局所的な反転に限定される．
本研究が対象とするのは，特定のニューロンの発火パターンが，データ集合全体でどのような意味的な特性を持つかを記述すること，すなわち集合間の一般的・代表的な差分を自然言語で要約するタスクである．
反事実的説明は，この集合レベルでのコントラスト記述という目標を達成できない．

以上のように，事後説明および反事実的説明は，いずれも単一インスタンスの局所的な挙動に焦点を当てており，モデルが内部にもつ概念構造や集合レベルの差分を直接的に捉えることはできない．
この限界を克服するため，近年は内部表現や概念空間そのものを説明の単位とするコンセプトベース XAI へと研究の焦点が移行している．

\section{非教師ありコンセプト発見と命名の課題}

前節で見たように，事後説明および反事実的説明手法は，真の判断根拠の再現性や倫理的操作可能性，グループレベル説明の欠如といった根本的な限界を抱えている．
この限界を受け，XAI 研究の焦点は，低レベルな特徴量から，人間が理解できる高レベルのコンセプトに基づいた説明へと移行した~\cite{kim2018interpretability}．
コンセプトベース XAI（C-XAI）の代表例である TCAV（Kim et al., 2018）は，概念活性化ベクトルを用いてモデルが特定の概念にどれだけ敏感であるかを定量化する．TCAV は既存のブラックボックスモデルを固定したまま内部表現空間における「価格」や「サービス」といった概念方向を事後的に学習し，その概念が予測スコアに与える影響を測ることで，説明の単位をピクセルやトークンといった低レベル特徴から人間が理解しやすい概念へと引き上げる．一方で，TCAV が与えるのはあくまで入出力関係に基づく感度の解析結果であり，モデル内部でどのような機構がどの概念を計算しているのかを直接的に特定するものではないという意味で，モデルの本質的な説明可能性を保障するものではない．この点で，TCAV は事後説明の枠組みに属しつつも概念レベルの情報を扱う中間的な手法と位置づけられる．

\subsection{人手ラベリングへの依存}

事後説明とは別の方向性として，そもそもモデル自体を概念レベルで解釈しやすい構造に設計するアプローチがある．コンセプトボトルネックモデル（Concept Bottleneck Model; CBM）~\cite{koh2020concept}は，入力から概念ベクトルを経由して最終予測を行うモデル構造を直接学習し，中間層の各次元を人間が定義した概念と一対一に対応付ける．これにより，モデルは入力ごとに各概念の有無や強さを中間層で明示的に表現し，最終予測がどの概念にもとづいて行われたのかを，中間表現から直接読み取ることが可能になる．
しかし CBM を含む従来の C-XAI は，解釈の基礎となる概念の定義（ラベリング）を人間に依存しているという構造的な問題を抱えてきた~\cite{kim2018interpretability,koh2020concept}．中間層が各概念の有無を正しく表現するように訓練するためには，どのデータにどの概念が含まれているかを示すラベルを大量に用意しなければならない．このプロセスは，同じく Kim らによって高コストな概念キュレーションと呼ばれ，特に医療や科学といった専門知識が必要なドメインでの XAI 導入の最大の障壁となっている．
第一章で述べた医療画像診断や病理スライドの注釈の事例が示すように，一枚の全スライド画像への精密なラベリングに数十分を要することもあり，専門性の高い領域ほど，この概念キュレーションの負担は実務的に看過できない水準に達している．
SemEval-2014 のような既存の ABSA ベンチマークでさえ，アスペクト（概念）ラベルは人手によるアノテーションに依存している~\cite{pontiki-EtAl:2014:SemEval2014Task4}．

\subsection{非教師あり概念抽出とメカニスティック解釈の命名問題}

近年，この人手依存の課題を克服するため，非教師ありでモデル内部から概念を抽出する手法が大きく進展している~\cite{schrodi2024unsupervised,stein2024towards}．
Unsupervised CBM (UCBM, Schrodi et al., 2024) は，CBM のボトルネック構造を引き継ぎつつ，人間による事前定義なしにモデルの内部表現から概念（潜在ベクトル）を自動抽出することを可能にした．
また，Compositional Concept Extraction (CCE, Stein et al., 2024)~\cite{stein2024towards}は，より構成的な概念の表現を抽出する．

UCBM や CCE の進歩にもかかわらず，これらの手法が「発見」するのは，依然として潜在的なベクトル表現である．
そのベクトルが人間にとって何を意味するのか（例：「コンセプト X」が「縞模様」を意味すること）という自然言語での「命名（ラベリング）」機能は，これらの研究には含まれていない．
すなわち，CBM では概念ラベルの設計とデータへの概念ラベル付けが，UCBM では発見された潜在ベクトルに対する意味付けが，いずれも人手に強く依存しており，命名プロセスは，発見された概念が強く発火するサンプルを研究者が手動で分析しラベルを付与するという，非教師あり C-XAI の「最後のワンマイル問題」として残されていた~\cite{schrodi2024unsupervised,stein2024towards}．

さらに，モデル内部の計算過程を直接解析するメカニスティック解釈（Mechanistic Interpretability, MI）分野においても，同様の課題が存在する．
Anthropic による Attribution Graphs（2025）~\cite{anthropic2025biology}は，LLM（例：Claude 3.5 Haiku）の内部計算プロセスをトレースし，特徴量間の相互作用をグラフとして可視化することで回路を発見する．
しかし，同研究においても，グラフのノードとして発見される個々の「特徴量」の意味付け，すなわちノードが具体的に何を検出しているかを自然言語で特定するプロセスは自動化されていない．
著者ら自身も，解釈を容易にするために，関連する意味を持つ特徴量を手動で「スーパーノード」としてグループ化しており，この手動ステップが労働集約的であり，情報の欠損を引き起こすと認めている~\cite{ameisen2025attribution}．

このように，C-XAI，非教師ありコンセプト発見，メカニスティック解釈のいずれにおいても，概念や特徴量に対する自然言語での命名・説明は，高コストな人手作業として残されている．

\subsection{自動概念命名と自動解釈パイプラインの系譜}

上記の課題に対し，近年はモデル内部の概念特徴に対して自然言語ラベルを半自動的に付与するパイプラインが多数提案されている．Network Dissection~\cite{bau2017networkdissection}は，CNN の中間ユニットの活性化マップと Broden データセットの概念マスクとの IoU を用いて，あらかじめ定義された概念集合から各ユニットに最も適合するラベルを自動で割り当てる．CLIP-Dissect~\cite{oikarinen2022clipdissect}は，CLIP の画像・テキスト埋め込み空間を利用し，ユニットが強く反応する画像群と多数のテキスト候補との類似度にもとづいて，よりオープンボキャブラリに近い形で命名を行う．Label-free CBM~\cite{oikarinen2023labelfreecbm}や Discover-then-Name CBM~\cite{rao2024discoverthenname}は，タスクに関連する概念バンク自体を GPT 系 LLM や CLIP から自動生成し，発見された潜在ベクトルに対して CLIP 類似度や LLM 出力を用いて名前を付与する枠組みを示している．

言語モデルに対しても，OpenAI による自動解釈パイプライン~\cite{openai2023neurons}や Anthropic の Auto-Interpretability 系列~\cite{anthropic2024monosemantic}のように，トップ発火トークン列を上位の LLM に入力して説明文を生成し，Simulation Score などの自動指標で説明の妥当性を評価する大規模パイプラインが構築されつつある．これらの手法は，(1) モデル内部アクティベーションへの直接アクセスを前提とし，(2) IoU，CLIP 類似度，Simulation Score など，主に内部挙動ベースのスコアを評価軸とするという点で共通している．一方で，アスペクト付きレビューのような実務的データ上で，既存のラベル体系との整合性を外部ゴールドラベルに対する意味的一致度として評価する枠組みは十分に整っていない．
本研究は，これらの自動命名パイプラインの系譜と，ABSA ベンチマークにもとづく対比因子生成タスクとの橋渡しを試みるものであり，概念特徴を持つテキスト集合と持たないテキスト集合の差分を入力として LLM に命名を行わせることで，モデル内部表現への直接アクセスを必須としないブラックボックス寄りの設定で自動概念命名の達成度を検証する．

\section{LLM を用いた自動ラベル生成とコントラスティブ要約}

前節までに見たように，非教師ありコンセプト発見やメカニスティック解釈は，モデル内部から潜在的な構造や概念を抽出することには成功している一方で，それらに対応する自然言語ラベルの自動生成という課題を残している．本節では，この「概念命名」と「対比的説明」という二つの観点から，LLM を用いたラベル生成とコントラスティブ要約の先行研究を整理し，本研究との位置づけの違いを明らかにする．
大規模言語モデル（LLM）は，その強力な文脈理解能力と自然言語生成能力により，様々なラベリングや要約タスクに応用されている．

\subsection{LLM による自動ラベル生成}

LLM は，テキストクラスタリングの結果に対して，クラスタ内のサンプル群の共通するテーマを要約し，クラスタラベルを自動生成するために活用されている~\cite{wang2024llmcluster}．
これにより，Embedding ベースのクラスタリングを Few-shot 学習を用いた分類タスクに変換する新たなパラダイムが提案されている．
また，アスペクトベースド感情分析（ABSA）の領域においても，LLM は Few-shot プロンプトを用いてアスペクトの抽出や，ノイズの多い合成ラベルの正規化（Normalization）に利用されている~\cite{luo2024chatabsa}．
これらは，単一集合内の共通点を自然言語で命名・正規化するという点で，本研究の「概念命名」の側面と関連している．

しかし，LLM による従来の命名研究の多くは，あくまで単一のデータ集合（クラスタ）内の共通点を記述することに焦点を当てている~\cite{wang2024llmcluster}．
一方，本研究は，ニューロンの発火群 A と非発火群 B の間の差分，すなわち対比を記述することを目的としており，命名対象が「集合内の共通点」ではなく「集合間の差分」である点に本質的な違いがある．
また，ABSA における LLM の適用例（例：ChatABSA）は，主に教師ありの環境下で特定のアスペクトを抽出するタスクであり，非教師ありのコントラスティブな設定で未知の概念を自動発見・命名する本研究とはタスク設定が異なる~\cite{luo2024chatabsa}．

\subsection{コントラスティブ要約との関係}

本研究のタスク設定に概念的に最も近いのは，自然言語処理（NLP）分野における「対比的要約 (Contrastive Summarization)」または「グループ差分要約 (Group-Difference Summarization)」の系譜である~\cite{kardale2023contrastive}．
このタスクは，2 つ以上の文書群を比較し，そのうちの 1 つの集合に特有で，かつ関連性の高い差異をハイライトする要約を生成することを目的とする．

第一章で定義した対比因子生成タスクは，このコントラスティブ要約の系譜に属しつつ，その比較対象をニューロン発火群と非発火群といったモデル内部状態に拡張する点に新規性がある．

LLM を用いた先行研究として，STRUM-LLM (Saha et al., 2024)~\cite{saha2024strumllm}が挙げられる．
これは，2 つの比較対象（例：製品 A vs 製品 B）の差分を，LLM を用いた多段階パイプラインで属性付きの構造化要約として生成する．

また，Luss et al. (2024)~\cite{luss2024cell}は，CELL（Contrastive Explanations for Large Language Models）を提案し，LLM の出力に対する対比的な説明（なぜその出力が選ばれ，他の出力が選ばれなかったのか）を生成するという点で，本研究と問題意識を共有している．
一方で，CELL はあくまで個々の出力インスタンスに対する説明に焦点を当てており，本研究が対象とするような，ニューロン発火群 $A$ と非発火群 $B$ の\textbf{集合レベルの一般的・代表的な差分}を自然言語ラベルとして要約するタスクとは目的と対象が異なる．

\subsection{本研究の新規性}

STRUM-LLM は，Web 検索を含む複雑なパイプラインを構築し，一般的な製品比較タスクに特化している．
対照的に，本研究は，この「コントラスティブ要約」のフレームワークを XAI ドメイン（モデル内部状態）に初めて適用する点に新規性がある．
本研究は，ニューロンの発火群と非発火群という，より抽象的なテキスト集合の差分抽出に対し，Few-shot プロンプティングという，より簡潔なアプローチで，スケーラブルな自然言語ラベリングの実現可能性を検証するものである．
これは，LLM 命名の知見と，コントラスティブ要約のタスク定義を，XAI の文脈で融合した新規な交差点に位置づけられる．

本研究は，個別インスタンスの説明に留まる LIME/SHAP の限界，命名の課題を残す UCBM/CCE ，そして一般 NLP タスクに留まっていた対比的要約の知見を統合することで，スケーラブルな内部状態の説明を実現する新たな手法を提案するものである．
具体的には，第一章で述べた二つの貢献，すなわち (1) C-XAI やメカニスティック解釈における概念命名作業の自動化に向けて，ニューロン発火群と非発火群のテキスト集合から対比因子ラベルを生成するタスクを定式化し，LLM を用いた自動命名モジュールとして機能しうることを示す点，(2) 個別インスタンス中心の事後説明が対応できない集合レベルの差分記述と，非教師ありコンセプト発見手法における発見概念への自然言語命名の人手依存という二つの課題を，ニューロン発火群と非発火群の集合差分にもとづく自然言語ラベリングという単一のタスクとして統一的に位置づけ直す点により，関連研究のギャップを埋める．
本章で整理した先行研究の限界を踏まえ，次章では対比因子生成タスクと提案手法の定式化について述べる．
