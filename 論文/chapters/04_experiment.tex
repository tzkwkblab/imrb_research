\chapter{評価実験}

\section{実験の目的と概要}
本章では，提案手法の評価実験の目的，使用データセット，実験設定，評価指標を述べる．実験結果と考察は第5章で報告する．
本実験の目的は，外部ラベルにもとづいて定義した 2 つのテキスト集合 $A/B$ の差分から，LLM（GPT-4o-mini 等）が対比因子ラベルをどの程度生成できるか，また生成ラベルが人手アノテーションによる正解ラベル（またはその説明文）とどの程度意味的に一致するかを，定量的に評価することである．

提案手法のドメイン汎用性を検証するために，レビュー，感情分類，画像キャプションという多様なドメインに属する 4 種類のデータセットを用いた．
これらのデータセットは，それぞれが持つ正解アスペクトラベルを，LLM が生成する対比因子ラベルの妥当性を評価するためのグラウンド・トゥルースとして使用した．
使用したデータセットの概要を表~\ref{tab:dataset_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{使用データセットの概要}
  \label{tab:dataset_overview}
  \begin{tabular}{lll}
    \toprule
    データセット & ドメイン & 特徴 \\
    \midrule
    SemEval-2014 ABSA & レビュー & 具体的アスペクト（Food，Service等） \\
    GoEmotions & 感情分類 & 抽象的概念（28感情カテゴリ） \\
    Steam Review Aspect Dataset & レビュー & ドメイン固有で抽象度の高いアスペクト（Gameplay等） \\
    COCO Retrieved Concepts & 画像キャプション & 視覚的概念記述 \\
    \bottomrule
  \end{tabular}
\end{table}

本実験は，以下の 6 つの実験カテゴリで構成された．
データセット別比較では，SemEval-2014 ABSA，GoEmotions，Steam Review Aspect Dataset の 3 データセットを用いて，提案手法の基本性能を評価した．
Few-shot 設定による性能比較実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．
グループサイズの影響分析実験では，group\_size を 50，100，150，200，300 の 5 段階で変化させた．
モデル比較実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを比較した．
アスペクト説明文の効果検証実験では，アスペクト説明文の有無による性能差を検証した．
COCO Retrieved Concepts 実験では，正解ラベルがない画像キャプションデータセットに対する対比因子生成を検証した．
各実験カテゴリの概要を表~\ref{tab:experiment_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{実験カテゴリの概要}
  \label{tab:experiment_overview}
  \begin{tabular}{ll}
    \toprule
    実験カテゴリ & 目的・検証内容 \\
    \midrule
    データセット別比較 & 基本性能評価 3データセット \\
    Few-shot実験 & 0/1/3-shot設定の比較 \\
    グループサイズ比較 & group\_size（50--300）の影響分析 \\
    モデル比較 & GPT-4o-mini vs GPT-5.1 \\
    アスペクト説明文比較 & 説明文の有無による性能差 \\
    COCO実験 & 正解ラベルなしデータセットでの検証 \\
    \bottomrule
  \end{tabular}
\end{table}

評価の観点として，以下の 3 つの指標を用いた（各指標の詳細な定義は\ref{subsec:evaluation_metrics}節を参照）．
有効性の評価には，Sentence-BERT（SBERT）文埋め込みのコサイン類似度を 0.0〜1.0 に正規化した値（以降，SBERT類似度）を主要指標として使用し，生成ラベルと正解ラベルの意味的類似度を測定した．
汎用性の評価には，複数のドメイン（レビュー，感情分類，画像キャプション）と複数のアスペクトタイプ（具体的アスペクト，抽象的概念）に対する性能を測定した．
性能の評価には，BLEU スコアを参考指標として使用し，語彙レベルの一致度を補助的に確認した．
また，LLM による意味的類似度評価を補助指標として用い，GPT-4o-mini による 5 段階評価を実施した．
用いた評価指標の概要を表~\ref{tab:evaluation_metrics_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{評価指標の概要}
  \label{tab:evaluation_metrics_overview}
  \begin{tabular}{lll}
    \toprule
    評価指標 & 役割 & 位置づけ \\
    \midrule
    SBERT類似度 & 意味的類似度測定 & 主要指標 \\
    BLEU & 語彙レベル一致度確認 & 参考指標 \\
    LLM評価 & 意味的類似度評価（5段階） & 補助指標 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{データセット}
\label{sec:dataset}

\subsection{データセットの選定理由}
本実験では，ドメインの多様性を確保するため，以下の 4 種類のデータセットを選定した．
SemEval-2014 ABSA は，アスペクトベース感情分析の標準的なベンチマークとして広く使用されており，正解ラベルが人手でアノテーションされている~\cite{pontiki-EtAl:2014:SemEval2014Task4}．
GoEmotions は，感情という抽象的な概念を扱うデータセットであり，具体的な物理的実体を持たない概念の命名精度を測るために選定した~\cite{demszky2020goemotions}．
Steam Review Aspect Dataset は，ゲームという特定の製品ドメインに特化したデータセットであり，専門性の高いテキスト集合間の意味的差分を抽出する能力を検証するために選定した~\cite{srec:steam-review-aspect-dataset}．
Retrieved Concepts（COCO Captions）は，画像キャプションデータセットであり，視覚的概念記述の生成能力を検証するために選定した~\cite{lin2014microsoft}．

ドメインの多様性として，レビューテキスト（SemEval-2014，Steam），感情分類テキスト（GoEmotions），画像キャプション（COCO）という異なるテキストタイプを網羅した．
また，具体的なアスペクト（Food，Price，Gameplay，Technical）と抽象的な概念（Atmosphere，Story，感情カテゴリ）を含むデータセットを扱うことで，概念の性質やデータセット特性と性能の関係を考察できるようにした．

以降の考察でデータセット特性を定量的に参照するため，テキスト統計の概要を表~\ref{tab:text_stats_overview}に示す（語数は小文字化後の空白分割による）．
\begin{table}[htbp]
  \centering
  \caption{テキスト統計の概要（train/dev/test 合算）}
  \label{tab:text_stats_overview}
  \begin{tabular}{lrrrrrr}
    \toprule
    データセット & 件数 & 平均語数 & 中央語数 & 最大語数 & 平均ラベル数 & 複数ラベル率 \\
    \midrule
    GoEmotions & 54,263 & 12.83 & 12 & 33 & 1.18 & 16.25\% \\
    Steam Review Aspect & 1,100 & 244.70 & 156 & 1,473 & 3.27 & 90.27\% \\
    SemEval-2014 restaurant14 & 4,728 & 19.41 & 18 & 79 & 1.00 & 0.00\% \\
    SemEval-2014 laptop14 & 2,966 & 20.75 & 18 & 83 & 1.00 & 0.00\% \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Steam Review Aspect Dataset}
Steam Review Aspect Dataset は，Steam ゲームレビューから収集されたテキストデータであり，特定のゲームアスペクトに関する言及を含むデータセットである~\cite{srec:steam-review-aspect-dataset}．
本データセットは英語の Steam ゲームレビュー 1,100 件（学習用 900 件，テスト用 200 件）で構成され，データ収集は SRec データベースのスナップショットに基づき行われた．レビューを特徴づける 8 種類のアスペクトが人手でアノテーションされており，Recommended（推奨），Story（物語），Gameplay（ゲームプレイ），Visual（視覚），Audio（聴覚），Technical（技術），Price（価格），Suggestion（提案・要望）からなる．ゲームという特定の製品ドメインに特化しており，特に \texttt{Gameplay} や \texttt{Technical} といったゲーム固有のメカニクスや技術的側面に関するアスペクトを含む．テストセットにおけるアスペクト件数の例として，Gameplay が 154 件，Recommended が 148 件，Story が 89 件である．
レビュー長は平均 244.70 語（中央値 156 語，最大 1,473 語）であり，語彙タイプ数は 29,056 語である．また，1 サンプルあたりの平均ラベル数は 3.27 で，複数アスペクトを併記するレビューは 90.27\% を占める．

実験での使用方法として，各アスペクトについて，そのアスペクトを含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出した．
分割タイプは \texttt{aspect\_vs\_others} を用い，特定のアスペクトが含まれるテキストと含まれないテキストを比較した．
グループ A と B の抽出は，各アスペクトのラベルに基づいて行い，group\_size パラメータに応じてサンプル数を調整した．
データセット別比較では，group\_size = 100 を用い，各グループから最大 100 件のテキストを抽出した．

% データセット別比較で使用したアスペクト一覧は第\ref{sec:experiment_setup}節でまとめて述べる．

\subsection{SemEval-2014 ABSA（Restaurants）}
SemEval-2014 ABSA（Restaurants）は，アスペクトベース感情分析（ABSA）の標準的なベンチマークとして広く使用されるレストランレビューのデータセットである~\cite{pontiki-EtAl:2014:SemEval2014Task4}．
レストランレビューのテキストを含み，各文に対してアスペクト（観点）とそれに対する感情極性が人手でアノテーションされている．本研究では主に Food（食べ物），Service（サービス），Price（価格），Atmosphere（雰囲気）の 4 種類のアスペクトを用いる．Food や Price は具体的な名詞や数値に関連する言及が中心となる一方，Atmosphere は広範な文脈や比喩的表現からの推論を必要とし，データセット特性と命名性能の関係を考察する際の比較対象となる．
本データセットは 1 文あたり 1 アスペクトを前提としており，restaurant14 は平均 19.41 語，laptop14 は平均 20.75 語と短文である（表~\ref{tab:text_stats_overview}）．

実験での使用方法として，SemEval-2014 データセットにおいて，Restaurant ドメインから Food と Service，Laptop ドメインから Battery と Screen の 4 アスペクトを用いた．
各アスペクトについて，そのアスペクトを含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出し，split\_type = \texttt{aspect\_vs\_others} で分割した．

使用したアスペクトの選定理由として，Food は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトとして，Service，Battery，Screen は製品の属性に関する具体的なアスペクトとして選定した．
これにより，具体的なアスペクトにおける命名性能を評価できるようにした．

\subsection{GoEmotions}
GoEmotions は，細粒度感情分類タスクのために Reddit コメントから収集されたデータセットである~\cite{demszky2020goemotions}．
Demszky らによって構築された総レコード数 63,812 件から成るマルチラベル形式のデータセットであり，28 の感情カテゴリ（27 感情 + neutral）でラベル付けされている．主要なカテゴリには Joy（喜び），Anger（怒り），Admiration（称賛），Neutral（中立）などが含まれる．感情という抽象的概念をテキスト集合差分から推論する必要があるため，具体的な物理的実体を持たない概念の命名精度を検証する目的で用いた．実験では任意の感情アスペクト（例：joy）を指定し，その感情を含むテキスト群 $A$ とその他のアスペクトを含むテキスト群 $B$ を比較する設定とした．
平均 12.83 語の短文が中心であり，複数感情ラベルを併記するサンプルは 16.25\% である（表~\ref{tab:text_stats_overview}）．

実験での使用方法として，GoEmotions データセットから全 28 感情カテゴリを用いた．
各感情カテゴリについて，その感情を含むテキスト群をグループ A，含まないテキスト群をグループ B として抽出し，split\_type = \texttt{aspect\_vs\_others} で分割した．

28 感情カテゴリの選定理由として，感情は物理的な実体を持たない高度に抽象的な概念であり，具体的なアスペクトと比較して命名精度が低下する傾向があるかを検証するために，全カテゴリを対象とした．
これにより，抽象的な概念における命名性能を包括的に評価できるようにした．

\subsection{Retrieved Concepts（COCO Captions）}
Retrieved Concepts（COCO Captions）は，視覚的概念記述の生成能力を検証するために，画像キャプションデータセット COCO に基づいて構築されたデータセットである~\cite{lin2014microsoft}．
MS-COCO 2017 train split の画像に対して，実験協力者のFarnoosh Javar によって訓練された非教師ありコンセプト発見モデルから得られた 300 個の潜在コンセプト埋め込みと，CLIP（ViT-B/32）による画像埋め込みとのコサイン類似度を計算し，各コンセプトについて類似度が高い画像 Top-100 と低い画像 Bottom-100 を取得している．この潜在コンセプト埋め込みは，UCBM~\cite{schrodi2024unsupervised} に代表される辞書学習型の非教師ありコンセプト発見手法と同様に，既存モデルの中間表現から自動的に概念ベクトルを抽出するタイプのモデルにより学習されているが，本論文では上流モデルの詳細には立ち入らず，得られた concept embeddings と，それに対して CLIP 類似度にもとづき取得された Top/Bottom 画像およびその COCO 由来キャプションのみを利用する．
各画像には COCO 由来の人手キャプションが 5 つ付与されており，本研究ではこれらのキャプションのみを集合 $A$, $B$ の要素として利用する．
非教師ありに学習された 300 の潜在コンセプト（concept\_0 ～ concept\_299）に対し，CLIP 類似度にもとづき取得された Top-100/Bottom-100 画像とそのキャプションからなる．各コンセプトに対して人手の正解ラベル（アスペクト名）は与えられておらず，潜在コンセプトとその Top/Bottom 例のみが提供されるため，正解アスペクト名が与えられない設定において，集合差分から対比因子ラベルを生成する挙動を補助的に検証できる．

実験では，300 コンセプトのうち concept\_0，concept\_1，concept\_2，concept\_10，concept\_50 の 5 コンセプトを用いた．
各コンセプトについて，潜在コンセプト埋め込みと画像埋め込みとの CLIP（ViT-B/32）コサイン類似度に基づき，類似度が高い順に Top-100，低い順に Bottom-100 の画像を選び，それらに付与されたキャプションをグループ A（Top 側），グループ B（Bottom 側）として用いた．
分割タイプは \texttt{aspect\_vs\_bottom100} を用いた．

正解ラベルが存在しないため，SBERT類似度 と BLEU スコアは参考値として記録するにとどめ，主に生成された対比因子と対応する画像群との整合性に基づく定性的評価を行った．

\section{実験設定}
\label{sec:experiment_setup}

\subsection{実験パイプラインの概要}
本実験では，コントラスティブ要約に基づく対比因子ラベル生成器として，GPT-4o-mini を含む複数の大規模言語モデルを用いた．
提案手法は，統一されたパイプラインとして構築され，その目的は，特定の概念に対応するテキスト集合 $A$（グループA）と，そうでないテキスト集合 $B$（グループB）の差分から，意味的な対比因子ラベル $L$ を LLM に生成させることである．本章の実験では，第\ref{sec:dataset}節で述べたとおり，SemEval，GoEmotions，Steam では人手アスペクトラベルにもとづき，COCO Retrieved Concepts では Top/Bottom 構造にもとづき，グループA/Bを構成する．

\begin{itemize}
  \item \textbf{タスク定式化}: 第3章で定義したとおり，集合 $A$ と $B$ の差分から自然言語ラベル $L$ を生成する写像 $\text{textContrastiveNaming}(A,B) \to L$ を用いる．本章の実験では，各データセットごとに定義されたグルーピング規則にもとづき構成したグループA/Bを入力として用いる．
  \item \textbf{グルーピング}: 各データセットごとに定義された規則に従い，ハイパーパラメータ $group\_size$ を用いて集合 $A$ と $B$ を抽出する．SemEval，GoEmotions，Steam ではアスペクトラベルの有無にもとづき，COCO Retrieved Concepts では各潜在コンセプトに対する Top-100/Bottom-100 画像のキャプションにもとづいてグループA/Bを構成する．
  データセット別比較では，プロンプトのコンテキスト長制限を考慮し，$group\_size = 100$ を用いた．
  \item \textbf{Few-shot ICL の検証}: LLM の出力形式の揺らぎや語彙の安定性を確保するための検証手段として，Few-shot インコンテキスト・ラーニング（ICL）のバリエーション（0-shot, 1-shot, 3-shot）を定量的に検証した．
  LLM は，プロンプト内でグループAとグループBを比較し，グループAに特徴的でグループBには欠如している意味的側面を推論するよう指示された．
\end{itemize}

実験パイプラインの全体フローを以下に示す．

\begin{enumerate}
  \item \textbf{データセット読み込み}：各データセットからテキストを読み込む．
\item \textbf{グループA/B抽出}：各データセットで定義されたグルーピング規則（アスペクトラベル，あるいは Retrieved Concepts における Top/Bottom 構造）に基づき，グループA（特定概念を含むテキスト群）とグループB（含まないテキスト群）を抽出する．
  \item \textbf{プロンプト生成}：グループAとグループBのテキストリストをプロンプトに組み込む．Few-shot例が設定されている場合は，プロンプトにFew-shot例を挿入する．
  \item \textbf{LLMによる対比因子ラベル生成}：GPT-4o-mini等のLLMにプロンプトを入力し，対比因子ラベルを生成する．
  \item \textbf{評価}：生成されたラベルと正解ラベルの意味的類似度をSBERT類似度，BLEU，LLM評価により測定する．
\end{enumerate}

データセット別比較で対象としたアスペクトは，SemEval-2014 から Food，Service，Battery，Screen の 4 種類，GoEmotions から全 28 感情カテゴリ，Steam から Gameplay，Visual，Story，Audio の 4 種類である．

\subsection{LLMモデルとパラメータ設定}
本実験では，対比因子ラベル生成器として GPT-4o-mini を主要モデルとして用いた．
モデル比較実験では，GPT-5.1 も使用した．
GPT-4o-mini を選択した理由は，コスト効率が高く，かつ十分な性能を発揮することが既存研究で確認されているためである．

各実験カテゴリでのモデル選択として，データセット別比較，Few-shot 実験，グループサイズ比較実験では GPT-4o-mini を使用した．
アスペクト説明文比較実験では GPT-4o を使用した．
モデル比較実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを比較した．
COCO Retrieved Concepts 実験では，GPT-4o-mini を使用した．

温度パラメータ（temperature）の設定として，データセット別比較では temperature = 0.0 を用いた．
この設定により，決定論的な出力が得られ，実験の再現性が確保される．
Few-shot 実験，グループサイズ比較実験，モデル比較実験，アスペクト説明文比較実験においても temperature = 0.0 を用いた．
COCO Retrieved Concepts 実験においても temperature = 0.0 を用いた．

最大トークン数（max\_tokens）の設定として，データセット別比較では max\_tokens = 2000 に設定した．
Few-shot 実験，モデル比較実験では max\_tokens = 100 に設定した．
グループサイズ比較実験，アスペクト説明文比較実験，COCO Retrieved Concepts 実験では max\_tokens = 2000 に設定した．

その他の生成パラメータとして，top\_p や frequency\_penalty はデフォルト値を使用した．

\subsection{プロンプト設計}
プロンプトテンプレートは，以下の構造を持つ．
まず，タスクの説明として \texttt{2つのデータグループを比較して，グループAに特徴的でグループBには見られない表現パターンや内容の特徴を特定してください} という指示を提示する．
次に，Few-shot 例が存在する場合は \texttt{examples\_section} に挿入される．
Few-shot 例の形式は \texttt{【例題N】グループA: [...] グループB: [...] 回答: [正解ラベル]} である．
その後，実際のデータとして【グループA】と【グループB】のテキストリストが提示される．
各テキストは \texttt{- [テキスト内容]} の形式で列挙され，コンテキスト長制限を考慮して最大 100 件に制限される．
最後に，出力形式の指示として \texttt{英語で5-10単語程度で，グループAに特徴的でグループBには見られない主要な違いを簡潔に回答してください} が追加される．

グループA/Bの提示方法として，各テキストは \texttt{- [テキスト内容]} の形式で列挙され，グループAとグループBはそれぞれ \texttt{【グループA】} と \texttt{【グループB】} の見出しで区別された．
コンテキスト長制限を考慮し，各グループから最大 100 件のテキストを抽出した．

出力形式の指示方法として，\texttt{英語で5-10単語程度で，グループAに特徴的でグループBには見られない主要な違いを簡潔に回答してください} という指示をプロンプトの末尾に追加した．

Few-shot 例の挿入方法として，Few-shot 設定が 0 より大きい場合，プロンプトのタスク説明の後に \texttt{examples\_section} を挿入した．
Few-shot 例の形式は \texttt{【例題N】グループA: [...] グループB: [...] 回答: [正解ラベル]} であり，$N$ は例題番号である．

アスペクト説明文の使用方法として，アスペクト説明文比較実験では，アスペクトの説明文をプロンプトの冒頭に追加した．
例えば，Food アスペクトの場合，\texttt{Food refers to mentions of food quality, taste, menu items, or dining experience} といった説明文を挿入した．
説明文ありの条件と説明文なしの条件を比較することで，アスペクト説明文の効果を検証した．

\subsection{データの前処理と分割方法}
テキストの前処理手順として，各データセットからテキストを読み込み，アスペクトラベルに基づいてグループ A とグループ B に分割した．
テキストの前処理として，特殊文字の処理や正規化は行わず，データセットの生のテキストをそのまま使用した．

グループA/Bの抽出方法として，各アスペクトについて，そのアスペクトを含むテキスト群をグループA，含まないテキスト群をグループBとして抽出した．
分割タイプは \texttt{aspect\_vs\_others} を用い，特定のアスペクトが含まれるテキストと含まれないテキストを比較した．
COCO Retrieved Concepts 実験では，分割タイプとして \texttt{aspect\_vs\_bottom100} を用い，Top-100のキャプションをグループA，Bottom-100のキャプションをグループBとして抽出した．

グループサイズ（group\_size）の決定方法として，データセット別比較では group\_size = 100 を用いた．
この値は，プロンプトのコンテキスト長制限を考慮して決定された．
グループサイズ比較実験では，group\_size を 50，100，150，200，300 の 5 段階で変化させた．

サンプリングは，各グループの候補集合から group\_size 件になるようランダムにサブサンプリングし，候補数が group\_size 未満の場合は重複を許して補完した．重み付きサンプリングは使用しなかった．

コンテキスト長制限への対応として，各グループから最大 100 件のテキストを抽出し，プロンプトのコンテキスト長を制限内に収めた．
データセット別比較では group\_size = 100 を用い，グループサイズ比較実験では最大 300 まで検証したが，コンテキスト長超過エラーを回避するため，実際のプロンプトでは必要に応じてテキスト数を制限した．

\subsection{Few-shot例の作成方法}
Few-shot例の選定基準として，各データセットのアスペクトラベルに基づき，正解ラベルが明確な例を選定した．
Few-shot例は，グループAとグループBのテキストリストと，それに対応する正解ラベルで構成された．

例の品質管理方法として，Few-shot 例は，各データセットのアスペクトラベルに基づいて作成し，正解ラベルが明確であることを確認した．
Few-shot 例の形式は \texttt{【例題N】グループA: [...] グループB: [...] 回答: [正解ラベル]} であり，$N$ は例題番号である．

0-shot，1-shot，3-shot の違いと設定方法として，0-shot 設定では Few-shot 例を挿入せず，タスク説明のみを提示した．
1-shot 設定では，1 つの Few-shot 例を \texttt{examples\_section} に挿入した．
3-shot 設定では，3 つの Few-shot 例を \texttt{examples\_section} に挿入した．
Few-shot 実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．

\subsection{実験カテゴリの定義}
各実験カテゴリのパラメータ設定を表~\ref{tab:experiment_config}に示す．

temperature は全条件で 0.0 とし，top\_p や frequency\_penalty などその他の生成パラメータはデフォルト値を使用した．
LLM 評価は gpt-4o-mini（temperature = 0.0）で実施し，COCO 実験のみ無効とした．

\begin{table}[htbp]
\centering
\caption{実験カテゴリ別パラメータ設定（変動項目のみ）}
\label{tab:experiment_config}
\small
\begin{tabular}{lllll}
  \toprule
  実験カテゴリ & max\_tokens & few\_shot & group\_size & 生成モデル \\
  \midrule
  データセット別比較 & 2000 & 0 & 100 & \texttt{4o-mini} \\
  Few-shot実験 & 100 & 0/1/3 & 100 & \texttt{4o-mini} \\
  グループサイズ比較 & 2000 & 0 & 50--300 & \texttt{4o-mini} \\
  モデル比較 & 100 & 0 & 100 & \texttt{4o-mini, 5.1} \\
  アスペクト説明文比較 & 2000 & 0 & 100 & \texttt{4o} \\
  COCO実験 & 2000 & 0 & 100 & \texttt{4o-mini} \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{比較手法とベンチマーク}
LLMによって生成された対比因子ラベルの品質を，人手アノテーションされた既存の ABSA ベンチマーク（SemEval-2014 Restaurant/Laptop，Steam レビューなど）の正解ラベルとの意味的類似性と比較することで評価した．

\section{評価指標}
\label{sec:evaluation_metrics}
生成された自然言語ラベル $L$ の品質を評価するために，SBERT類似度，BLEU，および LLM による意味的類似度評価を用いた．

\subsection{評価指標の選定理由}
\label{subsec:evaluation_metrics}
本実験では，SBERT類似度，BLEU，LLM 評価の 3 つの指標を用いた．
SBERT類似度 を選んだ理由は，生成ラベルと正解ラベルの意味的類似度を測定するためである．
本タスクでは，LLM が生成するラベルが \texttt{食べ物の品質に関する言及} のような説明的なフレーズとなるのに対し，正解ラベルは \texttt{food} のような単一の単語であるため，語彙レベルの一致度を超えたセマンティックな評価が必要である．
BLEU を選んだ理由は，語彙レベルの一致度を補助的に確認するためである．
LLM 評価を選んだ理由は，LLM による意味的類似度評価を補助指標として用いるためである．

各指標の役割と位置づけとして，SBERT類似度 は主要指標として位置づけられ，生成ラベルと正解ラベルの意味的類似度を測定した．
BLEU は参考指標として位置づけられ，語彙レベルの一致度を補助的に確認した．
LLM 評価は補助指標として位置づけられ，GPT-4o-mini による 5 段階評価を実施した．

\subsection{BERTスコア（Sentence-BERT類似度）}
BERT系モデルにより文を埋め込み表現に変換し，文間の意味的な近さを類似度として数値化する指標である．

本実験では，Sentence-BERT~\cite{reimers2019sentence} により生成ラベル $L$ と正解ラベル $L_{\mathrm{ref}}$（アスペクト説明文を用いる条件ではその説明文）を文埋め込みに変換し，コサイン類似度を計算する．実装では，SentenceTransformer（\texttt{all-MiniLM-L6-v2}）の文埋め込みを用い，コサイン類似度（$-1$〜$1$）を $(\mathrm{cos}+1)/2$ により 0.0〜1.0 に正規化した値を BERTスコアとして扱う．

本指標により，生成ラベルが正解ラベル（またはその説明文）と意味的にどの程度一致しているかを連続値で測り，語彙一致に依存しない有効性評価を行うことを狙う．

値が 1.0 に近いほど意味的に類似していることを示す．

\subsection{BLEU（Bilingual Evaluation Understudy）}
BLEU は，生成文と参照文の間の n-gram の重複にもとづき，語彙レベルの一致度を数値化する指標である~\cite{papineni-etal-2002-bleu}．

本実験では，NLTK~\cite{bird2009natural} の \texttt{sentence\_bleu} により，参照側に正解ラベル（またはその説明文），候補側に生成ラベルを与えて BLEU を算出し，SmoothingFunction.method1 を適用した．評価範囲は 0.0 から 1.0 であり，1.0 に近いほど一致度が高いことを示す．

本指標は，語彙レベルの一致度を補助的に確認するために用いた．ただし，本タスクでは正解ラベルが \texttt{food} や \texttt{price} のように 1 語の名詞句で与えられることが多い一方，生成ラベルは説明的フレーズになりやすく，n-gram が成立しにくいため BLEU が低値になりやすい．したがって，BLEU は参考値として解釈する．

\subsection{LLM評価スコア}
LLM 評価は，LLM を評価器として用い，参照テキストと候補テキストの意味的類似度を段階評価する指標である．

本実験では，参照側に正解ラベル（またはその説明文），候補側に生成ラベルを与え，GPT 系モデルに 5 段階（1--5）で評価させた．データセット別比較，Few-shot 実験，グループサイズ比較実験，アスペクト説明文比較実験では GPT-4o-mini を用い，temperature = 0.0 に設定した．モデル比較実験では GPT-4o を用い，temperature = 0.0 に設定した．COCO Retrieved Concepts 実験では LLM 評価を無効化した．

本指標により，埋め込み類似度では捉えにくい意味的一致／不一致を補助的に確認し，BERTスコアやBLEUの結果の解釈を支えることを狙う．ただし，単一モデルを評価器として用いる自動評価であるため，評価器バイアスが混入し得る点に留意し，補助指標として解釈する．

評価プロンプトの設計として，以下のプロンプトを使用した．
\begin{quote}
\ttfamily
参照テキストと候補テキストの意味的類似度を5段階（1-5）で評価してください．\par
参照テキスト: \{reference\_text\}\par
候補テキスト: \{candidate\_text\}\par
評価基準:\par
- 5: 完全に同じ意味\par
- 4: ほぼ同じ意味（細かい違いのみ）\par
- 3: 類似しているが一部異なる\par
- 2: 部分的に類似している\par
- 1: ほとんど異なる\par
出力形式（JSON形式）:\par
\{\par
    "score": 4,\par
    "normalized\_score": 0.8,\par
    "reasoning": "評価理由を簡潔に説明"\par
\}\par
\end{quote}

評価基準（5段階評価の詳細）として，1 から 5 の整数で評価し，5 が最も類似度が高く，1 が最も類似度が低い．
正規化方法として，5 段階評価（1-5）を 0.0-1.0 に正規化し，normalized\_score = (score - 1) / 4 として計算した．

SBERT類似度 との関係として，LLM 評価スコアは SBERT類似度 を補完する補助指標として位置づけられ，両指標を併用することで生成ラベルの品質を多角的に評価した．

\section{統計的分析}
\label{sec:statistical_tests}
追加実験では，条件差がアスペクトに依らず一貫して観測されるかを補足的に確認するため，統計的検定を行った．有意水準は \( \alpha=0.05 \)（両側）とした．検定対象は，生成ラベルと参照ラベル（または説明文）との意味的一致を表す主要指標である SBERT類似度（BERTスコア）とした．検定統計量の計算には SciPy を用いた~\cite{virtanen2020scipy}．

\subsection{繰り返し測定（ブロック）と多水準比較}
Few-shot（0/1/3-shot）および group\_size（50/100/150/200/300）の比較では，Steam の 4 アスペクトをブロック（繰り返し測定の単位）とみなし，条件を要因とする Friedman 検定を実施した~\cite{friedman1937use,demsar2006statistical}．補足として条件ペアごとの対応のある Wilcoxon 検定（両側）も算出し~\cite{wilcoxon1945individual}，多重比較の補正には Holm 法を用いた~\cite{holm1979simple}（主検定が非有意の場合，事後比較は参考として扱う）．

\subsection{2条件比較}
モデル比較およびアスペクト説明文あり/なしの比較では，Steam の 4 アスペクトをブロックとする対応のある Wilcoxon 検定（両側）を用いた~\cite{wilcoxon1945individual}．

\subsection{指標間の整合（補足）}
SBERT類似度と LLM 評価の序列がどの程度一致するかを補足的に示すため，全 36 条件の順位に対して Spearman の順位相関（両側）を算出した~\cite{spearman1904association}．
