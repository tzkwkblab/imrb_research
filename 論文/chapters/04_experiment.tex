\chapter{評価実験}

\section{データセット}
本章では、提案する大規模言語モデル（LLM）を用いた対比因子ラベル自動生成手法の有効性と汎用性を定量的に検証した結果を報告する \mbox{[1, 2]}。
本実験は、モデル内部の特定のニューロン発火条件に対応する集合的な意味的差分を、LLM（GPT-4o-mini）によるコントラスティブ要約によって自然言語ラベルとして抽出可能であるか、また生成されたラベルが人手アノテーションによる正解ラベルとどの程度意味的に類似するかを客観的に測定することを目的とする \mbox{[1, 3, 4]}。

提案手法のドメイン汎用性を検証するために、レビュー、感情分類、画像キャプションという多様なドメインに属する 4 種類のデータセットを用いた \mbox{[5--7]}。
これらのデータセットは、それぞれが持つ正解アスペクトラベルを、LLM が生成する対比因子ラベルの妥当性を評価するためのグラウンド・トゥルースとして使用した \mbox{[1, 2]}。

\subsection{Steam Review Aspect Dataset}
Steam Review Aspect Dataset は、Steam ゲームレビューから収集されたテキストデータであり、特定のゲームアスペクトに関する言及を含むデータセットである \mbox{[6, 8]}。

\begin{itemize}
  \item \textbf{概要}: 英語の Steam ゲームレビュー 1,100 件（学習用 900 件、テスト用 200 件）で構成される \mbox{[8]}。
  データ収集は SRec データベースのスナップショットに基づき行われた \mbox{[8]}。
  \item \textbf{アスペクト}: レビューを特徴づける 8 種類のアスペクトが人手でアノテーションされている \mbox{[6]}。
  内訳は Recommended（推奨）、Story（物語）、Gameplay（ゲームプレイ）、Visual（視覚）、Audio（聴覚）、Technical（技術）、Price（価格）、Suggestion（提案・要望）である \mbox{[6]}。
  \item \textbf{特徴}: ゲームという特定の製品ドメインに特化しており、特に「Gameplay」や「Technical」といったゲーム固有のメカニクスや技術的側面に関するアスペクトを含む \mbox{[6, 9, 10]}。
  テストセットにおけるアスペクト件数の例として、Gameplay が 154 件、Recommended が 148 件、Story が 89 件である \mbox{[11]}。
  これにより、LLM が専門性の高いテキスト集合間の意味的差分を抽出する能力を検証するためのベンチマークとして機能する。
\end{itemize}

\subsection{SemEval-2014 ABSA（Restaurants）}
SemEval-2014 ABSA（Restaurants）は、アスペクトベース感情分析（ABSA）の標準的なベンチマークとして広く使用されるレストランレビューのデータセットである \mbox{[2, 6, 12--14]}。

\begin{itemize}
  \item \textbf{概要}: レストランレビューのテキストを含み、各文に対してアスペクト（観点）とそれに対する感情極性が人手でアノテーションされている \mbox{[6, 12, 14--16]}。
  \item \textbf{アスペクト}: 本研究では、主に Food（食べ物）、Service（サービス）、Price（価格）、Atmosphere（雰囲気）の 4 種類のアスペクトを採用した \mbox{[6]}。
  \item \textbf{特徴}: このデータセットは、LLM による自動命名の性能が概念の具体性に依存するかを検証するための鍵となるデータを含む \mbox{[1, 17]}。
  「Food」や「Price」は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトである一方、「Atmosphere」は広範な文脈や比喩的表現からの高度な推論を必要とする抽象的なアスペクトに分類される \mbox{[6, 17, 18]}。
  本研究の主要な定量評価ベンチマークとして使用された \mbox{[1, 2]}。
\end{itemize}

\subsection{GoEmotions}
GoEmotions は、細粒度感情分類タスクのために Reddit コメントから収集されたデータセットである \mbox{[7, 19, 20]}。

\begin{itemize}
  \item \textbf{概要}: Demszky ら \mbox{[20--22]} によって構築されたもので、総レコード数 63,812 件から成るマルチラベル形式のデータセットである \mbox{[23]}。
  \item \textbf{アスペクト}: 28 の感情カテゴリ（27 感情 + neutral）でラベル付けされている \mbox{[7, 20, 24]}。
  主要なカテゴリには Joy（喜び）、Anger（怒り）、Admiration（称賛）、Neutral（中立）などが含まれる \mbox{[7, 24]}。
  \item \textbf{特徴}: 感情という極めて抽象的な概念を対比因子として扱えるかを検証するための挑戦的なデータセットとして位置づけられる \mbox{[7, 18, 20]}。
  このデータセットでは、感情という内在的な状態をテキストの集合差分から推論する必要があり、具体的な物理的実体を持たない概念の命名精度を測るために使用された \mbox{[18, 20]}。
  実験では、任意の感情アスペクト（例：joy）を指定し、「その感情を含むテキスト群 $A$」と「その他のアスペクトを含むテキスト群 $B$」を比較する設定が採用された \mbox{[23, 25]}。
\end{itemize}

\subsection{Retrieved Concepts（COCO Captions）}
Retrieved Concepts（COCO Captions）は、視覚的概念記述の生成能力を検証するために、画像キャプションデータセット COCO に基づいて構築されたデータセットである \mbox{[7]}。

\begin{itemize}
  \item \textbf{概要}: COCO Captions に基づく 300 の概念（concept\_0 ～ concept\_299）を扱う \mbox{[7]}。
  データには、Top-100/Bottom-100 の類似度順キャプションデータが含まれる \mbox{[7]}。
  \item \textbf{特徴}: LLM がテキストの集合差分から視覚的な特徴を抽象化した概念記述を生成できるかを確認するために使用された \mbox{[7]}。
  この検証は、本手法が将来的に画像モデルのニューロン解釈（例：縞模様、空の色といった視覚的概念）に適用可能であるかを探るための基礎的なデータを提供する。
\end{itemize}

\section{実験設定}

\subsection{モデルとパイプライン}
本実験では、コントラスティブ要約に基づく対比因子ラベル生成器として、大規模言語モデル GPT-4o-mini を採用した \mbox{[2--4, 26]}。
提案手法は、統一されたパイプラインとして構築され、その目的は、特定の概念に対応するテキスト集合 $A$（特徴あり群）と、そうでないテキスト集合 $B$（特徴なし群）の差分から、意味的な対比因子ラベル $L$ を LLM に生成させることである \mbox{[5, 26, 27]}。

\begin{itemize}
  \item \textbf{タスク定式化}: ニューロン $N$ が強く活性化するテキスト集合 $A$ と、活性化しない集合 $B$ を入力とし、集合 $A$ に特有で $B$ には見られない意味的差分を $L$ として生成する写像 $(A, B) \to L$ を定式化した \mbox{[28--30]}。
  \item \textbf{グルーピング}: 活性化値に基づき、ハイパーパラメータ $group\_size$ を用いて集合 $A$ と $B$ を抽出する \mbox{[26]}。
  メイン実験では、プロンプトのコンテキスト長制限を考慮し、$group\_size = 100$ を採用した \mbox{[26, 31]}。
  \item \textbf{Few-shot ICL の検証}: LLM の出力形式の揺らぎや語彙の安定性を確保するための検証手段として、Few-shot インコンテキスト・ラーニング（ICL）のバリエーション（0-shot, 1-shot, 3-shot）を定量的に検証した \mbox{[2, 32--34]}。
  LLM は、プロンプト内で集合 $A$ と $B$ を比較し、$A$ に特徴的で $B$ には欠如している意味的側面を推論するよう指示された \mbox{[4, 26]}。
\end{itemize}

\subsection{比較手法とベンチマーク}
本研究は、非教師ありコンセプト抽出（UCBM や CCE など）が発見した潜在ベクトルに名前を付与する「命名モジュール」としての機能に特化している \mbox{[29, 35]}。
そのため、生成されたラベルの品質を、人手アノテーションされた既存の ABSA ベンチマーク（SemEval-2014 Restaurant/Laptop、Steam レビューなど）の正解ラベルとの意味的類似性と比較することで評価した \mbox{[1, 2]}。
このアプローチにより、本手法の命名性能が、高コストな人手ラベリングによって確立された基準に対してどの程度妥当であるかを検証した \mbox{[1, 2, 36]}。

\section{評価指標}
生成された自然言語ラベル $L$ の品質を評価するために、以下の 2 つの指標を採用した。

\subsection{BERTScore}
\begin{itemize}
  \item \textbf{定義と役割}: BERTScore は、生成されたラベル $L$ と人手アノテーションされた正解ラベル $L_{\mathrm{ref}}$（またはその説明文）との間の文脈的意味的な類似性を測る主要な指標として採用された \mbox{[1, 2, 17, 37]}。
  この指標は、BERT などの事前学習済み言語モデルによって得られる文脈化埋め込み表現のコサイン類似度に基づき、語彙レベルの一致度を超えたセマンティックな評価を提供する \mbox{[17, 37, 38]}。
  \item \textbf{位置づけ}: 本タスクにおいては、LLM が集合差分という複雑な推論タスクの結果を要約した自然言語フレーズを生成するため、意味的な妥当性を定量的に示す BERTScore が最も重要な評価基準として位置づけられた \mbox{[1, 17]}。
\end{itemize}

\subsection{BLEU（Bilingual Evaluation Understudy）}
\begin{itemize}
  \item \textbf{定義と役割}: BLEU は、生成ラベルと正解ラベルとの間の語彙レベルの一致度（n-gram の重複）を確認するために補助的に使用された \mbox{[1, 17, 37]}。
  \item \textbf{本タスクにおける制約}: 本タスクの性質上、BLEU スコアは極めて低い値を示すことが前提とされた \mbox{[1, 39]}。
  これは、正解ラベルが「food」「price」のような単一の単語または簡潔なフレーズであるのに対し \mbox{[17, 39]}、
  LLM が生成するラベルはしばしば「食べ物の品質に関する言及」「価格設定の側面」といった説明的なフレーズとなるため、語彙的な重複（n-gram overlap）が本質的に生じにくいためである \mbox{[17, 39]}。
  したがって、BLEU スコアの低さはモデルの命名失敗を意味するものではなく、単に語彙的一致度を測る指標が本タスクの性質に適合していないことを示す参考値として扱われた \mbox{[1, 17, 39]}。
\end{itemize}

\section{実験結果}

\subsection{主要実験結果：自動命名の定量分析}
SemEval-2014 データセット（レストランレビュー）における Few-shot 設定ごとの平均 BERTScore および BLEU スコアは、以下の定量的な傾向を示した \mbox{[1, 17]}。

\begin{itemize}
  \item \textbf{BERTScore の達成値}: LLM によるコントラスティブ要約の結果、生成された対比因子ラベルは、正解ラベルとの間で平均約 0.551 という中適度な意味的関連性を達成した \mbox{[1, 3, 17]}。
  この値は、LLM がニューロンの発火群と非発火群というテキストの集合差分から、その集合の本質的な意味的核を抽出できる能力を有していることを示唆する \mbox{[1, 17]}。
  \item \textbf{Few-shot ICL の影響}: Few-shot ICL のバリエーション（0-shot, 1-shot, 3-shot）を比較した結果、1-shot 設定が他の設定と比較して最も高い BERTScore を示す傾向が観測された \mbox{[1, 17, 33]}。
  これは、LLM がタスクの定義と出力スタイルを学習する上で、少数の適切に選定された例（1 組）が最も効率的かつ効果的に機能することを示している \mbox{[1, 33]}。
  \item \textbf{BLEU スコアの傾向}: 一方、BLEU スコアは全ての Few-shot 設定において極めて低値（平均約 0.007）を示した \mbox{[1, 17]}。
  この低値は、生成ラベルと正解ラベルとの間で語彙的な重複がほとんど存在しないという、本タスクの性質を裏付ける結果である \mbox{[1, 17, 39]}。
\end{itemize}

\subsection{概念の具体性による性能比較}
LLM による対比因子ラベル生成の性能は、対象となる概念の具体性（Concrete vs.\ Abstract）によって明確な差異を示す傾向が観測された \mbox{[1, 17]}。

\begin{itemize}
  \item \textbf{具体的なアスペクトにおける優位性}: SemEval-2014 における「Food」や「Price」といった語彙的に安定した具体的なアスペクトの命名において、本手法は相対的に高い BERTScore を達成した \mbox{[1, 17]}。
  これらの概念は、具体的な製品や属性に関する明確な語彙的証拠（例：ピザ、高すぎる、割引）がテキスト集合 $A$ に含まれやすく、LLM が差分を容易に抽出できたことを示唆する \mbox{[18]}。
  Steam Review Aspect Dataset における「Technical」や「Gameplay」といった具体的特性も、同様に比較的高い意味的類似性を示す傾向があった \mbox{[6, 17]}。
  \item \textbf{抽象的な概念における性能劣位}: 対照的に、SemEval-2014 における「Atmosphere」や Steam Review Dataset における「Story」といった抽象的なアスペクトの命名精度は、具体的なアスペクトと比較して劣位となる傾向が確認された \mbox{[1, 17, 18]}。
  さらに、GoEmotions データセットで検証された 28 の感情カテゴリ（例：Joy, Anger, Admiration など）は物理的な実体を持たない高度に抽象的な概念であり、これらの概念に対する対比因子ラベルの生成は、具体的なアスペクトと比較して総じて低い BERTScore を記録した \mbox{[7, 18, 20]}。
  \item \textbf{傾向の対比}: これらの結果は、LLM が集合差分を推論する際、具体的な語彙や構造に強く依存する「抽出」タスクに近い性能を確保できる一方で \mbox{[18]}、
  広範な文脈や感情的なニュアンスといった抽象的な要素を要約し、簡潔なラベルとして命名するタスク（高度な「推論」を必要とする）においては、性能が相対的に低下する傾向があることを定量的に示した \mbox{[17, 18]}。
\end{itemize}

\subsection{補足分析}
Few-shot ICL の導入は、LLM の生成ラベルのスタイルを正解ラベルの簡潔なスタイルに近づけることを目的として検証された \mbox{[32, 33]}。
具体的には、Few-shot の例をプロンプトの \texttt{examples\_section} として挿入し \mbox{[33, 40]}、モデルがその出力形式を模倣する特性を利用した \mbox{[32]}。
1-shot 設定が最適であったことは、LLM がタスク定義と 1 つの高品質な例から、集合差分を命名するための有効な生成戦略を迅速に確立したことを示す。

また、SemEval-2014 や Steam レビューデータは、LLM 命名の評価ベンチマークとして使用された \mbox{[1, 2]}。
GoEmotions データセットは、総レコード数 63,812 件に及ぶ大規模な Reddit コメントから収集されており \mbox{[23]}、
28 の感情カテゴリは、本手法のロバスト性を抽象概念の集合に拡張するための重要な挑戦的要素として機能した \mbox{[18, 20]}。
実験パイプラインでは、グループ $A$ と $B$ を抽出する際、コンテキスト長超過エラーを回避するため、$group\_size$ が最大 100 件に制限された設定が用いられた \mbox{[26, 31]}。

さらに、BLEU スコア（約 0.007）の低さは、生成ラベルが「～に関する言及」といった説明文であり、正解ラベル（例：「food」）と直接的な語彙重複を持たないという、
評価指標とタスクの性質とのミスマッチによって生じている \mbox{[1, 17, 39]}。
この観測結果は、語彙的重複を測る指標が本タスクに不適であり、文脈的意味類似度を測る BERTScore の採用が妥当であるという評価戦略の選択を裏付ける定量的証拠となる \mbox{[1, 17, 39]}。
BERTScore が約 0.551 という中適度な値を示したことは、語彙レベルでは一致しないが意味レベルでは関連性が保持されているという事実を客観的に示す \mbox{[3, 17]}。

最後に、実験結果は、LLM（GPT-4o-mini）によるコントラスティブ要約が XAI におけるニューロン対比因子命名タスクの実現可能性を示し、
SemEval-2014 ベンチマークにおいて BERTScore で約 0.551 という妥当な意味的関連性を達成したことを定量的に示した \mbox{[1, 3, 17]}。
また、概念の具体性が命名精度に系統的に影響を与え、具体的なアスペクトにおいて優位性を示す傾向が確認された \mbox{[1, 17]}。
