\chapter{評価実験}

\section{実験の目的と概要}
本章では，提案手法の評価実験の目的，使用データセット，実験設定，評価指標を述べる．実験結果と考察は第5章で報告する．
本実験の目的は，外部ラベルにもとづいて定義した 2 つのテキスト集合 $A/B$ の差分から，LLM（GPT-4o-mini 等）が対比因子ラベルをどの程度生成できるか，また生成ラベルが人手アノテーションによる正解ラベル（またはその説明文）とどの程度意味的に一致するかを，定量的に評価することである．

提案手法のドメイン汎用性を検証するために，レビュー，感情分類，画像キャプションという多様なドメインに属する 4 種類のデータセットを用いた．
SemEval-2014 ABSA，GoEmotions，Steam Review Aspect Dataset では，データセットに付与されたカテゴリ（アスペクト）ラベルを，LLM が生成する対比因子ラベルの妥当性を評価するための正解（参照）として用いた．
使用したデータセットの概要を表~\ref{tab:dataset_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{使用データセットの概要}
  \label{tab:dataset_overview}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lll}
      \toprule
      データセット & ドメイン & 特徴 \\
      \midrule
      SemEval-2014 ABSA & レビュー & 具体的アスペクト（Food，Service等） \\
      GoEmotions & 感情分類 & 抽象的概念（28感情カテゴリ） \\
      Steam Review Aspect Dataset & レビュー & ドメイン固有で抽象度の高いアスペクト（Gameplay等） \\
      COCO Retrieved Concepts & 画像キャプション & 視覚的概念記述 \\
      \bottomrule
    \end{tabular}
  }
\end{table}

本実験は，以下の 6 つの実験カテゴリで構成された．
データセット別比較では，SemEval-2014 ABSA，GoEmotions，Steam Review Aspect Dataset の 3 データセットを用いて，提案手法の基本性能を評価した．
Few-shot 設定による性能比較実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．
グループサイズの影響分析実験では，group\_size を 50，100，150，200，300 の 5 段階で変化させた．
モデル比較実験では，GPT-4o-mini と GPT-5.1 の 2 モデルを比較した．
アスペクト説明文の効果検証実験では，アスペクト説明文の有無による性能差を検証した．
COCO Retrieved Concepts 実験では，正解ラベルがない画像キャプションデータセットに対する対比因子ラベル生成タスクを検証した．
各実験カテゴリの概要を表~\ref{tab:experiment_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{実験カテゴリの概要}
  \label{tab:experiment_overview}
  \begin{tabular}{ll}
    \toprule
    実験カテゴリ & 目的・検証内容 \\
    \midrule
    データセット別比較 & 基本性能評価 3データセット \\
    Few-shot実験 & 0/1/3-shot設定の比較 \\
    グループサイズ比較 & group\_size（50--300）の影響分析 \\
    モデル比較 & GPT-4o-mini vs GPT-5.1 \\
    アスペクト説明文比較 & 説明文の有無による性能差 \\
    COCO実験 & 正解ラベルなしデータセットでの検証 \\
    \bottomrule
  \end{tabular}
\end{table}

評価の観点として，以下の 3 つの指標を用いた（各指標の詳細な定義は\ref{sec:evaluation_metrics}節を参照）．
有効性の評価には，Sentence-BERT（SBERT）文埋め込みのコサイン類似度を 0.0〜1.0 に正規化した値（以降，SBERT類似度）を主要指標として使用し，生成ラベルと正解ラベルの意味的類似度を測定した．
汎用性の評価には，複数のドメイン（レビュー，感情分類，画像キャプション）と複数のアスペクトタイプ（具体的アスペクト，抽象的概念）に対する性能を測定した．
性能の評価には，BLEU スコアを参考指標として使用し，語彙レベルの一致度を補助的に確認した．
また，LLM による意味的類似度評価を補助指標として用い，GPT-4o-mini による 5 段階評価を実施した．
用いた評価指標の概要を表~\ref{tab:evaluation_metrics_overview}に示す．
\begin{table}[htbp]
  \centering
  \caption{評価指標の概要}
  \label{tab:evaluation_metrics_overview}
  \begin{tabular}{lll}
    \toprule
    評価指標 & 役割 & 位置づけ \\
    \midrule
    SBERT類似度 & 意味的類似度測定 & 主要指標 \\
    BLEU & 語彙レベル一致度確認 & 参考指標 \\
    LLM評価 & 意味的類似度評価（5段階） & 補助指標 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{データセット}
\label{sec:dataset}

本研究では，対比因子ラベル生成の汎用性を検証するため，レビュー（SemEval-2014，Steam），感情分類（GoEmotions），画像キャプション（Retrieved Concepts）という異なるテキストタイプから 4 データセットを用いた．
以降の考察でデータセット特性を定量的に参照するため，テキスト統計の概要を表~\ref{tab:text_stats_overview}に示す（語数は小文字化後の空白分割による）．
\begin{table}[htbp]
  \centering
  \caption{テキスト統計の概要（train/dev/test 合算）}
  \label{tab:text_stats_overview}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lrrrrrr}
      \toprule
      データセット & 件数 & 平均語数 & 中央語数 & 最大語数 & 平均ラベル数 & 複数ラベル率 \\
      \midrule
      GoEmotions & 54,263 & 12.83 & 12 & 33 & 1.18 & 16.25\% \\
      Steam Review Aspect & 1,100 & 244.70 & 156 & 1,473 & 3.27 & 90.27\% \\
      SemEval-2014 restaurant14 & 4,728 & 19.41 & 18 & 79 & 1.00 & 0.00\% \\
      SemEval-2014 laptop14 & 2,966 & 20.75 & 18 & 83 & 1.00 & 0.00\% \\
      \bottomrule
    \end{tabular}%
  }
\end{table}

\subsection{Steam Review Aspect Dataset}
Steam Review Aspect Dataset は，Steam ゲームレビューから収集されたテキストデータであり，特定のゲームに関する概念特徴（アスペクト）に関する言及を含むデータセットである~\cite{srec:steam-review-aspect-dataset}．
本データセットは英語の Steam ゲームレビュー 1,100 件（学習用 900 件，テスト用 200 件）で構成される．原著者は，データ収集が SRec データベースのスナップショットに基づくと説明している．本研究では，配布ページから公開データセットを取得して使用した\footnote{配布ページ：\url{https://srec.ai/blog/steam-review-aspect-dataset}（accessed: 2025-12-13）}．レビューを特徴づける 8 種類の概念特徴（アスペクトラベル）が人手でアノテーションされており，Recommended（推奨），Story（物語），Gameplay（ゲームプレイ），Visual（視覚），Audio（聴覚），Technical（技術），Price（価格），Suggestion（提案・要望）からなる．各ラベルは，レビュー中で言及される観点を表す．
\begin{itemize}
  \item \texttt{Recommended}: 他のユーザに対する推奨・非推奨や，全体としておすすめできるかに関する言及
  \item \texttt{Story}: 物語，キャラクター，プロット，世界観などストーリー要素に関する言及
  \item \texttt{Gameplay}: 操作感，難易度，ゲームシステム，戦闘や進行などプレイ体験に関する言及
  \item \texttt{Visual}: グラフィック，映像表現，アートスタイルなど視覚要素に関する言及
  \item \texttt{Audio}: BGM，効果音，音声，サウンドデザインなど聴覚要素に関する言及
  \item \texttt{Technical}: 動作の安定性，最適化，クラッシュ，バグなど技術的側面に関する言及
  \item \texttt{Price}: 価格，セール，値段に対する価値などコスト面に関する言及
  \item \texttt{Suggestion}: 改善要望，修正提案，追加してほしい要素など提案・要望に関する言及
\end{itemize}
ゲームという特定の製品ドメインに特化しており，特に \texttt{Gameplay} や \texttt{Technical} といったゲーム固有のメカニクスや技術的側面に関する概念特徴を含む．テストセットにおける概念特徴件数の例として，Gameplay が 154 件，Recommended が 148 件，Story が 89 件である．
レビュー長は平均 244.70 語（中央値 156 語，最大 1,473 語）であり，語彙タイプ数は 29,056 語である．また，1 サンプルあたりの平均ラベル数は 3.27 で，複数概念特徴を併記するレビューは 90.27\% を占める．

実験では，各概念特徴（アスペクトラベル）について，そのラベルを含むテキスト群をグループ A，含まないテキスト群をグループ B として構成した．サンプル抽出（ランダムサブサンプリング等）と group\_size の設定は第\ref{sec:preprocess_split}節で述べる．

% データセット別比較で使用したアスペクト一覧は第\ref{sec:experiment_setup}節でまとめて述べる．

\subsection{SemEval-2014 ABSA（Restaurants）}
SemEval-2014 ABSA（Restaurants）は，アスペクトベース感情分析（ABSA）の標準的なベンチマークとして広く使用されるレストランレビューのデータセットである~\cite{pontiki-EtAl:2014:SemEval2014Task4}．
レストランレビューのテキストを含み，各文に対してアスペクト（観点）とそれに対する感情極性が人手でアノテーションされている．本研究では主に Food（食べ物），Service（サービス），Price（価格），Atmosphere（雰囲気）の 4 種類のアスペクトを用いる．Food や Price は具体的な名詞や数値に関連する言及が中心となる一方，Atmosphere は広範な文脈や比喩的表現からの推論を必要とし，データセット特性と命名性能の関係を考察する際の比較対象となる．
本データセットは 1 文あたり 1 アスペクトを前提としており，restaurant14 は平均 19.41 語，laptop14 は平均 20.75 語と短文である（表~\ref{tab:text_stats_overview}）．

実験では，Restaurant ドメインから Food と Service，Laptop ドメインから Battery と Screen の 4 アスペクトを用い，各アスペクトについて，そのアスペクトを含むテキスト群をグループ A，含まないテキスト群をグループ B として構成した．サンプル抽出（ランダムサブサンプリング等）と group\_size の設定は第\ref{sec:preprocess_split}節で述べる．

使用したアスペクトの選定理由として，Food は具体的な名詞や数値に関連する言及が中心となる具体的なアスペクトとして，Service，Battery，Screen は製品の属性に関する具体的なアスペクトとして選定した．
これにより，具体的なアスペクトにおける命名性能を評価できるようにした．

\subsection{GoEmotions}
GoEmotions は，細粒度感情分類タスクのために Reddit コメントから収集されたデータセットである~\cite{demszky2020goemotions}．
Demszky らによって構築された総レコード数 63,812 件から成るマルチラベル形式のデータセットであり，28 の感情カテゴリ（27 感情 + neutral）でラベル付けされている．主要なカテゴリには Joy（喜び），Anger（怒り），Admiration（称賛），Neutral（中立）などが含まれる．感情という抽象的概念をテキスト集合差分から推論する必要があるため，具体的な物理的実体を持たない概念の命名精度を検証する目的で用いた．実験では任意の感情アスペクト（例：joy）を指定し，その感情を含むテキスト群 $A$ とその他のアスペクトを含むテキスト群 $B$ を比較する設定とした．
平均 12.83 語の短文が中心であり，複数感情ラベルを併記するサンプルは 16.25\% である（表~\ref{tab:text_stats_overview}）．

実験では，GoEmotions データセットの全 28 感情カテゴリを対象とし，各カテゴリについて，その感情を含むテキスト群をグループ A，含まないテキスト群をグループ B として構成した．サンプル抽出（ランダムサブサンプリング等）と group\_size の設定は第\ref{sec:preprocess_split}節で述べる．

28 感情カテゴリの選定理由として，感情は物理的な実体を持たない高度に抽象的な概念であり，具体的なアスペクトと比較して命名精度が低下する傾向があるかを検証するために，全カテゴリを対象とした．
これにより，抽象的な概念における命名性能を包括的に評価できるようにした．

\subsection{Retrieved Concepts（COCO Captions）}
Retrieved Concepts（COCO Captions）は，画像そのものではなく COCO 由来のキャプション集合のみを用いて，視覚的概念に対応するテキスト集合差分から対比因子ラベルを生成する挙動を補助的に検証するために，画像キャプションデータセット COCO に基づいて構築されたデータセットである~\cite{lin2014microsoft}．
MS-COCO 2017 train split の画像に対して，Unsupervised CBM（UCBM）~\cite{schrodi2024unsupervised} によって抽出された 300 個の潜在コンセプト埋め込みと，CLIP（ViT-B/32）による画像埋め込みとのコサイン類似度を計算し，各コンセプトについて類似度が高い画像 Top-100 と低い画像 Bottom-100 を取得している．
各画像には COCO 由来の人手キャプションが 5 つ付与されており，本研究ではこれらのキャプションのみを集合 $A$, $B$ の要素として利用する．
非教師ありに学習された 300 の潜在コンセプト（concept\_0 ～ concept\_299）に対し，CLIP 類似度にもとづき取得された Top-100/Bottom-100 画像とそのキャプションからなる．各コンセプトに対して人手の正解ラベル（アスペクト名）は与えられておらず，潜在コンセプトとその Top/Bottom 例のみが提供されるため，正解アスペクト名が与えられない設定において，集合差分から対比因子ラベルを生成する挙動を補助的に検証できる．

実験では，300 コンセプトのうち concept\_0，concept\_1，concept\_2，concept\_10，concept\_50 の 5 コンセプトを用いた．
各コンセプトについて，潜在コンセプト埋め込みと画像埋め込みとの CLIP（ViT-B/32）コサイン類似度に基づき，類似度が高い順に Top-100，低い順に Bottom-100 の画像を選び，それらに付与されたキャプションをグループ A（Top 側），グループ B（Bottom 側）として用いた．
サンプル抽出と group\_size の設定は第\ref{sec:preprocess_split}節で述べる．

正解ラベルが存在しないため，SBERT類似度 と BLEU スコアは参考値として記録するにとどめ，主に生成された対比因子ラベルと対応する画像群との整合性に基づく定性的評価を行った．

\section{実験設定}
\label{sec:experiment_setup}

\subsection{実験パイプラインの概要}
本実験は以下の手順で行う．

\begin{enumerate}
  \item \textbf{データセット読み込み}：各データセットからテキストを読み込む．
\item \textbf{グループA/B抽出}：各データセットで定義された規則（アスペクトラベル，あるいは Retrieved Concepts における Top/Bottom 構造）に基づき，グループA（特定概念を含むテキスト群）とグループB（含まないテキスト群）を抽出する．
  \item \textbf{プロンプト生成}：グループAとグループBのテキストリストをプロンプトに組み込む．Few-shot例が設定されている場合は，プロンプトにFew-shot例を挿入する．
  \item \textbf{LLMによる対比因子ラベル生成}：\texttt{4o-mini} 等のLLMにプロンプトを入力し，対比因子ラベルを生成する．
  \item \textbf{評価}：生成されたラベルと正解ラベルの意味的類似度をSBERT類似度，BLEU，LLM評価により測定する．
\end{enumerate}

データセット別比較で対象としたアスペクトは，SemEval-2014 から Food，Service，Battery，Screen の 4 種類，GoEmotions から全 28 感情カテゴリ，Steam から Gameplay，Visual，Story，Audio の 4 種類である．

\subsection{LLMモデルとパラメータ設定}
本実験では，対比因子ラベル生成器として \texttt{4o-mini} を主要モデルとして用いた．
モデル比較実験では，\texttt{5.1} も使用した．
\texttt{4o-mini} を選択した理由は，コスト効率が高く，かつ十分な性能を発揮することが既存研究で確認されているためである．

各実験カテゴリでのモデル選択として，データセット別比較，Few-shot 実験，グループサイズ比較実験では \texttt{4o-mini} を使用した．
アスペクト説明文比較実験では \texttt{4o} を使用した．
モデル比較実験では，\texttt{4o-mini} と \texttt{5.1} の 2 モデルを比較した．
COCO Retrieved Concepts 実験では，\texttt{4o-mini} を使用した．

温度パラメータ（temperature）は，全ての実験で 0.0 を用いた．この設定により，決定論的な出力が得られ，実験の再現性が確保される．

最大トークン数（max\_tokens）の設定として，データセット別比較では max\_tokens = 2000 に設定した．
Few-shot 実験，モデル比較実験では max\_tokens = 100 に設定した．
グループサイズ比較実験，アスペクト説明文比較実験，COCO Retrieved Concepts 実験では max\_tokens = 2000 に設定した．

その他の生成パラメータとして，top\_p や frequency\_penalty はデフォルト値を使用した．

\subsection{プロンプト設計}
プロンプトは第3章3.2節で述べたテンプレートを用い，タスク説明，オプションの Few-shot 例，グループA/Bのテキスト列挙，および出力制約から構成される．

アスペクト説明文の使用方法として，本研究の「アスペクト説明文比較」では，説明文を生成プロンプトに追加しない．
代わりに評価時の参照テキストを，従来のアスペクト名（例：\texttt{food}，\texttt{story}）から，対応するアスペクト説明文へ置換して用いる．生成ラベルが説明的フレーズになりやすい一方で，正解側が単語ラベルであるという粒度差が自動評価に与える影響を確認するため，説明文なし条件（参照：アスペクト名）と説明文あり条件（参照：アスペクト説明文）で，生成ラベルとの意味的類似度（SBERT類似度，BLEU，LLM評価）を比較した．

\subsection{データの前処理と分割方法}
\label{sec:preprocess_split}
テキストの前処理手順として，各データセットからテキストを読み込み，アスペクトラベルに基づいてグループ A とグループ B に分割した．
テキストの前処理として，特殊文字の処理や正規化は行わず，データセットの生のテキストをそのまま使用した．

グループA/Bの抽出方法として，各アスペクトについて，そのアスペクトを含むテキスト群をグループA，含まないテキスト群をグループBとして抽出した．
各アスペクトについて，そのアスペクトを含むテキスト群（グループA）と含まないテキスト群（グループB）を比較した．
COCO Retrieved Concepts 実験では，Top-100 のキャプションをグループA，Bottom-100 のキャプションをグループBとして抽出した．

グループサイズ（group\_size）の設定として，データセット別比較では group\_size = 100 を用い，グループサイズ比較実験では group\_size を 50，100，150，200，300 の 5 段階で変化させた．

サンプリングは，各グループの候補集合から group\_size 件になるようランダムにサブサンプリングし，候補数が group\_size 未満の場合は重複を許して補完した．重み付きサンプリングは使用しなかった．

コンテキスト長制限への対応として，プロンプトに投入するテキスト数は各グループで最大 group\_size 件としつつ，コンテキスト長超過を回避するため，必要に応じて group\_size 未満に制限した．

\subsection{Few-shot例の作成方法}
Few-shot例は，各データセットのアスペクトラベルにもとづき作成した．Few-shot例は，グループAとグループBのテキストリスト（抜粋）と，それに対応する正解ラベルで構成された．

選定基準として，(i) 正解ラベルに対応する手がかりがテキスト上で明示的であり，(ii) 他ラベルの手がかりが相対的に少なく，(iii) グループA/Bの差分から回答ラベルが一意に推測しやすい例を優先した．例えば，SemEval-2014のFoodでは (採用例) \texttt{The sushi was fresh and delicious.} のように food への言及が中心の文を用い，(非採用例) \texttt{Great service and tasty food.} のように複数アスペクトが同程度に含まれる文は避けた．同様に，GoEmotionsのjoyでは (採用例) \texttt{I'm so happy for you!} のような明示的表現を用い，皮肉や複数感情が混在する文は避けた．Steamでは gameplay に関して (採用例) \texttt{The gameplay loop is addictive.} のようにプレイ体験への言及が中心の文を用い，Story/Visual/Priceなどが同時に強く現れる文は避けた．
Few-shot 例の形式は \texttt{【例題N】グループA: [...] グループB: [...] 回答: [正解ラベル]} であり，$N$ は例題番号である．

0-shot，1-shot，3-shot の違いと設定方法として，0-shot 設定では Few-shot 例を挿入せず，タスク説明のみを提示した．
1-shot 設定では，1 つの Few-shot 例を \texttt{examples\_section} に挿入した．
3-shot 設定では，3 つの Few-shot 例を \texttt{examples\_section} に挿入した．
Few-shot 実験では，0-shot，1-shot，3-shot の 3 つの設定を比較した．

\subsection{実験カテゴリの定義}
各実験カテゴリのパラメータ設定を表~\ref{tab:experiment_config}に示す．

temperature は全条件で 0.0 とし，top\_p や frequency\_penalty などその他の生成パラメータはデフォルト値を使用した．
LLM 評価は gpt-4o-mini（temperature = 0.0）で実施し，COCO 実験のみ無効とした．

\begin{table}[htbp]
\centering
\caption{実験カテゴリ別パラメータ設定（変動項目のみ）}
\label{tab:experiment_config}
\small
\begin{tabular}{lllll}
  \toprule
  実験カテゴリ & max\_tokens & few\_shot & group\_size & 生成モデル \\
  \midrule
  データセット別比較 & 2000 & 0 & 100 & \texttt{4o-mini} \\
  Few-shot実験 & 100 & 0/1/3 & 100 & \texttt{4o-mini} \\
  グループサイズ比較 & 2000 & 0 & 50--300 & \texttt{4o-mini} \\
  モデル比較 & 100 & 0 & 100 & \texttt{4o-mini, 5.1} \\
  アスペクト説明文比較 & 2000 & 0 & 100 & \texttt{4o} \\
  COCO実験 & 2000 & 0 & 100 & \texttt{4o-mini} \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{比較手法とベンチマーク}
LLMによって生成された対比因子ラベルの品質を，人手アノテーションされた既存の ABSA ベンチマーク（SemEval-2014 Restaurant/Laptop，Steam レビューなど）の正解ラベルとの意味的類似性と比較することで評価した．

\section{評価指標}
\label{sec:evaluation_metrics}
生成された自然言語ラベル $L$ の品質を評価するために，SBERT類似度，BLEU，および LLM による意味的類似度評価を用いた．
SBERT類似度 は主要指標として位置づけられ，生成ラベルと正解ラベルの意味的類似度を測定した．
本タスクでは，LLM が生成するラベルが \texttt{食べ物の品質に関する言及} のような説明的なフレーズとなるのに対し，正解ラベルは \texttt{food} のような単一の単語であるため，語彙レベルの一致度を超えたセマンティックな評価が必要である．
BLEU は参考指標として位置づけられ，語彙レベルの一致度を補助的に確認した．
LLM 評価は補助指標として位置づけられ，GPT-4o-mini による 5 段階評価を実施した．

\subsection{BERTスコア（Sentence-BERT類似度）}
BERT系モデルにより文を埋め込み表現に変換し，文間の意味的な近さを類似度として数値化する指標である．

本実験では，Sentence-BERT~\cite{reimers2019sentence} により生成ラベル $L$ と正解ラベル $L_{\mathrm{ref}}$（アスペクト説明文を用いる条件ではその説明文）を文埋め込みに変換し，コサイン類似度を計算する．実装では，SentenceTransformer（\texttt{all-MiniLM-L6-v2}）の文埋め込みを用い，コサイン類似度 $\mathrm{cos} \in [-1, 1]$ を $\frac{\mathrm{cos}+1}{2} \in [0, 1]$ に正規化した値を BERTスコアとして扱う．
ここで $L_{\mathrm{ref}}$ は，説明文なし条件では正解アスペクト名，説明文あり条件ではそのアスペクト説明文である．

本指標により，生成ラベルが正解ラベル（またはその説明文）と意味的にどの程度一致しているかを連続値で測り，語彙一致に依存しない有効性評価を行うことを狙う．

値が 1.0 に近いほど意味的に類似していることを示す．

\subsection{BLEU（Bilingual Evaluation Understudy）}
BLEU は，生成文と参照文の間の n-gram の重複にもとづき，語彙レベルの一致度を数値化する指標である~\cite{papineni-etal-2002-bleu}．

本実験では，NLTK~\cite{bird2009natural} の \texttt{sentence\_bleu} により，参照側に正解ラベル（またはその説明文），候補側に生成ラベルを与えて BLEU を算出し，SmoothingFunction.method1 を適用した．評価範囲は 0.0 から 1.0 であり，1.0 に近いほど一致度が高いことを示す．

本指標は，語彙レベルの一致度を補助的に確認するために用いた．ただし，本タスクでは正解ラベルが \texttt{food} や \texttt{price} のように 1 語の名詞句で与えられることが多い一方，生成ラベルは説明的フレーズになりやすく，n-gram が成立しにくいため BLEU が低値になりやすい．したがって，BLEU は参考値として解釈する．

\subsection{LLM評価スコア}
\label{subsec:llm_eval_score}
LLM 評価は，LLM を評価器として用い，参照テキストと候補テキストの意味的類似度を段階評価する指標である．

本実験では，参照側に参照テキスト（説明文なし条件では正解アスペクト名，説明文あり条件ではアスペクト説明文），GPT 系モデルに 5 段階（1--5）で評価させた．データセット別比較，Few-shot 実験，グループサイズ比較実験，アスペクト説明文比較実験では GPT-4o-mini を用い，temperature = 0.0 に設定した．モデル比較実験では GPT-4o を用い，temperature = 0.0 に設定した．COCO Retrieved Concepts 実験では LLM 評価を無効化した．

本指標により，埋め込み類似度では捉えにくい意味的一致／不一致を補助的に確認し，BERTスコアやBLEUの結果の解釈を支えることを狙う．ただし，単一モデルを評価器として用いる自動評価であるため，評価器バイアスが混入し得る点に留意し，補助指標として解釈する．また，LLM 評価ではスコア（score）に加えて判断理由（reasoning）も JSON 形式で同時に出力させる．reasoning は，スコアが低くなった要因を定性的に確認し，第5章の考察で代表例を参照するために用いる．ただし，単一の LLM を評価器として用いる自動評価であり，LLM 評価自体と同様に補助的な情報として解釈する．

評価プロンプトの設計として，以下のプロンプトを使用した．
\begin{quote}
\ttfamily
参照テキストと候補テキストの意味的類似度を5段階（1-5）で評価してください．\par
参照テキスト: \{reference\_text\}\par
候補テキスト: \{candidate\_text\}\par
評価基準:\par
- 5: 完全に同じ意味\par
- 4: ほぼ同じ意味（細かい違いのみ）\par
- 3: 類似しているが一部異なる\par
- 2: 部分的に類似している\par
- 1: ほとんど異なる\par
出力形式（JSON形式）:\par
\{\par
    "score": 4,\par
    "normalized\_score": 0.8,\par
    "reasoning": "評価理由を簡潔に説明"\par
\}\par
\end{quote}

評価基準（5段階評価の詳細）として，1 から 5 の整数で評価し，5 が最も類似度が高く，1 が最も類似度が低い．
5 段階評価（1-5）を normalized\_score = $\frac{score - 1}{4} \in [0, 1]$ に正規化して評価に用いる．

SBERT類似度 との関係として，LLM 評価スコアは SBERT類似度 を補完する補助指標として位置づけられ，両指標を併用することで生成ラベルの品質を多角的に評価した．

\section{統計的分析}
\label{sec:statistical_tests}
追加実験では，条件差がアスペクトに依らず一貫して観測されるかを補足的に確認するため，統計的検定を行った．有意水準は \( \alpha=0.05 \)（両側）とした．検定対象は，生成ラベルと参照ラベル（または説明文）との意味的一致を表す主要指標である SBERT類似度（BERTスコア）とした．

\subsection{繰り返し測定（ブロック）と多水準比較}
Few-shot（0/1/3-shot）および group\_size（50/100/150/200/300）の比較では，Steam データセットの 4 アスペクトをブロック（繰り返し測定の単位）とみなし，条件を要因とする Friedman 検定を実施した~\cite{friedman1937use,demsar2006statistical}．補足として条件ペアごとの対応のある Wilcoxon 検定（両側）も算出し~\cite{wilcoxon1945individual}，多重比較の補正には Holm 法を用いた~\cite{holm1979simple}（主検定が非有意の場合，事後比較は参考として扱う）．

\subsection{2条件比較}
モデル比較およびアスペクト説明文あり/なしの比較では，Steam データセットの 4 アスペクトをブロックとする対応のある Wilcoxon 検定（両側）を用いた~\cite{wilcoxon1945individual}．
