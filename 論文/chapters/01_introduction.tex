\chapter{序章}

近年の深層学習（DNN）モデルは，医療，金融，自動運転といった社会的重要性の高いドメインにおいて優れた予測性能を達成している．しかし，その意思決定プロセスが人間にとって解釈不能な「ブラックボックス」であるという根本的な問題は，モデルの信頼性や公平性の担保，そして法的・倫理的な規制要件の遵守を妨げる最大の障壁となっている．この課題に対応するため，モデルの判断根拠を事後的に説明する説明可能 AI（XAI）の研究が活発化してきた．

従来，XAI の中心的な手法として広く普及したのは，LIME（Local Interpretable Model-agnostic Explanations）~\cite{ribeiro2016should}や SHAP（SHapley Additive exPlanations）~\cite{lundberg2017unified}に代表される，\textbf{事後説明（Post-hoc Explanation）}手法である．これらの手法は，入力データ（例：画像ピクセル，テキストトークン）の摂動やゲーム理論に基づく貢献度（Shapley 値）の計算を通じて，個別インスタンスの予測に対する特徴量の寄与を可視化する．

しかし，事後説明手法には，以下のような根本的な信頼性の問題が指摘されている．第一に，これらの説明は本質的な曖昧さ（high degree of ambiguity）を抱えており，特に共線性を持つ特徴量に対して不安定な結果を示すことがあり，同等の性能を持つ異なるモデルが全く異なる説明を生成する不安定性が問題となる．第二に，Bordt ら~\cite{bordt2022posthoc}は，事後説明アルゴリズムが，規制や倫理が求める「透明性」の目的を達成する上で\textbf{「不適切 (unsuitable)」}であると結論付けている．これは，説明提供者と受け手の利害が対立する「敵対的文脈 (adversarial contexts)」において，事後説明が容易に操作可能であり，モデル開発者が都合の良い説明を選択的に提示できてしまうためである．

さらに重要な点として，LIME や SHAP は，あくまで個別インスタンスの局所的な特徴寄与に焦点を当てており，モデルが特定のデータ集合（例：特定のニューロンが発火するすべてのサンプル）に対して系統的に何を学習しているか，というグループレベルの差異や集合間のコントラストを自然言語で説明する能力を欠いている．

事後説明の限界を克服するため，XAI の研究コミュニティは，低レベルの特徴量（ピクセル，トークン ID）から，人間が理解可能な\textbf{「コンセプト（概念）」}レベル（例：「縞模様」や「価格に関する言及」）で説明を提供するパラダイムへと移行している~\cite{kim2018interpretability}．コンセプトボトルネックモデル（CBM）~\cite{koh2020concept}や TCAV（Concept Activation Vectors）といったコンセプトベース XAI（C-XAI）は，モデルの内部状態を高レベルの概念に関連付けることで解釈可能性を向上させた．

しかし，このアプローチは新たな，かつ重大なボトルネックを生み出した．それは，どの概念を監視・学習すべきかという概念の定義（ラベリング）が，依然として人間に依存している点である．このプロセスは Kim らによって\textbf{「高コストな概念キュレーション (expensive concept curation)」}と呼ばれ，特に専門知識が不可欠なドメイン（例：医療）において，C-XAI の導入を阻む最大の障壁となっていた~\cite{kim2018interpretability}．既存のベンチマークである SemEval-2014 においても，アスペクト（概念）ラベルは人手によるアノテーションに依存している~\cite{pontiki-EtAl:2014:SemEval2014Task4}．

モデルの内部動作を回路図レベルで徹底的に理解しようとする最先端のメカニスティック解釈（MI）分野においても，解釈の「意味付け」は手作業に依存する．Anthropic による Attribution Graphs~\cite{anthropic2025biology}のような研究は，モデルの計算プロセスをトレースし，特徴量間の相互作用をグラフとして可視化することで，ニューロン間の回路を発見する．

しかし，Attribution Graphs などが発見する個々の「特徴量」やノードが，人間にとって何を意味するのかを自然言語で特定するプロセスは自動化されていない．研究者らは，解釈を容易にするために，関連する意味を持つ特徴量を手動でグループ化し\textbf{「スーパーノード」としてまとめているが，この手動ステップは「労働集約的 (labor-intensive)」}であり，情報の欠損を伴うことが指摘されている~\cite{ameisen2025attribution}．

本研究の動機は，これら従来の XAI アプローチが直面する，「個別インスタンス中心の説明の限界」，「高コストな人手ラベリング依存」，そして\textbf{「発見された内部構造への命名の自動化の欠如」という 3 つの課題の交差点に存在する}点にある．信頼性が高く，低コストで，スケーラブルな解釈可能性を実現するためには，モデルの内部状態に基づき，かつ集合間の差分を自然言語で記述する自動命名手法が不可欠となる．

本研究が取り組む主要な課題は，非教師ありコンセプト抽出~\cite{schrodi2024unsupervised,stein2024towards}などで発見されたモデル内部の解釈可能な構造，すなわちニューロン発火条件に対応すると想定される 2 つのテキスト集合の対比因子ラベルを，集合差分からスケーラブルに自動生成することである．理想的には，発火群 $A$ と非発火群 $B$ をニューロン活性値に基づき構成するが，本論文の実験では，第3章・第4章で述べるように，アスペクトラベルや CLIP 類似度に基づいて構成したグループA/Bを，このタスクの近似として用いる．このタスクは，従来の XAI 研究における「概念の発見」と「命名」を統合する新たな試みとして位置づけられる．

従来の非教師ありコンセプト発見手法（UCBM や CCE など）は，人間による事前定義なしにモデル内部から概念（潜在ベクトル）を抽出する点で大きな進歩を遂げた．しかし，これらの手法が発見するのは，あくまでも潜在的なベクトル表現であり，そのベクトルに人間が理解できる「自然言語の名前」を自動で付与する機能は欠如している．命名プロセスは，発火サンプルを見た研究者による手動分析に依存していた~\cite{schrodi2024unsupervised,stein2024towards}．

本研究は，この\textbf{「発見された概念の手動での意味付け」という C-XAI の主要なボトルネックの解消に貢献する}ことを目指す．具体的には，モデル内部の特定のニューロン（またはコンセプト）が強く発火したテキスト集合 A と，発火しなかったテキスト集合 B を入力とし，集合 A に特有で集合 B に見られない意味的差分を，自然言語ラベル $L$ として生成するタスクを定式化する．以降，本論文では，この理想的なタスク定義を，実験ではアスペクトラベルや類似度スコアに基づくグループA/Bで近似する（詳細は第3章を参照）．このタスクは，NLP における「コントラスティブ要約 (Contrastive Summarization)」~\cite{kardale2023contrastive}の系譜に属するが，これを XAI ドメイン（モデル内部状態の解釈）に適用する点で新規性を有する~\cite{saha2024strumllm}．

このタスクは，単一インスタンスに対する反事実的説明（例：CEM~\cite{wachter2017counterfactual}）とは異なり，集合間の一般的・代表的な差分を記述するものであり，ニューロンが何を計算しているかを集合レベルで説明することを可能にする．

本研究は，この高難度な「集合差分からの自然言語命名」タスクの解決策として，大規模言語モデル（LLM）の強力な文脈理解能力と生成能力を活用する．LLM（特に GPT-4o-mini）をコントラスト生成器として利用し，入力された A 群と B 群のテキスト群から，A 群に特徴的で B 群に欠如している意味的な側面を推論させ，簡潔なラベルを自動生成させる．

このアプローチは，人手によるアノテーションや複雑な事後分析のステップを排除し，非教師ありで抽出された概念（UCBM や Attribution Graphs が発見した特徴量）がもつ意味を，スケーラブルかつ自動で人間が理解できる「名前」として付与する命名モジュールとして機能する．この仮説の妥当性を，定量的な実験を通じて検証することが，本論文の主要な目標となる．

本研究は，LLM（GPT-4o-mini）を用いたコントラスティブ要約を核とする．手法の実現にあたっては，以下のステップを踏む．

\begin{enumerate}
  \item データ収集とグルーピング：モデル内部の特定のニューロンの発火度に基づき，発火が強いテキスト集合 $A$（発火群）と，発火が弱い（または非発火の）テキスト集合 $B$（非発火群）を抽出する（グループサイズ \texttt{group\_size} はパラメータとして調整可能）．
  \item Few-shot ICL（In-Context Learning）によるプロンプト設計：LLM に集合 $A$ と $B$ のテキストを入力として提示し，「集合 $A$ に特有の内容的・意味的差分」を抽出するよう指示する．Few-shot ICL は，本タスクにおける LLM の出力形式の揺らぎや語彙の安定性を確保する検証手段として導入し，さらに性能向上にも寄与することが確認された~\cite{saha2024strumllm}．
  \item 対比因子ラベルの生成：LLM に，集合差分を簡潔に要約した自然言語の「対比因子ラベル」を出力させる．
\end{enumerate}

本研究では，人手アノテーションされたアスペクトラベルを持つ SemEval-2014 Restaurant/Laptop~\cite{pontiki-EtAl:2014:SemEval2014Task4}，GoEmotions~\cite{demszky2020goemotions}，Steam Review Aspect Dataset~\cite{srec:steam-review-aspect-dataset}の 3 データセットを評価ベンチマークとして使用し，さらに COCO Retrieved Concepts データセット~\cite{lin2014microsoft}でも検証を行い，LLM が生成したラベルと正解ラベルとの意味的類似性を評価した．これらのデータセットに対して，アスペクトラベルや CLIP 類似度に基づきグループA/Bを構成し，ニューロン発火条件を模擬した集合差分タスクとして具体化している．

評価指標には，文脈的意味的類似度を測る BERTScore~\cite{zhang2019bertscore}（本論文では Sentence-BERT 埋め込みのコサイン類似度として実装）を主要指標として，語彙的一致度を測る BLEU~\cite{papineni-etal-2002-bleu}を参考指標として，さらに LLM による 5 段階評価を補助指標として採用した．その結果，以下の重要な知見が得られた（詳細は第4章を参照）．

\begin{itemize}
  \item 意味的関連性の達成：生成された対比因子ラベルの品質を BERTScore で測定したところ，全 36 実験における平均は 0.6980 を記録し，特に SemEval-2014 では 0.7531，GoEmotions では 0.7127 と中程度以上の意味的関連性を達成した．この結果は，LLM がニューロンの発火群と非発火群という集合的な差分から，人間が理解できる意味的な核を抽出できること，すなわち「LLM の文脈理解能力を活用すれば，概念の発見と命名を同時に行うプロセスを実現できる」という主要な仮説が部分的に正しいことを示す．
  \item 語彙的一致の限界：一方，BLEU スコアは全体平均 0.0082 と低値を示した．これは，正解ラベルが「food」のような短いアスペクト名であるのに対し，LLM がより説明的なフレーズを生成するタスク設定に起因するものであり，BLEU は本タスクでは参考指標にとどまる（詳細は第4章の評価指標節を参照）．
  \item 概念の具体性による影響：生成ラベルの品質は，具体的なアスペクト（SemEval-2014）において特に高い優位性を示す一方，抽象的な概念ではデータセット特性により性能が大きく変動することが確認された．
\end{itemize}

本研究は，XAI と NLP の境界領域において，以下の点で重要な貢献を果たす．

第一に，新規タスクの提案と実現可能性の検証である．XAI における「概念の発見」（UCBM, CCE）と「概念の命名」が分離していた構造的な課題を認識し，LLM によるコントラスティブ要約によって，この二つのプロセスを統合する\textbf{「集合差分によるニューロン対比因子命名」}という新規タスクを提案し，その実現可能性を定量的に検証した．

第二に，XAI パラダイムのギャップ解消である．個別インスタンス中心の事後説明（LIME/SHAP）の限界と，命名機能を持たない非教師ありコンセプト抽出（UCBM/CCE）のボトルネックを同時に解消する，スケーラブルなハイブリッドアプローチを提供する．

第三に，メカニスティック解釈の実用化への寄与である．Attribution Graphs などによって発見されたニューロン回路や特徴量に対し，人間が理解可能な意味的な名前を自動で付与する手段を提供し，手動ラベリングに依存していた MI 分野のボトルネック解消に貢献する~\cite{ameisen2025attribution}．

第四に，Few-shot ICL の応用と最適化に関する知見の提供である．Few-shot ICL を活用し，生成ラベルの品質とスタイルを矯正するアプローチを提案し，Few-shot 数の変動による影響を分析することで，LLM を用いた自動命名の安定性向上に向けた知見を提供する~\cite{saha2024strumllm}．

