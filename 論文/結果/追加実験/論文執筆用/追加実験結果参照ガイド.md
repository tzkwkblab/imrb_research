# 追加実験結果参照ガイド - 論文執筆用

このドキュメントは、論文執筆時に追加実験の全体像を把握し、必要な情報を迅速に参照するための包括的なガイドです。

**最終更新**: 2025-11-27

---

## 目次

1. [追加実験の全体概要](#追加実験の全体概要)
2. [ディレクトリ構造](#ディレクトリ構造)
3. [実験 1: モデル比較実験](#実験1-モデル比較実験)
4. [実験 2: Few-shot 実験（LLM 評価無効）](#実験2-few-shot実験llm評価無効)
5. [実験 3: Few-shot 実験（LLM 評価有効）](#実験3-few-shot実験llm評価有効)
6. [実験 4: Group Size 比較実験](#実験4-group-size比較実験)
7. [実験 5: アスペクト説明文比較実験](#実験5-アスペクト説明文比較実験)
8. [実験 6: COCO Retrieved Concepts 実験（画像付き考察）](#実験6-coco-retrieved-concepts実験画像付き考察)
9. [実験間の関係性と統合的考察](#実験間の関係性と統合的考察)
10. [論文執筆時の参照方法](#論文執筆時の参照方法)
11. [主要な発見と論文への反映](#主要な発見と論文への反映)

---

## 追加実験の全体概要

### 目的

本研究の追加実験は、LLM による対比因子ラベル自動生成の実現可能性を検証し、最適な設定を探索することを目的としています。

### 実施した追加実験

1. **モデル比較実験**: gpt-4o-mini と gpt-5.1 の性能比較
2. **Few-shot 実験（LLM 評価無効）**: Few-shot 設定（0-shot, 1-shot, 3-shot）の性能比較
3. **Few-shot 実験（LLM 評価有効）**: Few-shot 設定（0-shot, 1-shot, 3-shot）の性能比較（LLM 評価指標を含む）
4. **Group Size 比較実験**: グループサイズ（50/100/150/200/300）が対比因子生成性能に与える影響の調査
5. **データセット別性能比較実験**: Steam、SemEval、GoEmotions の 3 つのデータセット間での性能比較（0-shot 設定）
6. **アスペクト説明文比較実験**: アスペクト説明文の有無が対比因子抽出性能に与える影響の検証（説明文あり/なしの対比）
7. **COCO Retrieved Concepts 実験（画像付き考察）**: COCO データセットを使用した対比因子生成実験。画像との整合性を確認するための画像付き LLM 考察を生成

### 共通設定

- **データセット**: Steam レビューデータセット
- **アスペクト**: gameplay, visual, story, audio（4 アスペクト）
- **group_size**: 100（各グループ 100 件のレビュー）
- **評価指標**: BERT スコア（意味類似度）、BLEU スコア（n-gram 一致率）、LLM スコア（LLM 評価有効実験のみ）
- **分割タイプ**: aspect_vs_others（特定アスペクトを含むレビュー vs 含まないレビュー）

---

## ディレクトリ構造

```
論文/結果/追加実験/論文執筆用/
├── 追加実験結果参照ガイド.md    ← このファイル
├── model_comparison/              # モデル比較実験
│   ├── 実験結果/
│   │   ├── batch_results.json     # 統合実験結果（8実験分）
│   │   └── individual/            # 個別実験結果（8ファイル）
│   ├── 分析レポート/
│   │   ├── model_comparison_analysis.md    # LLM考察（gpt-5.1）
│   │   └── 実験結果レポート.md              # 統計サマリー
│   └── 実験設定/
│       ├── 実験パラメータ.md
│       └── steam_model_comparison_matrix.json
├── example_fewshot/           # Few-shot実験（LLM評価無効）
│   └── steam/
│       ├── 実験結果/
│       │   ├── batch_results.json          # 統合実験結果（12実験分）
│       │   └── individual/                  # 個別実験結果（12ファイル）
│       ├── 分析レポート/
│       │   ├── fewshot_analysis.md           # LLM考察（gpt-5.1）
│       │   └── fewshot_results_report.md    # 統計サマリー
│       └── 実験設定/
│           ├── fewshot_statistics.json
│           └── steam_fewshot_matrix.json
└── fewshot_llm_eval/          # Few-shot実験（LLM評価有効）
    └── steam/
        ├── 実験結果/
        │   ├── batch_results.json          # 統合実験結果（12実験分）
        │   └── individual/                  # 個別実験結果（12ファイル）
        ├── 分析レポート/
        │   ├── fewshot_llm_eval_analysis.md           # LLM考察（gpt-5.1）
        │   └── fewshot_llm_eval_results_report.md    # 統計サマリー
        └── 実験設定/
            └── fewshot_llm_eval_statistics.json
└── group_size_comparison/       # Group Size比較実験
    └── steam/
        ├── 実験結果/
        │   └── batch_results.json          # 統合実験結果（20実験分）
        ├── 分析レポート/
        │   ├── group_size_comparison_analysis.md           # LLM考察（gpt-5.1）
        │   └── group_size_comparison_results_report.md    # 統計サマリー
        ├── 実験設定/
        │   └── group_size_comparison_statistics.json
        ├── 実行ファイル/
        │   └── generate_group_size_analysis.py
        └── research_context.md
└── dataset_comparison/         # データセット別性能比較実験
    ├── 実験結果/
    │   ├── batch_results.json          # 統合実験結果（36実験分）
    │   └── individual/                  # 個別実験結果（36ファイル）
    ├── 分析レポート/
    │   ├── dataset_comparison_analysis.md           # LLM考察（gpt-5.1）
    │   └── dataset_comparison_results_report.md    # 統計サマリー
    ├── 実験設定/
    │   ├── dataset_comparison_statistics.json
    │   ├── dataset_comparison_matrix.json
    │   └── 実験パラメータ.md
    └── research_context.md
└── aspect_description_comparison/  # アスペクト説明文比較実験
    └── steam/
        ├── 実験結果/
        │   ├── batch_results.json          # 統合実験結果（8実験分）
        │   └── individual/                  # 個別実験結果（8ファイル）
        ├── 分析レポート/
        │   └── aspect_description_comparison_analysis.md    # LLM考察（gpt-5.1）
        ├── 実験設定/
        │   ├── aspect_description_comparison_statistics.json
        │   ├── aspect_description_comparison_matrix.json
        │   └── 実験パラメータ.md
        ├── 実行ファイル/
        │   ├── generate_statistics.py
        │   └── generate_analysis.py
        └── research_context.md
└── coco_retrieved_concepts/      # COCO Retrieved Concepts実験（画像付き考察）
    ├── 実行ファイル/
    │   └── generate_coco_analysis_with_images.py  # 画像付き分析スクリプト
    ├── 分析レポート/
    │   └── coco_analysis_with_images.md  # 画像付きLLM考察（GPT-5.1）
    └── research_context.md
```

---

## 実験 1: モデル比較実験

### 実験の目的

gpt-4o-mini と gpt-5.1 の 2 つのモデル間で、対比因子抽出の性能を比較し、モデル選択の指針を得る。

### 実験設定

| パラメータ         | 値                                             |
| ------------------ | ---------------------------------------------- |
| データセット       | Steam                                          |
| アスペクト         | gameplay, visual, story, audio（4 アスペクト） |
| GPT モデル         | gpt-4o-mini, gpt-5.1（2 モデル）               |
| group_size         | 100                                            |
| few_shot           | 0（固定）                                      |
| use_llm_evaluation | False                                          |
| 総実験数           | 8 実験（4 アスペクト × 2 モデル）              |
| 実行日時           | 2025-11-26                                     |

### 主要結果

#### モデル別平均 BERT スコア

| モデル      | 平均 BERT スコア | 最小 BERT スコア | 最大 BERT スコア |
| ----------- | ---------------- | ---------------- | ---------------- |
| gpt-4o-mini | 0.5402           | 0.5071           | 0.5788           |
| gpt-5.1     | 0.5404           | 0.5254           | 0.5537           |

**発見**: 平均性能はほぼ同等（差 0.0002、gpt-5.1 がわずかに優位）

#### アスペクト別比較

| アスペクト | gpt-4o-mini | gpt-5.1 | 差（4o-mini - 5.1） | 優位モデル  |
| ---------- | ----------- | ------- | ------------------- | ----------- |
| gameplay   | 0.5788      | 0.5537  | +0.0251             | gpt-4o-mini |
| visual     | 0.5071      | 0.5394  | -0.0322             | **gpt-5.1** |
| story      | 0.5352      | 0.5429  | -0.0078             | gpt-5.1     |
| audio      | 0.5398      | 0.5254  | +0.0144             | gpt-4o-mini |

**発見**: アスペクトによって優劣が異なる

- **gameplay, audio**: gpt-4o-mini が優位
- **visual, story**: gpt-5.1 が優位

#### BLEU スコア

全実験で BLEU スコアは 0.0000（0-shot 設定のため、表層的な一致がほとんどない）

### 主要な発見（LLM 考察より）

1. **モデル間の性能差の意味**

   - 平均性能がほぼ同等であることから、0-shot 設定ではモデル世代による決定的優位は確認できない
   - 対比因子説明という抽象的タスクでは、プロンプト設計や few-shot の有無の方が影響要因になりうる

2. **アスペクト特性とモデル性能**

   - gameplay や visual のような具体的・客観的な要素を含むアスペクトでは、モデルごとの「対比の切り取り方・抽象度」の違いが影響
   - story や audio のような抽象度が高く、感情・物語体験と強く結びつくアスペクトでは、gpt-5.1 がわずかに優位（抽象的・情緒的なパターン認識に長けている可能性）

3. **実用的なモデル選択の示唆**
   - 特定のアスペクトに応じてモデルを選択する戦略が有効
   - 「レビュー文体や感情の対比」を重視する用途では gpt-5.1、「プレイ内容やシステム面の差」を重視する用途では gpt-4o-mini を優先

### 論文執筆時の参照ファイル

- **数値データ**: `model_comparison/実験結果/batch_results.json`
- **詳細考察**: `model_comparison/分析レポート/model_comparison_analysis.md`
- **統計サマリー**: `model_comparison/分析レポート/実験結果レポート.md`
- **実験設定**: `model_comparison/実験設定/実験パラメータ.md`

---

## 実験 2: Few-shot 実験（LLM 評価無効）

### 実験の目的

Few-shot 設定（0-shot, 1-shot, 3-shot）が対比因子抽出の性能に与える影響を検証し、最適な Few-shot 設定を探索する。

### 実験設定

| パラメータ         | 値                                                                               |
| ------------------ | -------------------------------------------------------------------------------- |
| データセット       | Steam                                                                            |
| アスペクト         | gameplay, visual, story, audio（4 アスペクト）                                   |
| Few-shot 設定      | 0-shot, 1-shot, 3-shot                                                           |
| GPT モデル         | gpt-4o-mini                                                                      |
| group_size         | 100                                                                              |
| use_llm_evaluation | False                                                                            |
| 分割タイプ         | aspect_vs_others                                                                 |
| 例題ファイル       | `data/analysis-workspace/contrast_examples/steam/steam_examples_v1.json`（3 例） |
| 総実験数           | 12 実験（4 アスペクト × 3Few-shot 設定）                                         |
| 実行日時           | 2025-11-26                                                                       |

### 主要結果

#### Few-shot 別平均 BERT スコア

| Few-shot 設定 | 平均 BERT スコア | 最小 BERT スコア | 最大 BERT スコア | 改善幅（0-shot 基準） |
| ------------- | ---------------- | ---------------- | ---------------- | --------------------- |
| 0-shot        | 0.5676           | 0.5200           | 0.6260           | -                     |
| 1-shot        | 0.6438           | 0.5840           | 0.7175           | +0.0762（+13.4%）     |
| 3-shot        | 0.6899           | 0.5647           | 0.7925           | +0.1223（+21.5%）     |

**発見**: Few-shot 設定が性能向上に寄与（特に 0→1-shot で大きな改善）

#### アスペクト別最高 BERT スコア（3-shot）

| アスペクト | 3-shot BERT スコア | 0-shot BERT スコア | 改善幅            |
| ---------- | ------------------ | ------------------ | ----------------- |
| gameplay   | **0.7925**         | 0.5804             | +0.2121（+36.5%） |
| story      | **0.7676**         | 0.5200             | +0.2476（+47.6%） |
| visual     | 0.6347             | 0.6260             | +0.0087（+1.4%）  |
| audio      | 0.5647             | 0.5441             | +0.0206（+3.8%）  |

**発見**: gameplay と story で 3-shot の効果が特に大きい

#### Few-shot 影響分析表（アスペクト別）

| アスペクト | 0-shot | 1-shot | 3-shot     | 最大改善幅        |
| ---------- | ------ | ------ | ---------- | ----------------- |
| audio      | 0.5441 | 0.5840 | 0.5647     | +0.0399（1-shot） |
| gameplay   | 0.5804 | 0.6287 | **0.7925** | +0.2121（3-shot） |
| story      | 0.5200 | 0.7175 | **0.7676** | +0.2476（3-shot） |
| visual     | 0.6260 | 0.6449 | 0.6347     | +0.0189（1-shot） |

**発見**:

- gameplay と story では 3-shot が最も効果的
- audio と visual では 1-shot が最適

#### BLEU スコア

- 0-shot, 1-shot: 全実験で 0.0000
- 3-shot: 一部の実験で 0.0000、gameplay で 0.0408（表層一致は低いが、意味的類似度は高い）

### 主要な発見（LLM 考察より）

1. **Few-shot 設定による性能変化の傾向**

   - 0-shot から 1-shot への移行で大きな性能向上（+13.4%）
   - 3-shot で最高の平均 BERT スコア（0.6899）を達成
   - Few-shot 設定が対比因子抽出の性能に明確に寄与

2. **アスペクトによる性能差の要因**

   - 具体的なアスペクト（gameplay, visual）と抽象的なアスペクト（story, audio）で性能差が異なる
   - gameplay と story で 3-shot の効果が特に大きい（最高 0.7925, 0.7676）

3. **本研究における対比因子抽出への示唆**

   - 平均 BERT スコア 0.69 は、LLM による対比因子ラベル自動生成の実現可能性を示す
   - Few-shot 学習の効果と限界を定量化

4. **今後の研究への示唆**
   - アスペクト固有の few-shot 例を用いたプロンプト最適化
   - モデルアンサンブルやメタモデルによるアスペクト別モデル選択
   - 人手評価との相関分析（BERT スコアがどの程度「良い対比因子説明」と一致するか）

### 論文執筆時の参照ファイル

- **数値データ**: `example_fewshot/steam/実験結果/batch_results.json`
- **個別結果**: `example_fewshot/steam/実験結果/individual/`（12 ファイル）
- **詳細考察**: `example_fewshot/steam/分析レポート/fewshot_analysis.md`
- **統計サマリー**: `example_fewshot/steam/分析レポート/fewshot_results_report.md`
- **統計情報**: `example_fewshot/steam/実験設定/fewshot_statistics.json`

---

## 実験 3: Few-shot 実験（LLM 評価有効）

### 実験の目的

Few-shot 設定（0-shot, 1-shot, 3-shot）が対比因子抽出の性能に与える影響を検証し、LLM 評価指標を含む多角的な評価を行う。

### 実験設定

| パラメータ         | 値                                                                               |
| ------------------ | -------------------------------------------------------------------------------- |
| データセット       | Steam                                                                            |
| アスペクト         | gameplay, visual, story, audio（4 アスペクト）                                   |
| Few-shot 設定      | 0-shot, 1-shot, 3-shot                                                           |
| GPT モデル         | gpt-4o-mini                                                                      |
| group_size         | 100                                                                              |
| use_llm_evaluation | **True**（gpt-4o-mini による意味的類似度評価）                                   |
| LLM 評価モデル     | gpt-4o-mini                                                                      |
| LLM 評価温度       | 0.0                                                                              |
| 分割タイプ         | aspect_vs_others                                                                 |
| 例題ファイル       | `data/analysis-workspace/contrast_examples/steam/steam_examples_v1.json`（3 例） |
| 総実験数           | 12 実験（4 アスペクト × 3Few-shot 設定）                                         |
| 実行日時           | 2025-11-27                                                                       |

### 主要結果

#### Few-shot 別平均スコア

| Few-shot 設定 | 平均 BERT スコア | 平均 LLM スコア | BERT 改善幅（0-shot 基準） | LLM 改善幅（0-shot 基準） |
| ------------- | ---------------- | --------------- | -------------------------- | ------------------------- |
| 0-shot        | 0.5526           | 0.3000          | -                          | -                         |
| 1-shot        | 0.6530           | 0.3500          | +0.1004（+18.2%）          | +0.0500（+16.7%）         |
| 3-shot        | 0.5754           | 0.4000          | +0.0228（+4.1%）           | +0.1000（+33.3%）         |

**発見**:

- BERT スコアは 1-shot で最高（0.6530）、LLM スコアは 3-shot で最高（0.4000）
- 評価指標間で最良設定が一致しない（BERT と LLM 評価のトレードオフ）

#### アスペクト別統計

| アスペクト | 平均 BERT スコア | 最高 BERT スコア | 平均 LLM スコア | 最高 LLM スコア |
| ---------- | ---------------- | ---------------- | --------------- | --------------- |
| gameplay   | 0.5969           | 0.6802           | 0.4000          | 0.6000          |
| story      | 0.6419           | **0.8356**       | **0.6000**      | **0.8000**      |
| visual     | 0.5708           | 0.6449           | 0.2000          | 0.2000          |
| audio      | 0.5650           | 0.5850           | 0.2000          | 0.2000          |

**発見**:

- story で両指標が高い（BERT=0.8356, LLM=0.6000）
- visual と audio で LLM スコアが低い（0.2000）が、BERT スコアは中程度
- LLM 評価は「人間が読んだときの説明妥当性」に近い評価をしている可能性

### 主要な発見（LLM 考察より）

1. **Few-shot 設定による性能変化の傾向（BERT スコアと LLM スコアの両方）**

   - BERT スコアは 0-shot=0.5526 → 1-shot=0.6530 で大きく向上し、3-shot=0.5754 で低下
   - LLM スコアは 0-shot=0.30 → 1-shot=0.35 → 3-shot=0.40 と単調増加
   - 1-shot は「正解アスペクト名に近い語彙を出す」方向に強く誘導され、BERT が高まった
   - 3-shot ではより抽象的・説明的な対比因子を出力し、LLM スコアが最高

2. **LLM 評価指標の妥当性**

   - BERT との「完全な一致」は見られないが、相補的な特徴がある
   - story は両指標が高い（平均 BERT=0.6419・最高 BERT=0.8356、LLM=0.60）
   - visual と audio は平均 BERT=0.57 程度に対して LLM=0.20 と低い
   - BERT は「表層概念レベルの一致」、LLM スコアは「人間が読んだときの説明妥当性」に近い評価

3. **アスペクトによる性能差の要因**

   - gameplay と story は両指標とも比較的高い（多様な記述が現れ、集合 A/B の差分が明確）
   - visual・audio は表現がパターン化しやすく、対比因子としての情報密度が低い
   - アスペクトごとに「潜在概念の多様性」「レビュー言語のバリエーション」が異なる

4. **本研究における対比因子抽出への示唆**
   - 全体平均で BERT が 0.55〜0.65 程度、LLM スコアも 0.30〜0.40（story では 0.60）を達成
   - プロンプト設計を工夫することで、「人間が読んで納得する対比ラベル」を生成できる余地が大きい
   - 単一指標では性能を過小・過大評価しうるため、多角的な評価が必要
   - LLM 評価は自動指標として有用であり、特に BLEU のような語彙一致偏重の指標よりも、タスク目的に整合的なシグナルを与えている

### 論文執筆時の参照ファイル

- **数値データ**: `fewshot_llm_eval/steam/実験結果/batch_results.json`
- **個別結果**: `fewshot_llm_eval/steam/実験結果/individual/`（12 ファイル）
- **詳細考察**: `fewshot_llm_eval/steam/分析レポート/fewshot_llm_eval_analysis.md`
- **統計サマリー**: `fewshot_llm_eval/steam/分析レポート/fewshot_llm_eval_results_report.md`
- **統計情報**: `fewshot_llm_eval/steam/実験設定/fewshot_llm_eval_statistics.json`

---

## 実験 4: Group Size 比較実験

### 実験の目的

Steam データセットでのグループサイズ（50/100/150/200/300）が対比因子生成性能に与える影響を調査し、最適なグループサイズを探索する。

### 実験設定

| パラメータ         | 値                                             |
| ------------------ | ---------------------------------------------- |
| データセット       | Steam                                          |
| アスペクト         | gameplay, visual, story, audio（4 アスペクト） |
| Group Size 設定    | 50, 100, 150, 200, 300（5 段階）               |
| Few-shot 設定      | 0-shot（固定）                                 |
| GPT モデル         | gpt-4o-mini                                    |
| Temperature        | 0.0                                            |
| Max tokens         | 2000                                           |
| use_llm_evaluation | True（gpt-4o-mini による意味的類似度評価）     |
| LLM 評価モデル     | gpt-4o-mini                                    |
| LLM 評価温度       | 0.0                                            |
| 分割タイプ         | aspect_vs_others                               |
| 総実験数           | 20 実験（4 アスペクト × 5Group Size 設定）     |
| 実行日時           | 2025-11-27                                     |

### 主要結果

#### Group Size 別平均スコア

| Group Size | BERT 平均 | BERT 最小 | BERT 最大 | LLM 平均 | LLM 最小 | LLM 最大 |
| ---------- | --------- | --------- | --------- | -------- | -------- | -------- |
| 50         | 0.5455    | 0.5019    | 0.5636    | 0.3000   | 0.2000   | 0.4000   |
| 100        | 0.5436    | 0.5200    | 0.5600    | 0.3500   | 0.2000   | 0.6000   |
| 150        | 0.5321    | 0.5019    | 0.5600    | 0.2500   | 0.2000   | 0.4000   |
| 200        | 0.5396    | 0.5200    | 0.5600    | 0.3000   | 0.2000   | 0.4000   |
| 300        | 0.5375    | 0.5200    | 0.5600    | 0.2000   | 0.2000   | 0.2000   |

**発見**: Group Size による性能変化は限定的（変動幅約 0.02）。Group Size 300 でわずかに高い BERT スコア（0.5375）を示すが、決定的な差ではない。

#### アスペクト別統計

| アスペクト | BERT 平均 | BERT 最小 | BERT 最大 | LLM 平均 | LLM 最小 | LLM 最大 |
| ---------- | --------- | --------- | --------- | -------- | -------- | -------- |
| audio      | 0.5500    | 0.5200    | 0.5600    | 0.2000   | 0.2000   | 0.2000   |
| gameplay   | 0.5525    | 0.5200    | 0.5636    | 0.3200   | 0.2000   | 0.4000   |
| story      | 0.5285    | 0.5019    | 0.5600    | 0.4000   | 0.2000   | 0.6000   |
| visual     | 0.5276    | 0.5019    | 0.5600    | 0.2000   | 0.2000   | 0.2000   |

**発見**:

- gameplay と audio で BERT スコアがやや高い（0.55 程度）
- story で LLM スコアが最高（0.4000）
- visual と audio で LLM スコアが低い（0.2000）

#### BLEU スコア

全実験で BLEU スコアは 0.0000（表層的な語彙一致はほとんどない）

### 主要な発見（LLM 考察より）

1. **Group Size による性能変化の傾向**

   - BERT スコアは Group Size 50〜300 の間で 0.54 前後に収まり、変動幅は最大でも約 0.02 程度と小さい
   - Group Size 300 で最も高い BERT スコア（0.5603）を示すが、決定的な差ではない
   - LLM スコアは全 Group Size で平均 0.2000 と一定
   - **グループサイズによる極端な性能劣化は見られず、対比因子抽出の性能は概ねロバスト**

2. **アスペクトによる性能差の要因**

   - gameplay / story：ユーザーレビューにおける言語が比較的「説明的・抽象的」であり、LLM が抽出しやすい概念レベル差分を多く含む
   - visual / audio：実際にニューロンが捉えている差分とアスペクト名（ground truth ラベル）のミスマッチが大きい
   - audio の対比因子出力はほぼすべて「ストーリー」「感情的体験」「メカニクス」中心であり、音響的要素がほとんど言及されていない

3. **評価指標間の関係性**

   - BLEU スコアが全ケースで 0.0000 であることから、生成された対比因子ラベルは表層語彙レベルで正解アスペクト名とほぼ一致していない
   - BERT スコアは 0.50〜0.56 程度を保っており、語彙は違っても意味的にはある程度近接している
   - LLM スコアは Group Size の変化に対して感度が低い（平均値レベルでは変化せず）

4. **研究への貢献**
   - LLM による対比因子ラベル生成は、Group Size 50〜300 の範囲で大きく破綻せずに動作し、意味類似度（BERT ≒0.54）は安定している
   - グループサイズ増加による性能向上は漸増的であり、300 がわずかに有利だが、コストとのトレードオフを考えれば 100〜200 程度が実用的なバランス
   - LLM は「ストーリー・感情・メカニクス」といった抽象・内容面の差分を生成しやすく、audio / visual のような感覚的アスペクトでは、潜在概念と表層アスペクト名のギャップが評価を下げる

### 論文執筆時の参照ファイル

- **数値データ**: `group_size_comparison/steam/実験結果/batch_results.json`
- **詳細考察**: `group_size_comparison/steam/分析レポート/group_size_comparison_analysis.md`
- **統計サマリー**: `group_size_comparison/steam/分析レポート/group_size_comparison_results_report.md`
- **統計情報**: `group_size_comparison/steam/実験設定/group_size_comparison_statistics.json`

---

## 実験 5: アスペクト説明文比較実験

### 実験の目的

アスペクト説明文の有無が対比因子抽出の性能に与える影響を検証し、説明文の効果を定量的に評価する。

### 実験設定

| パラメータ         | 値                                             |
| ------------------ | ---------------------------------------------- |
| データセット       | Steam                                          |
| アスペクト         | gameplay, visual, story, audio（4 アスペクト） |
| GPT モデル         | gpt-4o                                         |
| group_size         | 100                                            |
| temperature        | 0.0                                            |
| Few-shot 設定      | 0-shot（固定）                                 |
| アスペクト説明文   | あり/なし（対比検証）                          |
| use_llm_evaluation | True（gpt-4o-mini による意味的類似度評価）     |
| LLM 評価モデル     | gpt-4o-mini                                    |
| LLM 評価温度       | 0.0                                            |
| 分割タイプ         | aspect_vs_others                               |
| 総実験数           | 8 実験（4 アスペクト × 説明文あり/なし）       |
| 実行日時           | 2025-11-27                                     |

### 主要結果

#### 説明文あり/なし別平均スコア

| 設定       | 平均 BERT スコア | 平均 LLM スコア | BERT 改善幅（なし基準） | LLM 改善幅（なし基準） |
| ---------- | ---------------- | --------------- | ----------------------- | ---------------------- |
| 説明文なし | 0.5395           | 0.2500          | -                       | -                      |
| 説明文あり | 0.5496           | 0.3000          | +0.0101（+1.9%）        | +0.0500（+20.0%）      |

**発見**: 説明文ありで BERT スコアと LLM スコアが向上（特に LLM スコアの改善幅が大きい）

#### アスペクト別対比（説明文あり - なし）

| アスペクト | 説明文なし | 説明文あり | 差分    | 効果 |
| ---------- | ---------- | ---------- | ------- | ---- |
| gameplay   | 0.5335     | 0.5523     | +0.0188 | 改善 |
| visual     | 0.5311     | 0.5186     | -0.0125 | 悪化 |
| story      | 0.5392     | 0.5467     | +0.0075 | 改善 |
| audio      | 0.5544     | 0.5810     | +0.0266 | 改善 |

**発見**: audio と gameplay で説明文の効果が大きく、visual では悪化

#### BLEU スコア

全実験で BLEU スコアは 0.0000（表層的な語彙一致はほとんどない）

### 主要な発見（LLM 考察より）

1. **アスペクト説明文の効果**

   - 全体平均で説明文ありが BERT スコア +0.0101、LLM スコア +0.05 と改善
   - LLM スコアの改善幅（+20%）が BERT スコア（+1.9%）より大きい
   - 説明文は「タスクに対する解釈的一貫性」を補正する効果がある

2. **アスペクトによる説明文効果の違い**

   - audio（+0.0266）と gameplay（+0.0188）で顕著な改善
   - story は小幅改善（+0.0075）
   - visual は悪化（-0.0125）—説明文が汎用的特徴へ注意をシフトさせた可能性

3. **評価指標間の関係性**

   - BLEU スコアは全条件で 0.0（本タスクには不適切）
   - BERT スコアと LLM スコアはおおむね整合的に動く
   - LLM スコアは「意味類似」よりも「アスペクト妥当性／対比性の明瞭さ」を評価

4. **対比因子抽出への示唆**
   - 0-shot でも BERT≈0.54 程度の性能を達成
   - アスペクト定義を明示することは有効だが、説明文の質が重要
   - 具体的感覚・機能に結びつくアスペクト（audio、gameplay）で特に有効

### 論文執筆時の参照ファイル

- **数値データ**: `aspect_description_comparison/steam/実験結果/batch_results.json`
- **個別結果**: `aspect_description_comparison/steam/実験結果/individual/`（8 ファイル）
- **詳細考察**: `aspect_description_comparison/steam/分析レポート/aspect_description_comparison_analysis.md`
- **統計情報**: `aspect_description_comparison/steam/実験設定/aspect_description_comparison_statistics.json`
- **実験設定**: `aspect_description_comparison/steam/実験設定/実験パラメータ.md`

---

## 実験 6: COCO Retrieved Concepts 実験（画像付き考察）

### 実験の目的

COCO データセット（Retrieved Concepts）を使用した対比因子生成実験。正解ラベルがないデータセットに対する対比因子生成の考察を目的とし、生成された対比因子と画像の視覚的特徴の整合性を確認する。

### 実験設定

| パラメータ         | 値                                                                      |
| ------------------ | ----------------------------------------------------------------------- |
| データセット       | Retrieved Concepts (COCO Captions)                                      |
| コンセプト         | concept_0, concept_1, concept_2, concept_10, concept_50（5 コンセプト） |
| GPT モデル         | gpt-4o-mini（対比因子生成）、gpt-5.1（画像付き考察生成）                |
| group_size         | 100                                                                     |
| Few-shot 設定      | 0-shot（固定）                                                          |
| temperature        | 0.0                                                                     |
| max_tokens         | 2000                                                                    |
| use_llm_evaluation | False                                                                   |
| 分割タイプ         | aspect_vs_bottom100                                                     |
| 総実験数           | 5 実験（5 コンセプト）                                                  |
| 実行日時           | 2025-11-27                                                              |

### データセットの特徴

- **データソース**: MS-COCO 2017 train split
- **類似度計算**: CLIP (ViT-B/32) コサイン類似度
- **コンセプト数**: 300 個の潜在コンセプト（concept_0 ～ concept_299）
- **取得数**: 各コンセプトあたり Top-100 と Bottom-100 の 2 セット
- **正解ラベル**: なし（評価スコアは参考値）

### 主要結果

#### コンセプト別結果

| コンセプト | 生成された対比因子                                                               | BERT スコア | BLEU スコア |
| ---------- | -------------------------------------------------------------------------------- | ----------- | ----------- |
| concept_0  | Group A features everyday scenes and objects, while Group B focuses on events... | 0.6460      | 0.0000      |
| concept_1  | Group A focuses on sports and outdoor activities.                                | 0.5714      | 0.0000      |
| concept_2  | Group A focuses on sports and outdoor activities.                                | 0.5714      | 0.0000      |
| concept_10 | Group A features animals and nature scenes prominently.                          | 0.5976      | 0.0000      |
| concept_50 | Group A focuses on electronics and mobile devices.                               | 0.6177      | 0.0000      |

**統計**:

- **平均 BERT スコア**: 0.6173
- **平均 BLEU スコア**: 0.0000

#### 画像との整合性確認

各コンセプトについて、GPT-5.1 を使用して画像付きの詳細な考察を生成しました。

**考察の観点**:

1. 生成された対比因子と画像の整合性
2. 画像から読み取れる特徴と対比因子の関係
3. 対比因子の妥当性
4. 本研究への示唆

### 主要な発見（画像付き LLM 考察より）

1. **画像との整合性確認の重要性**

   - concept_0 では、生成された対比因子「日常的な場面・物 vs フォーマルな場面・人々」が画像の視覚的特徴と高い整合性を示した
   - concept_1 では、対比因子「sports and outdoor activities」が画像の本質的特徴（時計・時間）を捉えていないことが判明
   - テキストベースの BERT スコアだけでは捉えきれない視覚的・社会的コンテクストの違いを、画像との整合性確認により検出可能

2. **正解ラベルなしデータでの評価方法**

   - 正解ラベルがないデータセットでも、画像との整合性確認により対比因子の妥当性を評価可能
   - Top-100 と Bottom-100 の画像を比較することで、「説明文が本当にグループの違いを表しているか」を判定できる
   - 視覚的検証は、将来の画像モデルニューロン解釈への応用において重要な評価手法となる

3. **LLM の限界とバイアス**

   - テキスト集合からの差分抽出では、キャプション中に多く出現した語に引きずられ、本質的な概念を取り逃がす可能性がある
   - 視覚的検証を通じて、頻度バイアスや共起バイアスを検出・是正できる

4. **本研究への貢献**
   - 画像との整合性確認が対比因子生成の評価に有効であることを実証
   - 正解ラベルがないデータセットでの評価方法の有効性を示した
   - 視覚的概念記述の生成能力を検証するための基礎的なデータを提供

### 論文執筆時の参照ファイル

- **数値データ**: `results/20251127_140836/batch_results.json`
- **個別結果**: `results/20251127_140836/individual/`（5 ファイル）
- **画像付き LLM 考察**: `coco_retrieved_concepts/分析レポート/coco_analysis_with_images.md`
- **研究コンテキスト**: `coco_retrieved_concepts/research_context.md`
- **実行スクリプト**: `coco_retrieved_concepts/実行ファイル/generate_coco_analysis_with_images.py`

### 注意事項

- 正解ラベルがないため、BERT/BLEU スコアは参考値として扱う
- 画像との整合性確認が主要な評価方法
- 各コンセプトの画像 URL は個別結果 JSON ファイルの`input.group_a_top5_image_urls`と`input.group_b_top5_image_urls`に含まれている

---

## 実験間の関係性と統合的考察

### 実験間の関係性

1. **モデル比較実験**は、**Few-shot 実験**の 0-shot 設定でのモデル間比較を実施
2. **Few-shot 実験（LLM 評価無効）**と**Few-shot 実験（LLM 評価有効）**は同じ設定で、評価指標のみが異なる
3. **Few-shot 実験**と**データセット別性能比較実験**は同じ 0-shot 設定で、データセットのみが異なる
4. **Group Size 比較実験**は、Few-shot 実験の 0-shot 設定と同じ条件で、グループサイズのみを変化させた実験
5. 全ての実験で同じデータセット（Steam）とアスペクト（4 アスペクト）を使用（データセット別性能比較実験を除く）
6. モデル比較実験の結果は、Few-shot 実験の 0-shot 設定と整合性がある
7. LLM 評価有効実験により、BERT スコアと LLM スコアの関係性が明らかになった
8. Group Size 比較実験により、グループサイズが性能に与える影響が限定的であることが明らかになった
9. データセット別性能比較実験により、データセット特性が性能に与える影響が明らかになった
10. COCO Retrieved Concepts 実験により、画像との整合性確認が対比因子生成の評価に有効であることが実証された

### 統合的な発見

1. **モデル選択の重要性**

   - 平均性能ではモデル間の差は小さいが、アスペクトによって最適モデルが異なる
   - 実用的には、アスペクト特性に応じたモデル選択が有効

2. **Few-shot 効果の確認**

   - Few-shot 設定が性能向上に明確に寄与（LLM 評価無効実験: 0.5676 → 0.6899）
   - 特に gameplay と story で 3-shot の効果が大きい
   - LLM 評価有効実験では、BERT スコアは 1-shot で最高、LLM スコアは 3-shot で最高

3. **評価指標間のトレードオフ**

   - BERT スコアと LLM スコアで最良設定が一致しない
   - BERT は「表層概念レベルの一致」、LLM スコアは「人間が読んだときの説明妥当性」に近い評価
   - 単一指標では性能を過小・過大評価しうるため、多角的な評価が必要

4. **対比因子抽出の実現可能性**

   - 0-shot でも平均 BERT スコア 0.55 程度の性能を達成
   - 3-shot では平均 BERT スコア 0.69 を達成し、実用的なレベルに到達
   - LLM 評価有効実験では、story で両指標が高い（BERT=0.8356, LLM=0.6000）

5. **Group Size の影響**

   - Group Size 50〜300 の範囲で性能は概ねロバスト（変動幅約 0.02）
   - Group Size 300 でわずかに高い BERT スコアを示すが、決定的な差ではない
   - 実用的には 100〜200 程度がコストと性能のバランスが良い

6. **評価指標の妥当性**

   - BLEU スコアはほぼ 0（表層一致は低い）
   - BERT スコアによる意味的類似度評価が主要指標として適切
   - LLM 評価は自動指標として有用であり、特に BLEU のような語彙一致偏重の指標よりも、タスク目的に整合的なシグナルを与えている

7. **画像との整合性確認の重要性**
   - COCO Retrieved Concepts 実験により、画像との整合性確認が対比因子生成の評価に有効であることが実証された
   - テキストベースの BERT スコアだけでは捉えきれない視覚的・社会的コンテクストの違いを、画像との整合性確認により検出可能
   - 正解ラベルがないデータセットでも、画像との整合性確認により対比因子の妥当性を評価可能
   - 視覚的検証は、将来の画像モデルニューロン解釈への応用において重要な評価手法となる

### 論文への反映ポイント

1. **実験結果の提示順序**

   - まず Few-shot 実験で性能向上の可能性を示す
   - 次にモデル比較実験でモデル選択の重要性を示す

2. **数値の引用**

   - Few-shot 別平均 BERT スコア（LLM 評価無効: 0.5676, 0.6438, 0.6899）
   - Few-shot 別平均スコア（LLM 評価有効: BERT=0.5526/0.6530/0.5754, LLM=0.3000/0.3500/0.4000）
   - モデル別平均 BERT スコア（gpt-4o-mini: 0.5402, gpt-5.1: 0.5404）
   - アスペクト別最高 BERT スコア（gameplay: 0.7925, story: 0.7676）
   - story での両指標の高さ（BERT=0.8356, LLM=0.6000）

3. **考察の引用**
   - LLM 考察（`*_analysis.md`）から主要な発見を引用
   - 統計サマリー（`*_results_report.md`）から数値データを引用

---

## 論文執筆時の参照方法

### 1. 実験結果の数値データを引用する場合

#### モデル比較実験

```markdown
参照ファイル: `model_comparison/実験結果/batch_results.json`

主要な数値:

- モデル別平均 BERT スコア: gpt-4o-mini=0.5402, gpt-5.1=0.5404
- アスペクト別 BERT スコア: 表を参照
```

#### Few-shot 実験（LLM 評価無効）

```markdown
参照ファイル: `example_fewshot/steam/実験結果/batch_results.json`

主要な数値:

- Few-shot 別平均 BERT スコア: 0-shot=0.5676, 1-shot=0.6438, 3-shot=0.6899
- アスペクト別最高 BERT スコア（3-shot）: gameplay=0.7925, story=0.7676
```

#### Few-shot 実験（LLM 評価有効）

```markdown
参照ファイル: `fewshot_llm_eval/steam/実験結果/batch_results.json`

主要な数値:

- Few-shot 別平均スコア: 0-shot(BERT=0.5526, LLM=0.3000), 1-shot(BERT=0.6530, LLM=0.3500), 3-shot(BERT=0.5754, LLM=0.4000)
- アスペクト別最高スコア: story(BERT=0.8356, LLM=0.6000)
```

#### Group Size 比較実験

```markdown
参照ファイル: `group_size_comparison/steam/実験結果/batch_results.json`

主要な数値:

- Group Size 別平均 BERT スコア: 50=0.5455, 100=0.5436, 150=0.5321, 200=0.5396, 300=0.5375
- 全体平均: BERT=0.5396, LLM=0.2800
- アスペクト別平均: gameplay(BERT=0.5525), story(LLM=0.4000)
```

#### COCO Retrieved Concepts 実験（画像付き考察）

```markdown
参照ファイル: `results/20251127_140836/batch_results.json`

主要な数値:

- コンセプト別 BERT スコア: concept_0=0.6460, concept_1=0.5714, concept_2=0.5714, concept_10=0.5976, concept_50=0.6177
- 平均 BERT スコア: 0.6173
- 平均 BLEU スコア: 0.0000
- 画像付き LLM 考察: `coco_retrieved_concepts/分析レポート/coco_analysis_with_images.md`
```

### 2. 考察を引用する場合

#### モデル比較実験の考察

```markdown
参照ファイル: `model_comparison/分析レポート/model_comparison_analysis.md`

主要な考察ポイント:

- モデル間の性能差の意味（平均性能はほぼ同等）
- アスペクト特性とモデル性能（アスペクトによって優劣が異なる）
- 実用的なモデル選択の示唆
```

#### Few-shot 実験（LLM 評価無効）の考察

```markdown
参照ファイル: `example_fewshot/steam/分析レポート/fewshot_analysis.md`

主要な考察ポイント:

- Few-shot 設定による性能変化の傾向
- アスペクトによる性能差の要因
- 対比因子抽出への示唆
- 今後の研究への示唆
```

#### Few-shot 実験（LLM 評価有効）の考察

```markdown
参照ファイル: `fewshot_llm_eval/steam/分析レポート/fewshot_llm_eval_analysis.md`

主要な考察ポイント:

- Few-shot 設定による性能変化の傾向（BERT スコアと LLM スコアの両方）
- LLM 評価指標の妥当性
- アスペクトによる性能差の要因
- 対比因子抽出への示唆（多角的な評価の重要性）
```

#### Group Size 比較実験の考察

```markdown
参照ファイル: `group_size_comparison/steam/分析レポート/group_size_comparison_analysis.md`

主要な考察ポイント:

- Group Size による性能変化の傾向（ロバスト性の確認）
- アスペクトによる性能差の要因（抽象的なアスペクトと具体的なアスペクト）
- 評価指標間の関係性（BLEU、BERT、LLM スコア）
- 研究への貢献（実用的なグループサイズ選択の指針）
```

#### COCO Retrieved Concepts 実験（画像付き考察）の考察

```markdown
参照ファイル: `coco_retrieved_concepts/分析レポート/coco_analysis_with_images.md`

主要な考察ポイント:

- 画像との整合性確認の重要性（視覚的特徴と対比因子の整合性）
- 正解ラベルなしデータでの評価方法（画像との整合性確認の有効性）
- LLM の限界とバイアス（頻度バイアスや共起バイアスの検出）
- 本研究への貢献（視覚的概念記述の生成能力の検証）
```

### 3. 統計サマリーを参照する場合

```markdown
参照ファイル:

- `model_comparison/分析レポート/実験結果レポート.md`
- `example_fewshot/steam/分析レポート/fewshot_results_report.md`
- `fewshot_llm_eval/steam/分析レポート/fewshot_llm_eval_results_report.md`
- `group_size_comparison/steam/分析レポート/group_size_comparison_results_report.md`

内容:

- Few-shot 別統計表
- Group Size 別統計表
- アスペクト別比較表
- 詳細結果表（LLM 評価有効実験では LLM スコアも含む）
```

### 4. 実験設定を確認する場合

```markdown
参照ファイル:

- `model_comparison/実験設定/実験パラメータ.md`
- `example_fewshot/steam/実験設定/steam_fewshot_matrix.json`
- `fewshot_llm_eval/steam/実験設定/fewshot_llm_eval_statistics.json`
- `group_size_comparison/steam/実験設定/group_size_comparison_statistics.json`

内容:

- 実験パラメータの詳細
- 実験マトリックス（実験計画）
- 統計情報（LLM 評価有効実験では LLM スコアも含む）
- Group Size 別統計情報
```

---

## 主要な発見と論文への反映

### 1. Few-shot 効果の確認

**発見**: Few-shot 設定が性能向上に明確に寄与

- 0-shot: 0.5676 → 1-shot: 0.6438（+13.4%）→ 3-shot: 0.6899（+21.5%）

**論文への反映**:

- 「Few-shot 設定が対比因子抽出の性能向上に寄与することを確認した」
- 「特に 0-shot から 1-shot への移行で大きな性能向上（+13.4%）が観察された」
- 「3-shot で最高の平均 BERT スコア（0.6899）を達成した」

### 2. アスペクト依存性の確認

**発見**: アスペクトによって最適な設定が異なる

- gameplay, story: 3-shot が最適（最高 0.7925, 0.7676）
- visual, audio: 1-shot が最適

**論文への反映**:

- 「アスペクトによって最適な Few-shot 設定が異なることを確認した」
- 「gameplay と story では 3-shot の効果が特に大きい（最高 0.7925, 0.7676）」

### 3. モデル選択の重要性

**発見**: 平均性能では差が小さいが、アスペクトによって最適モデルが異なる

- gameplay, audio: gpt-4o-mini が優位
- visual, story: gpt-5.1 が優位

**論文への反映**:

- 「モデル間の平均性能差は小さいが、アスペクトによって最適モデルが異なる」
- 「実用的には、アスペクト特性に応じたモデル選択が有効である」

### 4. 対比因子抽出の実現可能性

**発見**: LLM による対比因子ラベル自動生成の実現可能性を示す

- 0-shot でも平均 BERT スコア 0.54 程度
- 3-shot では平均 BERT スコア 0.69 を達成

**論文への反映**:

- 「LLM による対比因子ラベル自動生成の実現可能性を示した」
- 「3-shot 設定で平均 BERT スコア 0.69 を達成し、実用的なレベルに到達した」

### 5. Group Size の影響

**発見**: Group Size による性能変化は限定的で、対比因子抽出の性能は概ねロバスト

- Group Size 50〜300 の範囲で性能は概ねロバスト（変動幅約 0.02）
- Group Size 300 でわずかに高い BERT スコアを示すが、決定的な差ではない
- 実用的には 100〜200 程度がコストと性能のバランスが良い

**論文への反映**:

- 「Group Size 50〜300 の範囲で対比因子抽出の性能は概ねロバストであり、グループサイズによる極端な性能劣化は見られない」
- 「グループサイズ増加による性能向上は漸増的であり、実用的には 100〜200 程度がコストと性能のバランスが良い」

### 6. 評価指標の妥当性

**発見**: BERT スコアが主要指標として適切、LLM 評価は補助指標として有用

- BLEU スコアはほぼ 0（表層一致は低い）
- BERT スコアは意味的類似度を適切に評価
- LLM 評価は「人間が読んだときの説明妥当性」に近い評価をしている
- BERT スコアと LLM スコアで最良設定が一致しない（評価指標間のトレードオフ）

**論文への反映**:

- 「BLEU スコアは表層一致を評価するため、本タスクには不向きである」
- 「BERT スコアによる意味的類似度評価が主要指標として適切である」
- 「LLM 評価は自動指標として有用であり、特に BLEU のような語彙一致偏重の指標よりも、タスク目的に整合的なシグナルを与えている」
- 「単一指標では性能を過小・過大評価しうるため、多角的な評価が必要である」

### 7. 画像との整合性確認の重要性

**発見**: 画像との整合性確認が対比因子生成の評価に有効

- COCO Retrieved Concepts 実験により、画像との整合性確認が対比因子生成の評価に有効であることが実証された
- テキストベースの BERT スコアだけでは捉えきれない視覚的・社会的コンテクストの違いを、画像との整合性確認により検出可能
- 正解ラベルがないデータセットでも、画像との整合性確認により対比因子の妥当性を評価可能
- 視覚的検証は、将来の画像モデルニューロン解釈への応用において重要な評価手法となる

**論文への反映**:

- 「画像との整合性確認により、テキストベースの評価指標だけでは捉えきれない視覚的・社会的コンテクストの違いを検出可能であることを実証した」
- 「正解ラベルがないデータセットでも、画像との整合性確認により対比因子の妥当性を評価可能であることを示した」
- 「視覚的検証は、将来の画像モデルニューロン解釈への応用において重要な評価手法となる」

---

## 補足情報

### ファイル構造の説明

- **実験結果/**: 実験結果 JSON ファイル（統合結果と個別結果）
- **分析レポート/**: LLM 考察と統計サマリー
- **実験設定/**: 実験パラメータとマトリックス

### 新しい追加実験を追加する場合

1. `論文執筆用/` ディレクトリに新しい実験カテゴリのディレクトリを作成
2. 同じ構造（実験結果/、分析レポート/、実験設定/）で整理
3. このガイドに新しい実験の情報を追加

### 注意事項

- このディレクトリは論文執筆用の整理版です
- 実行ファイルや詳細なログは元のディレクトリ（`model_comparison/`, `fewshot/`）にあります
- 数値データは `batch_results.json` から直接参照できます
- LLM 考察は `*_analysis.md` から引用できます

---

**このガイドを読めば、追加実験の全体像を把握し、論文執筆時に必要な情報を迅速に参照できます。**
