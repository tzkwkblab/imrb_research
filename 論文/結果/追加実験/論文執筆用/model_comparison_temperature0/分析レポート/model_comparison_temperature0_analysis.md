# Steam モデル比較実験結果の LLM 分析（temperature=0 版）

生成日時: 2025-11-27 12:08:48

## 実験概要

- **データセット**: steam
- **アスペクト**: gameplay, visual, story, audio
- **GPT モデル**: gpt-4o-mini, gpt-5.1
- **総実験数**: 8 実験
- **成功**: 0 実験
- **失敗**: 0 実験
- **temperature**: 0.0
- **LLM 評価**: True

## 主要結果

### モデル別の平均スコア

- **gpt-4o-mini**: BERT=0.5453, LLM=0.3000
- **gpt-5.1**: BERT=0.5375, LLM=0.2500

### アスペクト別の比較

#### audio

- **gpt-4o-mini**: BERT=0.5214, LLM=0.2000
- **gpt-5.1**: BERT=0.5621, LLM=0.2000

#### gameplay

- **gpt-4o-mini**: BERT=0.5600, LLM=0.4000
- **gpt-5.1**: BERT=0.5423, LLM=0.2000

#### story

- **gpt-4o-mini**: BERT=0.5573, LLM=0.4000
- **gpt-5.1**: BERT=0.5167, LLM=0.4000

#### visual

- **gpt-4o-mini**: BERT=0.5425, LLM=0.2000
- **gpt-5.1**: BERT=0.5287, LLM=0.2000

## LLM による考察

1. **モデル間の性能差の意味**

まず平均値レベルでは、BERT スコアは

- gpt‑4o‑mini：0.5453
- gpt‑5.1：0.5375

と僅かに gpt‑4o‑mini が上回っており、0‑shot・temperature=0 という「事前知識だけに頼る」条件では、新しい gpt‑5.1 が一貫して優位とは言えないことが分かります。LLM 評価でも

- gpt‑4o‑mini：0.30
- gpt‑5.1：0.25

と、意味的類似度の観点でも gpt‑4o‑mini が優勢です。  
アスペクト別に見ると、audio だけは BERT で gpt‑5.1（0.5621）が gpt‑4o‑mini（0.5214）を明確に上回る一方、gameplay・story・visual では gpt‑4o‑mini が安定して高い（特に story での差：0.5573 vs 0.5167）。  
LLM スコアは gameplay・story で gpt‑4o‑mini が 0.4、gpt‑5.1 が 0.2〜0.4 とやや劣後しており、「対比因子として人間が妥当と感じる要約」を出せているのは gpt‑4o‑mini の方という解釈が自然です。したがって、Steam レビューのようなタスクで「安定した、妥当性の高い対比因子ラベル」を重視するなら、現状は gpt‑4o‑mini を優先的に選ぶ実用的根拠があると言えます。

2. **temperature=0 設定での結果の解釈**

temperature=0 では出力が決定論的になるため、「プロンプト × モデルの組合せ」の素の能力がそのまま現れます。ばらつきを利用した探索が効かないので、BERT・LLM スコアはいずれも「そのモデルが最初に思いつく典型的な対比因子表現の質」を測っていると解釈できます。  
全実験で BLEU スコアは 0.0000 であり、表層的な語彙一致はほとんど見られない。  
BERT スコアと LLM スコアの関係を見ると、BERT がやや高くても LLM スコアが 0.2 に留まる（audio/visual など）ケースが多く、逆に gameplay や story では BERT も高く LLM スコアも 0.4 に上がっています。これは「参照ラベルとの語彙・埋め込みレベルの近さ（BERT）」と「人間評価に近いレベルの意味妥当性（LLM）」が必ずしも一致しないことを示しています。  
0‑shot 条件にもかかわらず、gameplay・story で gpt‑4o‑mini が比較的高い LLM スコア（0.4）を安定して出している点は、「事前知識だけでレビュー集合間のメタ的な差異（詳細度、感情 vs 分析、構造化度など）をそれなりに抽出できる」ことを示すもので、対比因子抽出の 0‑shot 実現可能性を支持する結果と解釈できます。

3. **アスペクト特性とモデル性能**

アスペクトごとの違いを見ると、

- gameplay / visual：より具体的な要素（操作性、グラフィックス表現など）
- story / audio：より抽象的・主観的な要素（物語の深さ、雰囲気、音響の印象など）

という性質があります。  
本実験では、gameplay・story で gpt‑4o‑mini が BERT・LLM ともに優勢で、特に LLM 出力を見ると「A は具体的なメカニクス／スコアリングに言及、B は感情的・主観的な反応」といったレビュー文体・記述スタイルの差をうまく拾えているのが特徴です。これは、対比因子として「何について語っているか（コンテンツ）」だけでなく「どう語っているか（レジスター・文体）」を抽出できていることを意味し、説明可能性の観点では重要な性質です。  
一方、audio や visual では両モデルとも LLM スコアが 0.2 に留まっており、BERT スコアは中程度（0.52〜0.56）にとどまります。音声・映像はレビュー文中での言及濃度が低く、「雰囲気」「没入感」といった抽象的語彙に依存しやすいため、差分が曖昧で対比因子が抽出しにくいアスペクトだと考えられます。実務的には、

- gameplay / story のように言及の構造が豊富なアスペクト：gpt‑4o‑mini を使った自動対比因子抽出が有望
- audio / visual のように抽象度が高く記述が希薄なアスペクト：追加の few‑shot 例や明示的なキーワードリストを組み合わせる必要

という設計指針が導けます。

4. **研究への貢献**

この追加実験は、対比因子生成研究に対して以下の示唆を与えます。

- **モデル世代差よりタスク整合性**  
  0‑shot・temperature=0 条件では、より新しい gpt‑5.1 が必ずしも有利でなく、タスク（レビュー差分説明）との相性で gpt‑4o‑mini が優位になり得る。モデル選択は「世代」ではなく「対象タスクでの意味的評価」に基づくべきという教訓を与えます。
- **temperature=0 でも実用的な対比因子抽出が可能**  
  特に gameplay・story では、決定論的出力だけで BERT≒0.55、LLM スコア 0.4 程度を達成しており、「安定した基礎説明器」としての利用可能性が確認できます。対照的に、temperature>0 で多様な候補を生成し、そこからランキング・集約する手法のベースラインとして有用です。
- **LLM 評価指標の有用性**  
  BERT スコアが近い場合でも LLM スコアに差が出ており、単なる埋め込み類似度では捉えにくい「説明としての妥当性・焦点の合致」を LLM 評価が捉えていると解釈できます。本研究の最終目標（人間が理解できる対比因子説明）の観点では、LLM 評価は不可欠な補完指標と位置付けられます。
- **今後の方向性**
  - audio/visual のような難アスペクトに対して、few‑shot 例やアスペクト固有のガイドラインを導入し、LLM スコア向上が見込めるか検証する。
  - temperature>0 との比較により、「決定論的に安定だがやや凡庸な対比因子」vs「多様だがハルシネーションリスクを伴う対比因子」のトレードオフを定量化する。
  - LLM 評価を用いて、複数候補説明から最も対比因子として適切なものを選別するメタ評価フレームワークを構築する。

以上から、本実験は「0‑shot・決定論的条件でも、特定アスペクトに対して十分有用な対比因子抽出が可能である」ことと、「BERT ＋ LLM の多面的評価によりモデル選択・アスペクト設計の判断材料が得られる」ことを示し、創発言語の意味説明に向けた基盤整備として重要なステップになっています。
