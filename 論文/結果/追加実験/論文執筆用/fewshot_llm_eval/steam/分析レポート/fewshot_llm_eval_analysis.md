# Steam Few-shot実験結果のLLM分析（LLM評価有効）

生成日時: 2025-11-27 11:59:15

## 実験概要

- **データセット**: Steam
- **アスペクト**: gameplay, visual, story, audio（4アスペクト）
- **Few-shot設定**: 0-shot, 1-shot, 3-shot
- **総実験数**: 12実験
- **成功**: 12実験
- **失敗**: 0実験
- **LLM評価**: 有効（gpt-4o-mini）

## 主要結果

### Few-shot別の平均スコア
- **0-shot**: BERT=0.5526, LLM=0.3000
- **1-shot**: BERT=0.6530, LLM=0.3500
- **3-shot**: BERT=0.5754, LLM=0.4000

### アスペクト別の最高BERTスコア
- **gameplay**: BERT=0.6802, LLM=0.4000
- **visual**: BERT=0.6449, LLM=0.2000
- **story**: BERT=0.8356, LLM=0.6000
- **audio**: BERT=0.5850, LLM=0.2000

## LLMによる考察

Few-shot別の結果から，BERTスコアは0-shot=0.5526 → 1-shot=0.6530で大きく向上し，3-shot=0.5754で低下している。一方LLMスコアは0-shot=0.30 → 1-shot=0.35 → 3-shot=0.40と単調増加であり，両指標の間で最良設定が一致していない。1-shotは「正解アスペクト名に近い語彙を出す」方向に強く誘導され，BERT（意味表現類似度）が高まった一方，3-shotでは例示が増えたことで，より抽象的・説明的な対比因子（例：複数の側面をまとめた表現）を出力し，人手的には妥当だがゴールドラベルとの語彙・局所的文脈のずれが増え，BERTが相対的に下がったと解釈できる。それにもかかわらずLLMスコアが3-shotで最高であることは，「対比としての意味的一貫性」や「説明としての自然さ」は3-shotで改善している可能性を示唆する。したがって，Few-shotは単に性能を単調改善させるのではなく，出力スタイル（キーワード指向 vs 説明指向）を変え，BERTとLLM評価のトレードオフを生んでいると考えられる。

LLM評価指標の妥当性という観点では，全体としてBERTとの「完全な一致」は見られないが，相補的な特徴がある。例えばアスペクト別に見ると，storyは平均BERT=0.6419・最高BERT=0.8356と最も高く，LLMスコア平均も0.60と突出している。これは「物語性」「プロット」「キャラクター」といった語彙がゴールドと生成に共通しやすく，かつ人手的にも「ストーリー的対比」と判断されやすい領域であるため，両指標が整合しやすい。一方visualでは平均BERT=0.5708に対してLLM=0.20と低く，audioも平均BERT=0.5650・LLM=0.20と同様の傾向を示す。これらはレビュー文中で「グラフィック」「サウンド」といった語が頻出し，局所的な埋め込み類似度は得やすいものの，実際の対比因子としては「グラフィックの解像度」「アートスタイル」「BGMの存在感」といった細粒度の差異が重要であり，LLMはこの観点から厳しく採点していると考えられる。つまり，BERTは「表層概念レベルの一致」，LLMスコアは「人間が読んだときの説明妥当性」に近い評価をしており，両者の乖離は，とくに抽象的・多面的な説明が必要な設定で顕在化する。この意味で，LLM評価は自動指標として有用であり，特にBLEUのような語彙一致偏重の指標よりも，タスク目的に整合的なシグナルを与えているといえる。

アスペクト間の差に関して，gameplay（平均BERT=0.5969，平均LLM=0.40）とstory（0.6419, 0.60）は，両指標とも比較的高い。これらのアスペクトはレビュー文中で多様な記述（操作性，難易度，リプレイ性／物語構造，キャラクター描写）が現れ，発火／非発火集合間で「内容の質的な差」が明確に現れやすい。一方，visual・audioは表現がパターン化しやすく，「good graphics」「nice music」など汎用的な賛辞が多いため，集合A/Bの差分が「強度（良い／悪い）」レベルに留まり，対比的な条件を一意に捉えにくい。結果として，BERTは類似語彙の重なりによりそこそこのスコアを与えるが，LLMは「対比因子としての情報密度が低い」とみなし，低スコアとしたと解釈できる。この違いは，アスペクトごとに「潜在概念の多様性」「レビュー言語のバリエーション」が異なることに起因している可能性が高い。

本研究の目的である「LLMによる対比因子ラベル自動生成の実現可能性」という観点では，全体平均でBERTが0.55〜0.65程度，LLMスコアも0.30〜0.40（storyでは0.60）を達成していることから，「少なくともゴールドのアスペクト名と意味的に整合するラベルを，多くのケースで自動生成できる」ことが示唆される。特に1-shotでBERTがピークを取り，3-shotでLLMスコアが最大になる結果は，プロンプト設計を工夫することで，「人間が読んで納得する対比ラベル」を生成できる余地が大きいことを意味する。一方で，アスペクトによって評価指標間の乖離が大きく，単一指標では性能を過小・過大評価しうることも明らかになった。対比因子抽出という，本質的に「概念的・説明的」なタスクの評価には，BERTのような意味類似度指標に加え，LLM評価（あるいはBLEURT/BARTScoreのような学習ベース指標）を併用し，多角的に精度を測ることが必要である。とりわけ，本研究のように抽象的な対比概念（recommended/suggestionタイプのラベル）を扱う場合，LLMによる意味的妥当性評価は，人手評価との相関が高い補助指標として重要な役割を果たしうる。
