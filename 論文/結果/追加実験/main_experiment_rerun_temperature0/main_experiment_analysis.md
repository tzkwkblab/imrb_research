# メイン実験結果のLLM分析

生成日時: 2025-11-26 17:06:52

## 実験概要

- **データセット**: SemEval(4実験), GoEmotions(28実験), Steam(4実験)
- **総実験数**: 36実験
- **Few-shot設定**: 0-shot（固定）
- **temperature**: 0.0
- **max_tokens**: 2000
- **group_size**: 100
- **成功**: 36実験
- **失敗**: 0実験

## 主要結果

### データセット別の平均BERTスコア
- **semeval**: 0.7531
- **goemotions**: 0.7127
- **steam**: 0.5403

### アスペクト別の最高BERTスコア（上位5件）
- **embarrassment**: 0.8941
- **gratitude**: 0.8445
- **disgust**: 0.8316
- **disappointment**: 0.8255
- **fear**: 0.8209

## LLMによる考察

1. **データセット間の性能差の要因**

まず BERT スコアを見ると，SemEval（平均 0.7531）＞ GoEmotions（0.7127）＞ Steam（0.5403）という明確な序列がある．SemEval は「battery / food / screen / service」といったプロダクトレビューの具体的アスペクトであり，正解ラベルも「battery」「screen」のような単語レベルの明示的カテゴリであるため，LLM が A/B の差分を単一語で要約しやすい．実際に *screen* で 0.8012，*battery* で 0.7646 と高い値が得られている．

一方，Steam は同じレビュー系だが，「audio / gameplay / story / visual」というゲーム固有アスペクトであり，テキスト側の言い換え多様性が大きく，かつアスペクト間の語彙的オーバーラップも多い（例：game, play, sound, graphics が複数アスペクトで出現）ため，対比が弱く，BERT=0.52–0.56 程度に留まっていると解釈できる．

GoEmotions（平均 0.7127）は感情ラベルを正解とするが，感情語は日常文中では多様な表現（喜び→嬉しい / 楽しい / 最高 / love it, etc.）で現れるため，**表層一致が低くても意味類似は高く出やすい**タスクである．事実，BLEU は全体で 0.0073 とほぼ 0 だが，BERT スコアは 0.54–0.89 と広い範囲で分布しており，ドメインが会話的で語彙多様性が大きいことが反映されている．

2. **アスペクトによる性能差の要因**

アスペクト別トップ 10 では，*embarrassment* 0.8941，*gratitude* 0.8445，*disgust* 0.8316 など，**感情語の中でも比較的明確で強い情動カテゴリ**が高スコアを示す．これらはテキスト集合 A/B の語彙分布も「恥ずかしい」「ありがとう」「気持ち悪い」といった典型表現で強く分かれるため，LLM が「このニューロンは embarrassment 系の文で発火する」と抽象化しやすいと考えられる．

一方で，GoEmotions 内でも *neutral*（0.5437），*approval*（0.5900），*realization*（0.5881）など，境界が曖昧でコンテキスト依存的なアスペクトでは性能が低い．中立や気づきなどは，表層から判別できる決定的キーワードが少なく，他ラベルとの重なりも大きいため，A/B 差分が弱い「ノイズの多いタスク」になりやすい．

SemEval の *screen*（0.8012）や *battery*（0.7646）は，対象物が具体的かつレビュー文中で高頻度に名指しされるため，高 BERT に直結している．対照的に Steam の *story*（0.5383）などは，物理的対象ではなく抽象的なゲーム要素であり，かつレビューでは「story, plot, narrative」が混在して用いられるので，LLM が原ラベル *story* と厳密に一致する語を選べるとは限らず，スコアが抑えられていると推察できる．

総じて，**(1)語彙的に明示的で，(2)他アスペクトから分離しやすい概念ほど BERT スコアが高い**という傾向が明瞭である．

3. **対比因子抽出への示唆**

全 36 実験で，SemEval/GoEmotions の多くのアスペクトが BERT>0.7 を達成していることは，「A/B に基づく LLM の自動ラベル生成」が少なくとも**意味的近さの点ではかなり有望**であることを示す．特に GoEmotions の強い感情カテゴリ（embarrassment, gratitude, disgust, joy, love など）は 0.77–0.89 と高く，LLM が純粋にテキスト差分だけを見て，適切な抽象的ラベルを選べていると解釈できる．

temperature=0.0 での実験という点も重要で，**出力の多様性を犠牲にして決定論的かつ一貫したラベリング挙動**を観測している．それにもかかわらず BERT スコアにアスペクト間・データセット間の差が生じているのは，ランダム性ではなく，データ側の可分性・概念の明瞭さが主因であることを示す．

さらに，SemEval/GoEmotions では LLM 評価平均がそれぞれ 0.55 / 0.47 と，**人手的な妥当性も中程度には確保されている**一方で，Steam は 0.30 に落ちる．これは，対比因子ラベル自動生成が「どのニューロン／どのアスペクトに対しても均質にうまくいくわけではなく，ドメイン選択とクラスタ品質に強く依存する」ことを示唆する．

4. **今後の研究への示唆**

改善方向として，以下が示唆される：

- **Few-shot プロンプトの導入**：本実験は 0-shot 固定であり，スタイル誘導が弱い．SemEval など，正解ラベルの形式を数例見せれば，GoEmotions や Steam でも「短い名詞フレーズ」「単一感情語」への収束を促せる可能性が高い．
- **temperature / max_tokens の調整**：対比因子ラベルは通常 1–3 語程度が望ましい．max_tokens=2000 は冗長説明を許容しており，モデルが「説明文」に寄りやすい．max_tokens を小さくし，temperature を 0.1–0.3 程度に上げて「近縁語の中から最も適切なラベルを探る」挙動を検証する余地がある．
- **データ前処理とクラスタリングの改善**：Steam のような低スコア領域は，A/B の分離度が低い可能性が高い．ニューロン発火サンプルのフィルタリング（上位発火のみ使用），もしくはサブクラスタリングにより，より「純度の高い」集合を作ることで，対比因子が明瞭になるかを検証すべきである．
- **評価指標の拡充**：BLEU がほぼ 0 に張り付いていることから，本タスクへの不適合が実証的に裏付けられた．BERTScore に加え，BLEURT, BARTScore, MoverScore など学習ベース指標を導入し，人手 LLM 評価との相関を分析することで，「抽象ラベル命名性能」をより適切に評価できる．

総合すると，本研究の結果は，「LLM による対比因子ラベル自動生成は，概念が明瞭で A/B の差分が強い場面では十分実用的な精度が得られる」一方で，「境界の曖昧な感情・ドメイン混在アスペクト・可分性の低いクラスタでは性能が劣化する」ことを定量的に示しており，今後はデータ側の構成とプロンプト設計を含めた**パイプライン全体の最適化**が必要だと結論付けられる。
