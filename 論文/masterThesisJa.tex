\documentclass[a4paper]{jreport}

\usepackage{masterThesisJa}
\usepackage{amsmath}
\usepackage{url}
\DeclareUnicodeCharacter{2248}{\ensuremath{\approx}}
\setcounter{tocdepth}{3}
\setcounter{page}{-1}

\maintitle{大規模言語モデルを用いた対比因子ラベル生成手法に関する研究}{A Study on Generating Contrastive Factor Labels for Explainable AI}

\publish{2025}{12}

\student{202421675}{清野　駿}{Seino Shun}

\jabst{
　大規模言語モデル（LLM）の実運用では，判断の妥当性を説明可能にすることが重要であるが，既存手法は未知データごとに人手で解釈ラベルを付与する負担が大きい．本研究は，将来的にニューロン発火条件から得られることを想定した二つのテキスト集合 A/B に対し，その差異を自然言語で要約する「対比因子ラベル」を自動生成する枠組みの実現可能性を検証する．実験では，ニューロンの活性化ログそのものではなく，アスペクトラベルや CLIP 類似度に基づいて構成したテキスト集合（グループA: 特定概念を含むテキスト, グループB: 含まないテキスト）を，この理想的タスクの近似として用いる．手法は，A/B の代表テキストを入力として，Few-shot 例示（0/1/3-shot）付きプロンプトで LLM に差分説明を生成させる．評価は SemEval-2014 ABSA（Restaurants/Laptop），Steam ゲームレビュー，GoEmotions，COCO Retrieved Concepts を含む多様なドメインのデータセットを用い，各グループ 100 件の条件で GPT-4o-mini を含む複数の LLM を用いて対比因子ラベルを生成し，その品質を BERTScore（本論文では Sentence-BERT 埋め込みのコサイン類似度），BLEU，および LLM による意味的類似度評価を用いて多角的に測定した．結果として，全 36 実験の平均で BERTScore ≈ 0.698，BLEU ≈ 0.008 を得て，意味レベルの一致は中程度だが語彙一致は低いことが示された．また Few-shot 実験では 1-shot が最も良く，出力スタイルが「説明的叙述」から「一意に特定する語彙」へ矯正される傾向が確認された．アスペクト別には gameplay/food 等の語彙的に安定した具体的概念で高く，recommended/suggestion 等の抽象概念で低い傾向が見られた．以上より，本枠組みは一定の条件で有効であり，人手ラベリングの一部代替となる可能性がある一方，抽象概念の扱いと評価指標の高度化（人手・LLM 補助，非語彙的類似）が今後の課題である．
}

\eabst{
This study examines the feasibility of generating contrastive factor labels that are intended to explain neuron activation conditions, by comparing two text groups A/B. In our experiments, these groups are not constructed from actual neuron activation logs, but are approximated using aspect labels and similarity scores (e.g., CLIP-based Top-100 vs Bottom-100) so that group A contains texts strongly associated with a target concept and group B contains texts that do not. We prompt an LLM (GPT-4o-mini) with 0/1/3-shot examples to produce concise differences, and evaluate semantic and lexical alignment against gold aspect labels using BERTScore (implemented as cosine similarity of Sentence-BERT embeddings) and BLEU on SemEval-2014 ABSA (Restaurants/Laptop), Steam game reviews, GoEmotions, and COCO Retrieved Concepts, mainly with 100 samples per group. Across 36 main experiments, results show moderate semantic similarity (BERTScore ≈ 0.698) but very low lexical overlap (BLEU ≈ 0.008), with 1-shot giving the best performance in few-shot settings by steering outputs toward uniquely identifying terms. Performance is higher for lexically stable aspects (e.g., gameplay, food) and lower for abstract ones (e.g., recommended, suggestion). The approach can partially reduce manual labeling, while handling abstract concepts and improving evaluation beyond lexical matches remain as future work.
}

\advisors{若林　啓}{伊藤　寛祥}

\begin{document}

\makecover

\addtolength{\textheight}{-5mm}
\setlength{\footskip}{15mm}
\fontsize{11pt}{15pt}\selectfont

\pagebreak\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\parindent=1zw
\pagebreak\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{plain}

% 各章を読み込む
\input{chapters/01_introduction}
\input{chapters/02_related_work}
\input{chapters/03_proposal}
\input{chapters/04_experiment_and_results}
\input{chapters/05_discussion}
\input{chapters/06_conclusion}
\input{chapters/acknowledgments}
\input{chapters/references}

\end{document}
