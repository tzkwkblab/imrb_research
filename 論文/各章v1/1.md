第 1 章 序論 (Introduction)
1.1 説明可能 AI（XAI）の必要性と従来の限界
近年の深層学習（DNN）モデルは、医療、金融、自動運転といった社会的重要性の高いドメインにおいて優れた予測性能を達成している。しかし、その意思決定プロセスが人間にとって解釈不能な「ブラックボックス」であるという根本的な問題は、モデルの信頼性や公平性の担保、そして法的・倫理的な規制要件の遵守を妨げる最大の障壁となっている [1, 2]。この課題に対応するため、モデルの判断根拠を事後的に説明する説明可能 AI（XAI）の研究が活発化してきた。
従来の事後説明手法の信頼性とスケーラビリティの限界
従来、XAI の中心的な手法として広く普及したのは、LIME（Local Interpretable Model-agnostic Explanations）[3, 4] や SHAP（SHapley Additive exPlanations）[5, 6] に代表される、**事後説明（Post-hoc Explanation）**手法である [1, 7, 8]。これらの手法は、入力データ（例：画像ピクセル、テキストトークン）の摂動やゲーム理論に基づく貢献度（Shapley 値）の計算を通じて、個別インスタンスの予測に対する特徴量の寄与を可視化する [9, 10]。
しかし、事後説明手法には、以下のような根本的な信頼性の問題が指摘されている。第一に、これらの説明は本質的な曖昧さ（high degree of ambiguity）を抱えており、特に共線性を持つ特徴量に対して不安定な結果を示すことがあり、同等の性能を持つ異なるモデルが全く異なる説明を生成する不安定性が問題となる [1]。第二に、Bordt et al. (2022) は、事後説明アルゴリズムが、規制や倫理が求める「透明性」の目的を達成する上で**「不適切 (unsuitable)」**であると結論付けている [1, 11, 12]。これは、説明提供者と受け手の利害が対立する「敵対的文脈 (adversarial contexts)」において、事後説明が容易に操作可能であり、モデル開発者が都合の良い説明を選択的に提示できてしまうためである [1, 11]。
さらに重要な点として、LIME や SHAP は、あくまで個別インスタンスの局所的な特徴寄与に焦点を当てており [3, 5, 7, 9, 10, 13, 14]、モデルが特定のデータ集合（例：特定のニューロンが発火するすべてのサンプル）に対して系統的に何を学習しているか、というグループレベルの差異や集合間のコントラストを自然言語で説明する能力を欠いている [7, 9, 13, 15, 16]。
コンセプトベース XAI と「人手による命名」のボトルネック
事後説明の限界を克服するため、XAI の研究コミュニティは、低レベルの特徴量（ピクセル、トークン ID）から、人間が理解可能な**「コンセプト（概念）」**レベル（例：「縞模様」や「価格に関する言及」）で説明を提供するパラダイムへと移行している [2, 17]。コンセプトボトルネックモデル（CBM）[18, 19]や TCAV（Concept Activation Vectors）[18, 20]といったコンセプトベース XAI（C-XAI）は、モデルの内部状態を高レベルの概念に関連付けることで解釈可能性を向上させた [19]。
しかし、このアプローチは新たな、かつ重大なボトルネックを生み出した。それは、どの概念を監視・学習すべきかという概念の定義（ラベリング）が、依然として人間に依存している点である [2, 7, 17]。このプロセスは**「高コストな概念キュレーション (expensive concept curation)」**と呼ばれ [17, 19]、特に専門知識が不可欠なドメイン（例：医療）において、C-XAI の導入を阻む最大の障壁となっていた [16, 17, 19]。既存のベンチマークである SemEval-2014 においても、アスペクト（概念）ラベルは人手によるアノテーションに依存している [7, 21-23]。
メカニスティック解釈における「意味付け」の課題
モデルの内部動作を回路図レベルで徹底的に理解しようとする最先端のメカニスティック解釈（MI）分野においても、解釈の「意味付け」は手作業に依存する [8, 24, 25]。Anthropic による Attribution Graphs [24, 26, 27]のような研究は、モデルの計算プロセスをトレースし、特徴量間の相互作用をグラフとして可視化することで、ニューロン間の回路を発見する [24, 28]。
しかし、Attribution Graphs などが発見する個々の「特徴量」やノードが、人間にとって何を意味するのかを自然言語で特定するプロセスは自動化されていない [19, 24, 29]。研究者らは、解釈を容易にするために、関連する意味を持つ特徴量を手動でグループ化し**「スーパーノード」としてまとめているが、この手動ステップは「労働集約的 (labor-intensive)」**であり、情報の欠損を伴うことが指摘されている [24, 29]。
本研究の動機は、これら従来の XAI アプローチが直面する、「個別インスタンス中心の説明の限界」、「高コストな人手ラベリング依存」、そして**「発見された内部構造への命名の自動化の欠如」という 3 つの課題の交差点に存在する [8, 30]。信頼性が高く、低コストで、スケーラブルな解釈可能性を実現するためには、モデルの内部状態に基づき、かつ集合間の差分を自然言語で記述する自動命名手法**が不可欠となる [30]。
1.2 研究課題：集合差分によるニューロン対比因子命名
本研究が取り組む主要な課題は、非教師ありコンセプト抽出などで発見されたモデル内部の解釈可能な構造、すなわちニューロン発火条件の対比因子ラベルを、2 つのテキスト集合（発火群 A vs 非発火群 B）の差分からスケーラブルに自動生成することである [7, 8, 31, 32]。このタスクは、従来の XAI 研究における「概念の発見」と「命名」を統合する初の試みであると位置づけられる [7, 33, 34]。
タスクの定式化と新規性
従来の非教師ありコンセプト発見手法（UCBM [33, 35, 36]や CCE [33, 37, 38]など）は、人間による事前定義なしにモデル内部から概念（潜在ベクトル）を抽出する点で大きな進歩を遂げた [11, 33, 35]。しかし、これらの手法が発見するのは、あくまでも潜在的なベクトル表現であり、そのベクトルに人間が理解できる「自然言語の名前」を自動で付与する機能は欠如している [13, 19, 33]。命名プロセスは、発火サンプルを見た研究者による手動分析に依存していた [13, 33]。
本研究は、この**「発見された概念の手動での意味付け」という C-XAI の最大のボトルネックを直接解消することを目指す [13, 33]。具体的には、モデル内部の特定のニューロン（またはコンセプト）が強く発火したテキスト集合 A と、発火しなかったテキスト集合 B を入力とし [31, 39, 40]、集合 A に特有で集合 B に見られない意味的差分**を、自然言語ラベル L として生成するタスクを定式化する [31, 32]。このタスクは、NLP における「コントラスティブ要約 (Contrastive Summarization)」[7, 13, 15, 19, 41]の系譜に属するが、これを XAI ドメイン（モデル内部状態の解釈）に初めて適用する点で新規性を有する [13, 37, 42]。
このタスクは、単一インスタンスに対する反事実的説明（例：CEM [15, 43]）とは異なり、集合間の一般的・代表的な差分を記述するものであり [13, 33, 43, 44]、ニューロンが何を計算しているかを集合レベルで説明することを可能にする [15, 19]。
LLM による解決の仮説
本研究は、この高難度な「集合差分からの自然言語命名」タスクの解決策として、大規模言語モデル（LLM）の強力な文脈理解能力と生成能力を活用する [7, 8, 31, 45]。LLM（特に GPT-4o-mini）をコントラスト生成器として利用し、入力された A 群と B 群のテキスト群から、A 群に特徴的で B 群に欠如している意味的な側面を推論させ、簡潔なラベルを自動生成させる [31, 46, 47]。
このアプローチは、人手によるアノテーションや複雑な事後分析のステップを排除し、非教師ありで抽出された概念（UCBM や Attribution Graphs [13, 26]が発見した特徴量 [8]）に、スケーラブルかつ自動で人間が理解できる「名前」を付与する命名モジュールとして機能する [13, 19, 33]。この仮説の妥当性を、定量的な実験を通じて検証することが、本論文の主要な目標となる。
1.3 提案手法の概要と本研究の貢献
提案手法の概要
本研究は、LLM (GPT-4o-mini) を用いたコントラスティブ要約を核とする [7, 31]。手法の実現にあたっては、以下のステップを踏む。

1. データ収集とグルーピング：モデル内部の特定のニューロンの発火度に基づき、発火が強いテキスト群 A と、発火が弱い（または非発火の）テキスト群 B を抽出する（グループサイズ group_size はパラメータとして調整可能）[24, 48]。
2. Few-shot ICL（In-Context Learning）によるプロンプト設計：LLM に A 群と B 群のテキストを入力として提示し、「A 群に特有の内容的・意味的差分」を抽出するよう指示する [31, 39, 40]。Few-shot ICL は、本タスクにおける LLM の出力形式の揺らぎや語彙の安定性を確保するための**検証手段（サブ実験）**として導入する [3, 31, 49-51]。
3. 対比因子ラベルの生成：LLM に、集合差分を簡潔に要約した自然言語の「対比因子ラベル」を出力させる [31, 46]。
   実験と主要な発見
   本研究では、人手アノテーションされたアスペクトラベルを持つ SemEval-2014 Restaurant/Laptop データセット [1, 7, 21, 23, 52]を評価ベンチマークとして使用し、LLM が生成したラベルと正解ラベルとの意味的類似性を評価した [53, 54]。
   評価指標には、語彙的一致度を測る BLEU [30, 53, 55, 56]と、文脈的意味的類似度を測る BERTScore [7, 45, 53, 54, 57, 58]を採用した [53]。その結果、以下の重要な知見が得られた。
   • 意味的関連性の達成：生成されたラベルの品質を BERTScore で測定したところ、約 0.551 という中適度な意味的関連性を達成した [7, 45, 53, 54]。この結果は、LLM がニューロンの発火群と非発火群という集合的な差分から、人間が理解できる意味的な核を抽出できること、すなわち「LLM の文脈理解能力を活用すれば、概念の発見と命名を同時に行うプロセスを実現できる」という主要な仮説 [Main Hypothesis] が部分的に正しいことを示す [45, 53, 54]。
   • 語彙的一致の限界：一方、BLEU スコアは極めて低値（
   approx0.007）を示した [50, 53, 55, 59]。これは、正解ラベルが「food」のような単一の単語であるのに対し、LLM が「食べ物の品質に関する言及」のような説明的なフレーズを生成するため、語彙的重複を測る BLEU が本タスクの性質に不適であることを示唆している [53, 55, 59]。
   • 概念の具体性による優位性：生成ラベルの品質は、「food」「price」のような語彙的に安定した具体的なアスペクトにおいて特に高い優位性を示すことが確認された [3, 7, 45, 53]。対照的に、「story」「atmosphere」のような抽象的な概念の命名は、LLM が単純な要約ではなく高度な推論を必要とするため、性能が劣位となるという課題も特定された [33, 53, 60]。
   本研究の学術的貢献
   本研究は、XAI と NLP の境界領域において、以下の点で重要な貢献を果たす。
4. 新規タスクの提案と実現可能性の検証：XAI における「概念の発見」（UCBM, CCE）と「概念の命名」が分離していた構造的な課題を認識し、LLM によるコントラスティブ要約によって、この二つのプロセスを統合する**「集合差分によるニューロン対比因子命名」**という新規タスクを提案し、その実現可能性を定量的に検証した [7, 34, 54, 61]。
5. XAI パラダイムのギャップ解消：個別インスタンス中心の事後説明（LIME/SHAP）[7, 9, 54]の限界と、命名機能を持たない非教師ありコンセプト抽出（UCBM/CCE）[7, 13, 19, 54]のボトルネックを同時に解消する、スケーラブルなハイブリッドアプローチを提供する [7, 45, 54, 62]。
6. メカニスティック解釈の実用化への寄与：Attribution Graphs [24, 26]などによって発見されたニューロン回路や特徴量に対し、人間が理解可能な意味的な名前を自動で付与する手段を提供し、手動ラベリングに依存していた MI 分野のボトルネック解消に貢献する [13, 24, 31, 63, 64]。
7. Few-shot ICL の応用と最適化： Few-shot ICL を活用し、生成ラベルの品質とスタイルを矯正するアプローチを提案し、Few-shot 数の変動による影響を分析することで、LLM を用いた自動命名の安定性向上に向けた知見を提供する [3, 31, 65]。
