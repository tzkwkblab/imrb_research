# 実験計画書：修士論文最終版追加実験

## 1. 実験の目的

### 1.1 主要目的
- Steam Reviews、SemEval ABSA、GoEmotionsデータセットにおける対比因子生成の性能評価
- BERT/BLEU/LLMスコアの包括的な比較分析
- Few-shot例題の影響評価
- GPTモデル性能差の影響評価
- COCOデータセットにおける対比因子生成と解釈分析

### 1.2 期待される成果物
- データセット別スコア比較表（BERT/BLEU/LLM）
- Few-shot影響分析表（0-shot vs 1-shot vs 3-shot）
- GPTモデル性能差比較表（gpt-4o-mini vs gpt-5.1）
- COCO実験結果と解釈レポート

## 2. データセットとアスペクト設定

### 2.1 Steam Reviews
- **データセットID**: `steam`
- **分割タイプ**: `aspect_vs_others`
- **グループサイズ**: 300
- **対象アスペクト**: 
  - gameplay
  - visual
  - story
  - （必要に応じて追加）

### 2.2 SemEval ABSA
- **データセットID**: `semeval`
- **分割タイプ**: `aspect_vs_others`
- **グループサイズ**: 300
- **対象ドメイン・アスペクト**:
  - restaurant: food, service
  - laptop: battery, screen

### 2.3 GoEmotions
- **データセットID**: `goemotions`
- **分割タイプ**: `aspect_vs_others`
- **グループサイズ**: 300
- **対象感情**（代表的なものを選択）:
  - joy
  - anger
  - sadness
  - fear
  - surprise
  - （必要に応じて追加）

### 2.4 COCO (Retrieved Concepts)
- **データセットID**: `retrieved_concepts`
- **分割タイプ**: `aspect_vs_bottom100`
- **グループサイズ**: 50-100（テスト用）
- **対象コンセプト**（代表的なものを選択）:
  - concept_0
  - concept_1
  - concept_2
  - concept_10
  - concept_50
  - （必要に応じて追加）

## 3. 実験パラメータ設定

### 3.1 Few-shot設定
- **0-shot**: 例題なし
- **1-shot**: 例題1個
- **3-shot**: 例題3個

### 3.2 GPTモデル設定
- **gpt-4o-mini**: デフォルトモデル（コスト効率重視）
- **gpt-5.1**: 最新高性能モデル（比較用）

### 3.3 LLM評価設定
- **LLM評価モデル**: `gpt-4o-mini`
- **LLM評価温度**: `0.0`
- **LLM評価有効化**: `True`

### 3.4 その他パラメータ
- **温度**: `0.7`
- **最大トークン数**: `100`
- **ランダムシード**: `42`
- **アスペクト説明文**: 使用しない（標準設定）

## 4. 実験マトリックス

### 4.1 Steam/SemEval/GoEmotions実験
各データセット×アスペクト×Few-shot設定×GPTモデルの組み合わせ

| データセット | アスペクト | Few-shot | GPTモデル | 実験数 |
|------------|----------|----------|-----------|--------|
| Steam | gameplay | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| Steam | visual | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| Steam | story | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| SemEval (restaurant) | food | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| SemEval (restaurant) | service | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| SemEval (laptop) | battery | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| SemEval (laptop) | screen | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| GoEmotions | joy | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| GoEmotions | anger | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| GoEmotions | sadness | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| GoEmotions | fear | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| GoEmotions | surprise | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |

**合計実験数**: 72実験

### 4.2 COCO実験
各コンセプト×Few-shot設定×GPTモデルの組み合わせ

| コンセプト | Few-shot | GPTモデル | 実験数 |
|----------|----------|-----------|--------|
| concept_0 | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| concept_1 | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| concept_2 | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| concept_10 | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |
| concept_50 | 0, 1, 3 | gpt-4o-mini, gpt-5.1 | 6 |

**合計実験数**: 30実験

### 4.3 総実験数
**合計**: 102実験

## 5. 評価指標

### 5.1 自動評価スコア
各実験で以下のスコアを計算：
- **BERTスコア**: 意味類似度（0.0-1.0）
- **BLEUスコア**: 表層一致度（0.0-1.0）
- **LLMスコア**: LLM評価スコア（0.0-1.0）

### 5.2 出力データ形式

#### 5.2.1 データセット別スコア比較表
| データセット | アスペクト | Few-shot | GPTモデル | BERT | BLEU | LLM |
|------------|----------|----------|-----------|------|------|-----|
| Steam | gameplay | 0 | gpt-4o-mini | 0.xxx | 0.xxx | 0.xxx |
| ... | ... | ... | ... | ... | ... | ... |

#### 5.2.2 Few-shot影響分析表
| データセット | アスペクト | GPTモデル | 0-shot BERT | 1-shot BERT | 3-shot BERT | 0-shot BLEU | 1-shot BLEU | 3-shot BLEU | 0-shot LLM | 1-shot LLM | 3-shot LLM |
|------------|----------|-----------|-------------|-------------|-------------|-------------|-------------|-------------|------------|------------|------------|
| Steam | gameplay | gpt-4o-mini | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |

#### 5.2.3 GPTモデル性能差比較表
| データセット | アスペクト | Few-shot | gpt-4o-mini BERT | gpt-5.1 BERT | gpt-4o-mini BLEU | gpt-5.1 BLEU | gpt-4o-mini LLM | gpt-5.1 LLM |
|------------|----------|----------|------------------|--------------|------------------|--------------|-----------------|-------------|
| Steam | gameplay | 0 | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx | 0.xxx |
| ... | ... | ... | ... | ... | ... | ... | ... | ... |

#### 5.2.4 COCO実験結果
各コンセプトごとに：
- 実験設定（Few-shot、GPTモデル）
- LLM出力テキスト
- BERT/BLEU/LLMスコア（参考値）
- **解釈**: 出力の意味的妥当性、コンセプトとの関連性、特徴的な表現パターン

## 6. 実験実行手順

### 6.1 準備
1. 仮想環境のアクティベーション
2. 環境変数の確認（OPENAI_API_KEY）
3. データセットの存在確認

### 6.2 実行方法
各実験は以下のいずれかの方法で実行：
- **対話型スクリプト**: `scripts/run_interactive_experiment.sh`
- **コマンドライン実行**: `src/analysis/experiments/2025/10/10/run_experiment.py`
- **設定ファイル実行**: YAML設定ファイルを使用した一括実行

### 6.3 結果保存
- **保存場所**: `src/analysis/experiments/{YYYY}/{MM}/{DD}/results/`
- **ファイル形式**: JSON（個別実験結果）+ Markdown（集計表）

## 7. 実装ファイル

### 7.1 主要モジュール
- **データセット管理**: `src/analysis/experiments/utils/datasetManager/dataset_manager.py`
- **対比因子分析**: `src/analysis/experiments/utils/cfGenerator/contrast_factor_analyzer.py`
- **スコア計算**: `src/analysis/experiments/utils/scores/get_score.py`
- **実験パイプライン**: `src/analysis/experiments/2025/10/10/experiment_pipeline.py`

### 7.2 設定ファイル
- **データセット設定**: `src/analysis/experiments/utils/datasetManager/configs/dataset_configs.yaml`
- **LLM設定**: `src/analysis/experiments/utils/config/paramaters.yml`

## 8. 注意事項

### 8.1 COCO実験の特殊性
- 正解ラベルがないため、BERT/BLEUスコアは参考値
- 出力の解釈が主要な評価軸
- 画像URLの取得と表示（可能な場合）

### 8.2 実験実行時間
- 1実験あたり約10-30秒（API呼び出し時間含む）
- 102実験の合計実行時間: 約17-51分（順次実行の場合）
- 並列実行可能な場合は短縮可能

### 8.3 コスト見積もり
- gpt-4o-mini: 約$0.15/1M tokens（入力）+ $0.60/1M tokens（出力）
- gpt-5.1: 価格は要確認（最新モデルのため）
- 102実験の概算コスト: 数ドル〜数十ドル（モデルとトークン数による）

## 9. 次のステップ

1. 実験計画書の確認と承認
2. 実験実行スクリプトの準備（必要に応じて）
3. 実験の順次実行
4. 結果の集計と表形式化
5. COCO実験結果の解釈とレポート作成
6. 修士論文への結果反映

