# 実験考察レポート: steam_visual_group_size_100_1_4o-mini_word

## 個別実験の詳細考察

以下では、ご提示データ（グループA/Bの代表サンプル群、正解ラベル "visual related characteristics"、LLM出力が事実上空（BERT/BLEU=0）であった事実）を踏まえ、要求された観点ごとに詳細に考察します。単語レベルの分析を重視し、具体例を挙げながら問題点と改善案を提示します。

1) 単語レベルでの特徴分析
- 代表的な有意語（グループAに多く見られる語／表現）
  - graphics / improved graphics / presentation / screenshots / beautiful / landscape / pixel / retro 2D / colors / day(time) and weather change / visuals / look(s) / display
  - UI/視覚関連語: minimap / GPS / coordinate system / screenshots won't reveal
  - aesthetic / mood-lifter / sweet colors / catchy music（音に触れる例もあるがビジュアル語と共出）
  - その他頻出語: relaxing / masterpiece / addictive / controls（ただしこれは視覚以外のゲーム性語でもある）
- グループBに多く見られる語
  - font / unreadable / privacy policy / key-binds / controls too difficult / patch / bug / works / port / replayability / performance
  - 一部で "driving physics, fuel and tire mechanics" や "retro 2D" 等、Aと語が重複する例あり
- 文脈（語がどのように用いられているか）
  - Aの "Screenshots won't reveal the full beauty..."、"landscape experience" 、"daytime and weather change"、"improved graphics noticeable" 等は視覚的印象を直接描写（視覚情報が体験の核であることを示す）。
  - Aの "pixel" / "retro 2D pixel" は見た目（画素表現・スタイル）に関する言及で、ビジュアル特徴に密接。
  - Bの "font is almost unreadable" は視覚―しかしUI/可読性に限定された視覚問題で、Aの「グラフィック美」の肯定的言及とは性質が異なる。
  - 多くのサンプルはゲームプレイやバグ、マルチプレイ、難易度、開発者評など視覚以外の語を混在させており、視覚語が必ずしも主題とは限らない。
- 感情的／意味的ニュアンス
  - Aは肯定的情動語（masterpiece, love, relaxing, mood-lifter, addictive, beautiful）が頻出。視覚表現に対する肯定的評価と共起する傾向。
  - Bは批判的・中立的な語（unreadable, privacy policy, bug, controls too difficult）や事務的報告が多く、視覚言及があっても不満の文脈（例：font unreadable）であることが多い。
- 留意点（データ品質で明らかになった問題）
  - 同一文や極めて類似の文がAとBに重複して見える（例：「The good news is the driving physics, fuel and tire mechanics... However, the overall package...」が双方に登場）。グループ間の重複はコントラスト信号を弱め、LLMが差分を学びにくくする重大要因。

2) 文脈・意味的ニュアンスの考察
- A群の共通的文脈特徴
  - 視覚的・感性的体験の強調（"landscape experience", "Screenshots won't reveal", "improved graphics", "sweet colors" など）。視覚表現が体験の中心であり、肯定的情緒と結びつく例が多い。
  - 「表示（見た目）→感情（感嘆・没入）」という因果的言及が多く、視覚要素が評価を牽引している。
- B群との意味的差異
  - Bは技術的・手続き的な不満や操作性（controls, key-binds）、ポリシー（privacy）や互換性/移植（port）に関する言及が目立ち、視覚は問題点の一つに過ぎない。Aは視覚が主題、Bは多岐の問題が主題である点で概念的に異なる。
  - ただし両群に視覚語が混在しているため、単純な「A=visual, B=non-visual」という二分は不十分。視覚の「ポジティブな美観」か「ネガティブな可読性/UI問題」かという細かな違いが実際の差分。
- 抽象概念・間接表現
  - Aには抽象化された表現（"landscape experience", "more than just a driving simulator"）があり、視覚的・情緒的な総体を指すメタ表現がある。これらは単語頻度だけで抽出しにくい抽象概念であり、LLMに要約を委ねる価値がある一方で、例示不足だと適切な命名を誘導できない。

3) 正解ラベルとの比較
- 与えられた正解ラベル: "visual related characteristics"
- LLM生成対比因子の状況: 提示では空（あるいは無意味出力で評価が0）であり、実際の出力が無かった／評価系に渡せる文字列がなかった可能性が高い（BERT/BLEUとも0は、出力空や参照未設定のケースを強く示唆）。
- 一致する部分
  - データ自体には「visual」語彙・概念が確かに含まれ、A群サンプルは正解ラベルと整合する情報を持っている（graphics, screenshots, pixel, landscape など）。
- 不一致・乖離の原因（具体的）
  - モデルが出力を返さなかった（空出力／無効出力）可能性が高い：その場合当然一致は0。
  - プロンプト設計やFew-shot例が不適切で、モデルが「名詞句の短い対比ラベル」を生成しない指示になっていた可能性。
  - A/B間の語彙重複とデータノイズ（重複サンプル、視覚語が両群に出現）が高く、モデルが抽出すべき明確な差分を見出せなかった可能性。
- BERTスコアとBLEUの乖離について
  - 本ケースでは両者とも0。通常BERTScoreは語彙的に一致しなくても意味的類似があれば0より大きくなるため、両スコア0は「評価対象テキストが空」（hypothesisが空）か「評価処理エラー（参照未設定／トークナイザ不一致）」のどちらかである可能性が高い。
  - もしモデルは短い英単語句を出したが評価に使われた参照（"visual related characteristics"）とトークン化や大文字小文字で整合しなかった場合でも、BERTScoreが完全に0になるのは稀。従って実運用面ではまず出力有無と評価パイプラインのログを確認すべき。

4) 実験設定の影響
- Few-shot（1-shot）の影響
  - 1-shotは指示の「スタイル学習」には弱い。今回のタスクは「集合差分を一語／短語で命名する」という非常に具体的な出力形式を要するため、複数例（3–5-shot）で「入力（A例/B例）→出力（短い名詞句）」のペアを見せる方がLLMに形式を学習させやすい。
  - さらに、例示は正例（視覚差分があるケース）と負例（語彙重複あるが差分は別）を混ぜることでモデルが「何を対比因子とみなすか」を学べる。1-shotではこの一般化が難しい。
- グループサイズ・データ特性の影響
  - 現データはA/B各100件（代表サンプルでは100と明示）で、グループサイズ=100は小さくも大きくもないが
    - ノイズ（同一文の重複、複合的主題の存在）が多い場合、比較的まとまったサンプルでも差分を埋もれさせる。
    - group_size を小さくするとランダムノイズの影響が増す。大きくすると雑多な語が混入してコントラストがぼやける（特にレビューのように話題が広いデータでは）。したがって最適サイズはデータの「トピック集中度」に依存する。
  - Steamデータの特性（ゲームレビュー）は「グラフィックス、操作性、バグ、音楽、システム要件、ストーリー、マルチプレイ」など複数トピックが混在するため、グループ化基準（発火の定義）が視覚に偏っていないとA/Bに別トピックが混ざりやすい。
- 実験実行上の技術的問題点（推定）
  - 出力がない／評価で0になる問題は、LLM呼び出し・タイムアウト、出力フォーマット違反、あるいは出力をフィルタリング・破棄する後処理のバグが原因の可能性あり。まずはAPIログ／stdoutを確認すべき。

5) 改善の示唆（実施可能で具体的）
- データ前処理・統計的抽出フェーズを追加する（LLMに直接投げる前の自動前処理）
  - 代表的手法：log-odds ratio（informative Dirichlet prior）、chi-squared、TF-IDF上位単語、PMIでA/Bの差分語を抽出。これにより「視覚関連の候補語群（graphics, screenshots, pixel, colors, font, presentation）」を候補として得られる。
  - 抽出した候補n-gram（名詞句・形容詞＋名詞）をLLMに渡し、「これらを統合して1–3語の対比因子ラベルを作れ」と指示するパイプラインが有効。
- プロンプト設計の改善
  - 例示を3–5ショットに増やし、各例は (A集合例抜粋／B集合例抜粋) → 期待ラベル（短い英語名詞句）の形式にする。例はポジティブ視覚差分／ネガティブ視覚差分／視覚以外の差分の否定例を混ぜる。
  - 出力制約を明確に（"Output must be a single short noun phrase (<=4 words), in English, no explanation"）。さらにトップ3候補＋支持するキーワード（supporting n-grams）を求めると解釈の透明性が上がる。
- グループ構築の改善（サンプリング）
  - A/Bにおける「話題の均一化」を試みる：例えばレビューから視覚関連文のみ抽出してAを構成し、Bは視覚以外の文のみで構成するか、両群とも共通の話題分布（ゲームジャンル・評価スコア・長さ）を揃える。これにより視覚以外の混同変数（感情／バグ言及等）を減らせる。
  - 重複サンプルの除去（同一文や近似文が両群に存在するとコントラストが破壊されるため必須）。
- 評価方法の改善
  - 参照ラベルを単一の短語だけに依存しない。人手による複数の合意ラベルを収集し（トップ3許容）、自動評価にはBLEURT/BARTScore/MoverScoreを併用して語彙差異と意味的一致性の双方を評価する。
  - また人手評価（ラベルの妥当性、支持するn-gramsの一致）を少数件で行い、自動スコアと相関を確認する。
- モデル側の出力・ロギング改善
  - APIレスポンスを生ログで保存し、空出力／エラー発生時に再試行を行う。出力が空になった場合のフォールバック（再プロンプトや別モデル）を実装する。
- 具体的ワークフロー提案（工程）
  1. A/Bそれぞれから上位差分n-gramsをlog-odds等で抽出（例: graphics, screenshot, pixel, colors）。
  2. 3–5ショットプロンプト（事前に用意）＋抽出キーワードを与えてLLMに「1語〜3語で命名」させる。出力は top-3 候補 + supporting n-grams。
  3. 候補をスコア（頻度×信頼度）でソートし人手で最終承認（運用時は閾値以上で自動採用）。
  4. 評価はBLEURT/BARTScore + 少数の人手評価で検証。
- group_sizeに関する具体的アドバイス
  - 小さいgroup_size（例50）はばらつきに敏感、ノイズが強い場合はNG。大きいgroup_size（≥200）ではトピックの混入が増えるため、まずは中間（100程度）でトピック収束度（例：A内に占める視覚語出現率）を測ってから最適サイズを決定する。実験としては50/100/150/200/300それぞれで（1）前処理後の"視覚語率"、（2）LLM出力の信頼度（モデルが提示するself-reported confidenceがあれば）をプロットして安定点を探すとよい。

総括（短く）
- データ自体には「視覚関連」を示す語彙・表現が含まれるが、A/B間で語彙重複や話題の混在（操作性、バグ、プライバシー等）が多く、明確な集合差分信号が弱い。加えて代表サンプルの重複が観測され、これが最も破滅的にコントラスト学習を阻害している可能性が高い。
- 実務的改善は二段構えが有効： (1) 統計的に差分語を抽出してノイズを除去／候補生成、(2) LLMに形式化されたFew-shotプロンプト（複数ショット・出力制約）で候補を命名。評価はBLEURT等の学習ベース指標＋人手評価で行う。
- まずは「モデルがなぜ出力を返さなかったか（ログ確認）」「A/B内重複の除去」「Few-shot例の拡充」を優先し、それから上記パイプラインで再実験することを強く推奨します。

必要であれば、（A）代表サンプル全体の単語頻度・log-odds解析を実行して差分上位語を列挙する、（B）具体的な3–5ショットプロンプト案を作成する、（C）評価パイプライン（BLEURT/BARTScoreの導入手順）を提示します。どれを優先するか指示ください。

## steam_group_sizeカテゴリ全体の考察

以下は「steam_group_size」カテゴリ（Steamレビューの群比較での対比因子自動生成実験群）に対する、与えられた個別実験考察ログ（20件）を踏まえたカテゴリ全体の総合考察です。問題点の要約、観察された共通パターン、設定要因の影響、今後の示唆を優先度付きで整理しました。

1. カテゴリ全体の傾向（共通パターン）
- 出力欠落／評価ゼロが支配的
  - 多くの実験で「LLM生成対比因子」が実質的に空で、BERT/BLEU が共に 0.0 になっている。これは単なる性能低下ではなく「生成または評価パイプラインの欠陥（出力保存ミス、エンコード/前処理の不整合、タイムアウト／トランケーション等）」を強く示唆する。
- データ側の確度は概ね高い（ラベル妥当性）
  - 代表サンプル観察では、各カテゴリ（gameplay/visual/story/audio）に対応する語彙が群のどちらかに確かに偏在しているケースが多い（例：gameplay→controls/combat、visual→graphics/artstyle、story→narrative/characters、audio→soundtrack/voice）。つまり「正解ラベル自体は妥当」であり、問題はLLMの出力取得・整合化にあることが多い。
- ノイズ・トピック混在が顕著
  - Steamレビューは長文・罵倒・編集タグ（[h1],[b]等）や固有名詞、複数トピック（アート/音楽/操作/価格/サーバ）が混在するため、集合レベルの差分は「単一軸」ではなく複合的になりがち。これがラベル化の難しさを増している。

2. パフォーマンスの特徴（スコア分布と傾向）
- スコア分布
  - ログ上は多くが BERT/BLEU = 0.0。出力が存在すればBLEUは語彙一致により低めになりがち、BERTScore は通常一定の非ゼロ値を示すはずだが今回はゼロが多発しているため「評価不能（出力欠落/処理ミス）」が主因。
- 高スコア実験の共通特徴（観察からの仮説）
  - （観察が限られるが）高評価が期待できる条件は、（1）群内で特定トピック語が高頻度に偏在、（2）プロンプトが短ラベル出力を明確に指示、（3）前処理でノイズを除去し差分語を与えた、という組合せ。
- 低スコア（ゼロ）実験の特徴
  - ほとんど全ての実験に共通：Few-shot=1 のまま生テキストを大量投入、出力の生ログ未保存／評価前処理ミス、BLEUのみ依存等。これらが低スコア（あるいは評価不能）を招いている。

3. 設定パラメータの影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力形式（短いラベル vs 長文説明）」の誘導力が弱く、不安定。タスク（集合差分→短い概念ラベル）では 3–5 shot の方が安定性が上がるというログ中の示唆が一貫している。
- グループサイズ（group_size）
  - 小（50）: ノイズや偶発的サンプルに影響されやすく、代表性が不安定。  
  - 中（100–150）: 差分シグナルが比較的安定して抽出しやすいバランス帯。多くの改善案で推奨されているのはこのレンジを基準に試行すること。  
  - 大（200–300）: 多様性が増えシグナルが希薄化する一方で、十分な前処理（クラスタリング・代表抽出）を行えば安定化も可能。だが生データをそのままプロンプトに入れるとトークン制限や情報過多で失敗しやすい。
- モデル（gpt-4o-mini 等）
  - gpt-4o-mini 自体は汎用性が高いが、長文集合比較や厳密なフォーマット出力（短い名詞句ラベル）に対してはプロンプト工夫と前処理が不可欠。モデル変更（より大きなモデル）で改善は見込めるが、まずはパイプライン／プロンプト改善が先決。
- 評価指標の選択
  - BLEU は短いラベル比較に不向き（語彙揺れに敏感）。BERTScoreは意味的に優れるが今回の0多発はパイプライン問題を示す。BLEURT、BARTScore、埋め込みコサイン等の導入と、多参照/同義語辞書の準備が推奨される。

4. 洞察と示唆（主要知見と今後の研究方向）
A. 主要知見（要点）
- 根本問題は「運用（パイプライン）＋設計（プロンプト／前処理／評価）」にあることが最も多くの実験で示唆される。データ自体はラベルに対応するシグナルを持つことが多いが、LLM出力の取得/正規化/評価のいずれかで失敗している。
- 単にモデルを変えるより先に、（1）出力ログ保存、（2）評価パイプラインの前処理整合、（3）出力フォーマット強制、（4）差分語抽出などの前処理ワークフローを整備することが効果的。
- group_size の調整だけでなく「群内部のトピック収束度（視覚語率、音語率など）」を計測し、最適なサンプル数・代表化方法を決めるべき。

B. 優先度付き改善提案（実務的）
1) 最優先（必ず行う）
  - raw LLM 出力（API応答）を全て保存し、出力が空かどうか、トークン上限で切れていないかなどを検証する。出力が空なら直ちにAPIログ／エラー原因を調査。
  - 評価パイプラインの入出力前処理を固定：正解ラベル・生成ラベルともに同一の正規化（小文字化・trim・Unicode正規化・HTML除去）を行い、評価を再実行する。
2) 高効果（次に実施）
  - Prompt engineering：Few-shot を 3–5 ショットに増やし、出力を「1–3語の英語名詞句のみ (no explanation)」に厳格化。成功例 / 失敗例（bad example）を混ぜて示す。
  - 前処理パイプライン導入：TF-IDF / log-odds で A/B の差分キーワード上位を抽出し、そのリストを LLM に与えてラベル命名させる（二段階化）。またはクラスタリングで代表文を抽出して提示する。
  - 評価指標改善：BLEU廃止→BLEURT/BARTScore/embedding cosine を導入し、同義語マップ（許容ラベル群）を作る。
3) 中長期（実験設計 / 研究）
  - group_size 感度実験：50/100/150/200/300 の各サイズで複数ラン（シード）を実行し、ラベル出力の安定度（同一ラベル再現率、embedding類似度分散）を評価して最適サイズを選定。
  - ハイブリッドワークフロー：統計的手法で候補語を自動抽出 → LLM が短ラベルに正規化 → 小規模人手で承認する運用（半自動ラベリング）を構築。
  - 出力の透明化：LLMにラベルと同時に「支持する代表例/キーワード」を出力させ、説明可能性（explainability）を担保する。

C. 研究的示唆
- 集合差分ラベリング（group-level concept discovery）は「ノイズの多いUGC（Steam等）」では直接LLMに大量テキストを渡すだけでは不安定。統計的差分解析（log-oddsなど）とLLMの組合せ（証拠→命名）が有望である。
- 評価手法研究：短ラベル評価に適する自動指標の検証（BLEURT等）と人手評価の少量混入によるキャリブレーションが必要。
- 出力欠落の発生源（API側タイムアウト・filtering・プロンプト長超過など）を定量的にログし、再現性の高い障害モデルを作ると将来的な改善に寄与する。

5. 最後に：短期チェックリスト（実装担当向け）
- 保存ログの確認（raw responses + HTTP status）→ 出力が無ければAPIログを精査。
- 評価パイプラインの単体テスト（既知のref + hyp でBERT/BLEUが期待値を返すか）。
- 簡素な sanity-check プロンプト（3対3の代表テキストを入力、temperature 0、出力1語）で動作確認。
- 差分語の自動抽出を1回実行（各群 top-20）して、LLMに与えてラベル化する方式を試す（短期実験）。

———

要約：現状の失敗は主に「出力欠落／評価パイプライン不備」＋「プロンプト設計と前処理不足」に起因する。データ自体はラベルと整合するシグナルを含む場合が多い（各aspectごとに該当語彙あり）。まずはログ確認と前処理＋プロンプト改善（3–5 shot・短ラベル強制）、差分語抽出を組み合わせた二段階ワークフローを実装し、その上でgroup_size感度テストと評価指標の改善（BLEURT等）を進めることを強く推奨します。必要であれば、（A）代表サンプルからのTF-IDF/log-odds抽出結果、（B）3–5-shotプロンプトテンプレート、（C）評価パイプラインチェックリストの具体案を作成します。どれを先に出しますか？

