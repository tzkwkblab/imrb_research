# 実験考察レポート: semeval_restaurant_service_1_4o-mini_word

## 個別実験の詳細考察

以下、要求された観点に沿って実験結果（グループA/Bの代表サンプル群、正解ラベル "service related characteristics"、LLM出力が実質的に得られていない／評価スコアが 0 の状況）を詳細に分析します。まず要点をまとめ、その後に項目別の詳細考察と改善提案を示します。

要約（結論）
- グループAは「人・接客・応対に関する記述（friendly／attentive／prompt／people／tip／didn't get what they asked for 等）」が頻出し、正解ラベル "service related characteristics" と高い対応性を持つ。文脈は飲食店や顧客対応に関する主観的評価が中心。
- グループBはより多様で、「料理・品目名（tuna／teriyaki／salad 等）」「製品／技術属性（WiFi／USB／design／speed／weight 等）」が目立ち、必ずしもサービス記述に特化していないため A と意味的に差が出ている。
- LLM出力が得られていない（あるいは非常に乖離している）ため、BERT/BLEU 両スコアが 0 になっている可能性が高い。原因は（1）推論失敗（空応答やフォーマット不一致）、（2）プレースホルダ "$T" とノイズ的トークン（:-RRB- 等）による混乱、（3）Few-shot例が不十分でタスク指示が曖昧、のいずれか／複合であると推定される。
- 改善としては：プレ処理（$T の置換／正規化）、単語レベルの差分抽出（log-odds／TF-IDF）→ LLM に要約させる2段構成のプロンプト、Few-shotの増強（典型例を複数用意）、候補ラベル辞書の導入、評価指標の変更（BLEURT/BARTScore/人手評価）を推奨する。

以下、指定5観点ごとの詳細分析と示唆。

1. 単語レベルでの特徴分析
（A）グループAに特徴的な単語・表現（代表）
- 人的・接客語彙：friendly, attentive, people, staff に相当する表現（例： "friendly $T"（代表5）, "attentive $T"（代表16）, "some of the people did n't get what they asked for"（代表4））
- サービス品質を指す語：prompt, scatty（散漫な）, prompt（代表20 の "T prompt" は「対応が迅速」）、vibe/atmosphere/ambience（間接的に接客雰囲気と結びつく）
- 推奨／評価表現：highly recommend（代表11）, I must say how delicious the food and the $T was（代表8）→ "recommend"/"highly recommend" はサービス満足／不満いずれにも紐づく
- 価格・勘定周り：exorbitant prices（代表1）, less money you have to tip（代表6）→ チップや価格に関連する言及はサービス経験を反映することが多い
- カスタマーサポート語：call in, spend hours on their $T, no record（代表17）→ 企業の顧客対応を示す語

（B）グループBに特徴的な単語・表現（代表）
- 料理・品目名：Yellowfin Tuna, teriyaki, salad, side orders, appetizers（代表1,2,7,13,15）
- 製品／技術語：WiFi, USB, wireless system, PC, design, speed, weight（代表5,6,10,19,20）
- 品質／物性表現：cheaply made, cold, consistently good, portions sizes are very small（代表3,4,16,15）
- 文脈幅広さ：B に含まれる文は、飲食・製品・家電・ハードウエア等の複数ドメインが混在

（C）文脈での使用例とニュアンス
- "friendly $T" / "attentive $T"（A）→ 直接的に「スタッフ／店員／サービス担当者の振る舞い」を評している。肯定（friendly, attentive）または否定（didn't get what they asked for, scatty）でサービス質を示す。
- "prompt"（A）→ しばしば "service was prompt" の形で「対応の速さ」を指す。代表20の "T prompt" は文脈上「（サービスが）迅速だった」可能性が高い。
- "tip"（A）→ チップの言及は顧客-スタッフ関係に特有であり、料金/サービスの価値判断に関与する。
- Bの "WiFi", "USB", "design" 等は「機能／物理的特性」への評価であり、人対人的振る舞いではなく客観的属性の評価を行っている。
- 感情的側面：A は接客に関する強い感情表現（"INCREDIBLY POOR"（代表1）, "delicious"（代表8）等）が多く、サービスに対する満足／不満どちらも感情色が強い。B は物・料理自体の品質や仕様に対する評価で、必ずしも「社員／店員の態度」に言及しないため感情は対象依存でやや薄くなる場合がある。

2. 文脈・意味的ニュアンスの考察
（A）グループAの共通文脈的特徴
- ドメイン：飲食店や小売／カスタマーサポート等、人の介在が重要な場面が多い。サンプル17の「warranty」や「call in」などはサービスセンター等の B2C 対応も含む。
- 注目対象：$T（プレースホルダ）は多くのサンプルで「staff」「service」「waiter/waitress」などを指す語として使われている可能性が高い。文脈から、Aの $T は「人（スタッフ）」を表すケースが高頻度。
- 語用論的特徴：A の文章は「人の行動や応対（有能さ・無能さ・親切さ・注意深さ）」を記述する言い回しが多く、行為主体（スタッフ）に対する評価が中心となっている。

（B）グループBとの意味的差異
- ドメイン差：B は「物／料理の属性／技術仕様」に関する記述が多く、A の「人・サービス指向」記述とは概念層で異なるため、対比として自然に「service related characteristics」が浮かび上がる。
- 抽象度の差：A は「人間中心の振る舞い」という抽象概念（サービス品質，接客）に結びつく表現がまとまっている一方、B はより具体的で多様な属性（料理名、技術仕様）を含む。つまり A は集合として意味的に凝縮しているが、B は散逸している。
- 間接表現の有無：A には "vibe"／"ambience" といった間接的表現もあり、サービスの雰囲気まで含む概念が見られる。B では「雰囲気」に関する言及はあるものの、主に対象は料理やデバイス。

3. 正解ラベルとの比較
（A）正解ラベル "service related characteristics" との一致度
- 人手分析では、グループAのテキスト集合は明確に「サービス関連の特徴（スタッフの対応、接客の速さ/質、雰囲気、チップや顧客対応に関する苦情）」に集中しており、正解ラベルと高い一致を示す。代表例：
  - 「friendly $T」「attentive $T」→ 直接一致（スタッフ／サービスの良さ）。
  - 「some of the people did n't get what they asked for」→ サービス不備の指摘。
  - 「call in, spend hours on their $T」→ カスタマーサービス関連問題。
- したがって、人間の判断では正解ラベルは妥当であり、A と B の差分要約として "service related characteristics" は妥当である。

（B）LLM出力との不一致（観測）
- 実験ログでは LLM 生成対比因子が空欄（表示されていない）か、あるいは評価基準に全く対応していない出力が返ってきたため、BERTスコア・BLEUともに 0 となっている。
- 考えられる原因：
  1. 生成が「空」または改行などのみで返された（評価ツールは空出力で 0 を返す）。
  2. 生成ラベルが極端に長い説明文や逸脱したフォーマットで返り、評価スクリプトが想定する "短いラベル" とマッチしなかった（例えば JSON フィールド名が異なる等）。
  3. 生成が $T をそのまま残した多様な文で、評価ラベル "service related characteristics" と語彙的・意味的に接近せずスコアが 0 となった（ただし BERTScore が真に 0 になるのは稀。実際には計算上非常に低い値が出ることが多いので、スコアが「0.0000」と報告されているのは実装上の問題を示唆する）。
- したがって、スコア 0 の直接原因は「LLMの生成不成功＋評価スクリプト／前処理の不整合」が最も可能性が高い。

（C）BERTScore と BLEU の乖離に関する考察
- 今回は両方とも 0 だが、一般論として：
  - BLEU は語彙一致に極めて敏感（短文ラベル評価には不適）。語彙差異や同義語でも低スコアになりやすい。
  - BERTScore は文意味の埋め込み類似度を測るため、同義語やパラフレーズに寛容。ただし、出力が空・非テキスト・フォーマット不整合である場合は 0 に近くなる。
- 実験では双方 0 のため、評価プロセスか生成プロセスのどちらかで致命的な不整合（空出力・非標準トークン混入・評価スクリプトのバグ）があると考えられる。特に "$T" や特殊トークン（:-RRB-）がそのまま残っていると評価器側で正しくトークナイズされない可能性がある。

4. 実験設定の影響
（A）Few-shot（1-shot）の影響
- 1-shot はスタイル誘導や望ましい出力例を示すのに最小限で有効だが、本タスクは「集合差分を抽象的なラベルに圧縮」する高度な一般化を要するため、1-shot は不十分なケースが多い。期待される問題点：
  - 出力の粒度（長い説明 vs 短いラベル）の揺らぎ：1例だけでは「どの程度抽象化するか／名詞句で返すか」を安定して学習させにくい。
  - ドメイン混在（飲食・家電など）を示す例が無いと、モデルは A/B の差を誤解する可能性が高い。
- 改善策：典型的なA/B→ラベルのペアを 3–5 件与える（3-shot 以上を推奨）、アウトプット形式を厳格に指定（"one short noun phrase" など）。

（B）グループサイズ（group_size=100）とデータ特性の影響
- group_size=100 は統計的には十分なサンプル量で「集合レベルの差分」を抽出するには良いが、今回のデータ自体が「ドメイン混合（飲食・製品・カスタマーサポート等）」である点が問題。
  - A に比較的多く「サービス志向」サンプルが含まれている一方、B は多ドメインに散らばっているためコントラストが強く出るのは望ましい。しかし、LLMに与える生データにドメインノイズが多いと、モデルがどの語彙に注目すべきか迷う。
  - また "$T" の意味がサンプルごとに変わる（ある文は "staff" を指すが別文は "dish" を指す）と、集合差分の抽出が難化する。
- group_size の増減（サブ実験で予定している50/150/200/300）は、ノイズに対する頑健性を測るうえで有益。ただしドメイン均質化（同一ドメインのみで比較）と混合ドメインの比較を分けて評価する必要がある。

（C）その他設定要因
- プレースホルダ "$T" の存在：LLM が文脈から $T を「aspect term」と認識するか不明。もし $T が事前に「要素名」だと定義されていなければ、モデルは $T を外れ値／未知トークンとして扱い要約が困難になる可能性が高い。
- プロンプトの厳密さ：出力を「短い名詞句」に限定するなど具体的に指定するだけで生成安定性は大きく改善する。

5. 改善の示唆（具体策）
下記は短期〜中期で実行可能な改善案。

（A）前処理
- $T の正規化：可能なら $T を「[ASPECT]」など明示的なトークンに置換し、さらに文脈上で $T が何を指すか（staff／food／device）を事前に推定してタグ付けする（例：A-1..A-n の中で $T の直近単語を集計→staff/food/device の確率を算出）。
- ノイズ除去：":-RRB-" 等の HTML / エスケープ表現を正規化してモデルへの混乱を減らす。

（B）解析パイプラインの二段階化（推奨）
- ステップ1（統計的差分抽出）：A と B に対して unigram/bigram の正規化頻度、log-odds-ratio（モルガン等）や TF-IDF 差分を計算し、A に有意に高頻度な語（上位 N 個）を抽出する。これにより「特徴語リスト」を得る。
  - 例: A の上位差分語 = {friendly, attentive, prompt, tip, people, ambience, highly recommend, call in}
  - B の上位差分語 = {tuna, teriyaki, WiFi, USB, design, speed, cheaply made}
- ステップ2（LLM要約）：上で抽出した特徴語リストと代表サンプルを LLM に提示し、「これらの語を用いて一語〜短い名詞句で A の特徴を要約せよ（例：'service related characteristics'）」と厳密にフォーマット指定して出力させる。
  - こうすることで LLM の抽象化の役割に専念させ、ノイズを減らせる。

（C）プロンプト改良（Few-shot の強化）
- Few-shot を 3–5 に増やし、各例は「A集合（短い代表） / B集合（短い代表） → 正解ラベル（短い名詞句）」の形式で与える。
- 出力の形式を厳格に固定（出力は "label: <one short phrase>" のみ等）して評価スクリプトとの整合性を担保する。

（D）評価指標改良
- BLEU は短いラベルには不適。代替として：BLEURT（意味的一致を学習済みで短文に強い）、BARTScore、または Semantically-aware な埋め込み類似度（Sentence-BERT による cosine）を併用する。
- 最終的には人手による精度（ラベルが妥当かどうかのアノテータ評価）を採用し、学習ベース指標との相関を確認する。

（E）追加実験提案
- $T を特定語 ("staff" / "service" / "waiter" 等) に人工置換した場合の性能（プローブ実験）→ プレースホルダの影響を定量化。
- ドメイン単一化実験：同一ドメイン（飲食店のみ）での A/B 比較 vs 混合ドメインの比較を行うことで、ドメイン混合が与える影響を評価。
- Few-shot 離散化：1/3/5/10 shot を比較して所与タスクのサンプル効用を推定。

（F）モデル選択と出力検証
- gpt-4o-mini で失敗する場合、高性能モデル（GPT-4 系）で可否を比較。もし高性能モデルで良い結果が出るなら、現行失敗はモデル能力の限界か prompting に依存する。
- 出力が空や不正フォーマットだった場合に備え、返却が期待形式であるかを検査するガード（post-check）を入れる。例えば「出力が1ワード以上か」「コロンを含まないか」等の簡易検査を実装する。

付録：具体的な単語差分の例（手作業抽出）
- A に多い語句（抜粋）: friendly, attentive, prompt, people (didn't get what they asked for), tip, ambience/vibe, highly recommend, call in, warranty/customer service
- B に多い語句（抜粋）: tuna, teriyaki, salad, side orders, WiFi, USB, cheaply made, cold, design, speed, weight, portions sizes

結語
- 人手での検討結果としては、グループAは確かに "service related characteristics" を示しており、正解ラベルは妥当である。一方、実験で得られた LLM 出力・評価の障害は主にデータのノイズ（$T プレースホルダ、特殊トークン）、Few-shot の弱さ、及び評価パイプラインの整合性不良に起因している可能性が高い。上記の前処理・二段構成プロンプト・評価改善を適用すれば、対比因子ラベル生成の成功率は大幅に改善される見込みです。

必要であれば、A/B サンプル全100件ずつを用いた単語頻度差（log-odds ratio、TF-IDF 差分）を実際に計算し、上位特徴語をリスト化して差分可視化を行うスクリプト案を提示します。実データでの数値出力をご希望でしたらお知らせください。

## メイン実験全体の考察

以下は「メイン実験（group_size=100 統一）／各データセットに対する対比因子ラベリング実験群（gpt-4o-mini, few-shot=1 を基本）」に関する、個別実験考察ログ（実験1〜37）を総合して導いたカテゴリ全体の分析・洞察です。実務的な原因推定と優先的改善案を含めてまとめます。

要約（結論）
- 最も顕著な事象：多数の個別実験で「LLMの出力が得られていない／評価に回せる形式で取得できていない」ため BERTScore・BLEU が 0.0000 になっている。すなわち「モデルが出力しなかった／出力が消失した／評価パイプラインが壊れている」ことが主要因で、モデル能力の評価は事実上行えていないケースが多い。
- データ側では A 群が典型的に「一貫したドメイン語彙（例：food/service/battery/screen/emotions別語彙）」を示すことが多く、理論上は短い名詞句ラベルで要約可能であるにもかかわらず、実験設定（few-shot=1・入出力前処理・評価方式）の不備で失敗が多発している。

以下、指定観点ごとに整理します。

1. カテゴリ全体の傾向（個別実験から抽出された共通パターン）
共通パターン
- 出力欠落問題が圧倒的に多い
  - 多数の実験で「LLM生成対比因子」が空欄、BERT/BLEU が 0。ログ上は出力欠落（APIエラー／レスポンス保存漏れ／評価入力が空）または評価前処理での消失が最有力。
- A群はドメインに固有の語彙がまとまる
  - semeval/restaurants/laptop/amazon/goemotions 各セットで、A群は明瞭なトピック語（例：food→fresh/menu/taste、service→friendly/attentive、battery→charge/last、screen→resolution/froze、emotion→sad/joy/pride等）に収束している。つまり「集合差分」は存在するケースが多い。
- B群はより分散／雑多／ノイズが多い
  - B群は混在テーマ・技術語・雑談などが多く、対比が単純ではない場合もある（Aの信号を薄める）。
- $T$ や [NAME] のようなプレースホルダ・特殊トークンが入力に多く存在
  - マスク・プレースホルダが意味手がかりを隠すためモデルが迷う、あるいはプレースホルダが雑音となるケースが複数確認された。
- 生成は「短いラベル（名詞句）」の想定なのに、プロンプトやショットが説明文や長文を誘導してしまうケースが目立つ

データセット／アスペクト差
- 感情系（goemotions）
  - A群は明確にその感情カテゴリの語彙（例えば「admiration→amazing/wonderful」「anger→fuck/idiot」）を含む場合が多く、理想的には非常にラベル化しやすい。
- SemEval / Amazon / Laptop 等（属性系）
  - A群は特定アスペクト（food quality, service, battery life, price, delivery 等）を語る語彙で凝縮しており、本来は差分抽出＋命名が実務的に可能。
- 実際の失敗の分布はアスペクト依存より「実験運用（プロンプト・評価・ログ）」に強く依存している

2. パフォーマンスの特徴
スコア分布・傾向
- 並列して報告された多くの実験で BERT/BLEU が 0.0000（完全ゼロ）：実際には「性能低下」より「実験的欠損（出力や評価入力の欠如）」を示す
- 正常に出力が得られていたサブケースがほとんど報告されておらず、スコアの有意な正負分布を評価するデータが不足

高スコアになり得る条件（ログや人手観察からの逆推定）
- A群が語彙的に凝縮（同一アスペクト語彙が高頻度）→ラベリング容易
- プロンプトで出力形式が明示され、few-shot で短い名詞句例を与えている
- 前処理で代表語（top-n tokens by log-odds/TF-IDF）を抽出して入力に含めている（つまり「二段階ワークフロー」）
- 出力検査・フォーマット検証（非空チェック）を行っている

低スコア（今回の大多数）に共通する特徴
- 出力が空、あるいは出力フォーマットが評価用フォーマットと異なる（例：説明文 vs 短いラベル）
- BLEU を単独で評価に使っている（短いラベル評価に BLEU は不適）
- 評価パイプラインで文字コード・トークナイザ・改行などが整合していない

3. 設定パラメータの影響
Few-shot（ショット数）
- 1-shot は不安定要因
  - 多数の事例で「1-shot が出力形式の安定を欠く」→モデルが説明的応答や空応答を返すリスクが高いと示唆
  - 推奨：3～5ショットで例の質（短い名詞句によるラベル例を複数）を確保することで出力安定化

group_size（100）
- group_size=100 は概念抽出には十分
  - ただし「群の純度（A にアスペクトが集中しているか）」が重要。A 内にノイズ（異トピック）が多いと対比が弱まる
  - グループサイズの感度解析は有益（50/100/150/200/300）→既に計画されている通りだが、代表抽出法（クラスタ代表 or top tokens）併用を推奨

モデル（gpt-4o-mini 等）
- gpt-4o-mini は汎用性は高いが、
  - 出力の「短いラベル」を安定的に生成させるにはプロンプトとショット設計が重要
  - 一部の失敗（空出力）はモデルより運用（API/ログ/評価）に起因している可能性が高い
- モデル選択の影響は存在するが、まずはプロンプト・前処理・評価周りを安定化させるのが優先

その他実行パラメータ
- temperature（多様性）→低め(0–0.2)で決定的出力を狙うべき（短いラベル生成は多様性不要）
- max_tokens／stop_sequences：短ラベルを切られないように設定し、長文を強制しない

4. 洞察と示唆（研究・実務への具体的提案）
主要知見（実験群全体から）
1. 実験失敗の主要原因は「運用／パイプライン（出力取得・評価）の欠陥」であり、モデルそのものの性能評価が十分に回収されていないケースが多い。
2. A群は多くの場合、明確な集合的語彙手がかり（discriminative tokens）が存在するため、原理的には自動ラベリングは実現可能である。
3. 単語ベースのみでの判定は限界がある：句・共起・文脈（"at least", "I never knew" 等）を踏まえた処理が必要。
4. BLEU は短いラベル評価に不向き。意味的評価（BERTScore, BLEURT, BARTScore, embedding cosine）が必要。

優先的改善（短期：実験フローを直す）
- A. ログ＆評価健全性の確保（絶対必須）
  1. LLM の raw response（text）を全て保存し、出力の有無を自動チェックして「空なら再試行/アラート」。
  2. 評価前に参照と生成の正規化（UTF-8, strip, lower, NFKC）を行い空入力を弾く。
- B. 出力形式の強制化（プロンプト）
  1. 出力は「1–3語の英語名詞句（short label）」のみと指示し、few-shot で 3–5 例を与える（例は必ず正しい形式）。
  2. 生成候補を n-best で取り、後段で埋め込み類似度で再ランキングする。
- C. 前処理→LLM の二段構成
  1. 統計的差分（log-odds / TF-IDF / chi-square）で上位 discriminative tokens を抽出（A vs B）。
  2. それらトークン＋代表サンプルを LLM に与え、「この語群を要約して短いラベルを1つ出せ」とする。
  - これによりノイズ低減と安定性向上が期待できる。
- D. 評価改善
  1. BLEURT / BARTScore / SBERT cosine を導入（BLEU廃止または補助）。
  2. 多参照（paraphrase set）と人手評価（少量）を用意し、自動指標のキャリブレーションを行う。

中期的研究提案
- 1. アブレーション実験：few-shot数（0/1/3/5）、group_size（50/100/150/200）、前処理有無の 3 因子実験で安定性曲線を描く。
- 2. ハイブリッド法の評価：統計的差分→LLM命名 vs LLM単体（対比）で精度・再現性を比較。
- 3. 出力の根拠提示（explainable output）：LLM にラベルと「根拠トークン3つ」の同時出力を求め、信頼性指標を作る（自動スコア+人手検査で検証）。

運用上の注意点（倫理・安全）
- データに攻撃的／差別語が含まれる場合、モデル応答のフィルタや出力抑制が働く可能性がある（その結果出力が空になるケースあり）。フィルタの有無／ログは必ずチェックすること。
- ラベル化で人を動かす用途（自動モデレーション等）では検出誤差・誤学習に注意。人手介在を残すハイブリッド運用を推奨。

最後に：今すぐ着手すべき実務タスクリスト
1. raw model response の保存／空応答検出を実装（必須）。  
2. 評価前処理の自動チェック（参照・生成が非空であること）を実装。  
3. few-shot を 3-shot に増やしたテンプレートを作り、代表的小実験で安定化（短いラベルを返すかを検証）。  
4. 差分トークン抽出（log-odds）スクリプトを作成して A/B の上位語を確認 → LLM に提示するワークフローを試す。  
5. BLEURT / BARTScore / SBERT-cosine を導入し、少数人手ラベルで指標調整を行う。

もし望まれるなら、次のいずれかをすぐ用意します：
- A. 差分トークン抽出（A/B 各100件）を行う Python スクリプトと上位語リスト（出力形式 csv）  
- B. 改良版 few-shot プロンプト（英語／日本語）テンプレート（3-shot）＋出力検証ロジック（空応答検出）  
- C. 評価パイプライン修正案（BLEURT 並列導入手順・SBERT 電卓）

どれを優先して作成しましょうか？

