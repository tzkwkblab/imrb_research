# 実験考察レポート: steam_story_group_size_150_1_4o-mini_word

## 個別実験の詳細考察

以下、提示された観点に沿って本実験（グループA/B = 各150件、Few-shot=1、モデル=gpt-4o-mini、正解ラベル="story related characteristics"、評価でBERT/BLEU=0）の詳細な考察を示します。単語レベルの分析を重視し、具体例を交えて理由と改善案を提示します。

1. 単語レベルでの特徴分析
- グループAに特徴的な単語・表現（代表例）
  - 感情・物語性を示す語彙: "story"／"story's going"／"tears"／"cry"／"The tears won't stop"／"Oh shit..."／"pray"／"I'm an atheist"／"honest"／"not Recommended"／"Game of the year"／"love"／"delighted"／"deeply devoted"
  - 時系列・進展を示す表現: "First hour", "Second hour", "Third hour", "Fourth hour"（物語の進行や感情の高まりを記述）
  - 叙述的フレーズ・レビュー文体: "At its core", "To be completely honest", "I find it hard to describe", "One word to describe"
  - 作品関連固有名詞／文脈語: "Dishonored 2", "Steins;Gate", "Resdient Evil 4"（シリーズ性や続編言及）
  - その他雑多だが強い主観語: "cheap"（ネガ）、"Damn"、"bomb-diggity"（強い評価表現）
  - 一部ハードウェア言及（ノイズになりうる）: "16 gb ram", "8 gigs of ram", "gtx 660", "FPS"（パフォーマンス言及）

- グループBに特徴的な単語・表現（代表例）
  - 構造化・技術的／評価的語: "Gameplay : 9/10", "Graphic : 9/10", "Community", "achievement", "no autosave", "crashes", "glitches", "bugs"
  - メタ情報・テンプレート要素: "[h1]", "[b]", "[h2]" といったタグや表形式記述、"Playing status:", "Target Audience:"（レビュー構造化）
  - 事実列挙・箇条記述: "Events that took place:", "Optional Achievement(s):"（出来事や仕様列挙）
  - 説明的/客観的語彙: "military simulator", "visual novel", "target audience", "technical disaster"

- 文脈での使用とニュアンス
  - Aの語彙は「主観的評価・感情表現・物語の経緯の描写」に寄っている。例: "First hour ... Fourth hour ... The tears won't stop" → プレイ体験を時間経過と感情変化で語る叙述的レビュー。
  - Aには「物語への没入・感情反応」を示す語が目立つ（tears, cry, pray, love, Game of the year）。肯定的・否定的の両極で強い情動表現が多い。
  - Bは「メタ情報／機能的評価／バグ報告／数値化されたスコア」に偏り、客観的・構造化された記述が多い。例: "Gameplay : 9/10"、"no autosave, plus crashes"。
  - ハードウェアやFPSの言及は両群に混入するが、Aでは"performance as part of complaint"（例: "FPS on the PC are bad"）といった文脈で感情と結び付きやすい。Bでは仕様やバグの説明に使われることが多い。

- 感情的側面
  - A：強い感情語（喜び、涙、怒り、不満など）＋語調の強弱（"Oh shit..."、"Damn"）→ ストーリー性・体験語りが主。
  - B：冷静で機能的な記述が多く、感情表現は比較的抑制される（例外的に強い立場表明もあるが少数）。

2. 文脈・意味的ニュアンスの考察
- グループAの共通文脈的特徴
  - 物語中心性：登場人物・プロットの理解や感情的反応について言及する例が多い（"unable to understand without playing previous", "story's going somewhere", "tears won't stop"）。
  - 叙述的レビュー：時系列での感情の変化や体験を語る（"First hour...Fourth hour..."）。これは"story-related"なレビュー特有のパターン。
  - 評価の語彙が情緒化している：賛否だけでなく感情の強度（深い感動や失望）の表現が多い。
  - 一部に形式上のノイズ（HTMLタグや箇条書き）が混在するが、語彙分布としては「物語／感情」を中心に収束する。

- Bとの意味的・概念的差異
  - 粒度の違い：Aは「物語性／感情体験」という高次の意味概念に集中。Bは「機能／技術／構造的側面（操作性、バグ、スコア）」に集中している。
  - 表現様式の違い：Aは自由記述・叙述的／感情的、Bは構造化・箇条的・記号的（スコアやタグ）で、ここが概念差。
  - 抽象性：Aには暗示的・間接的表現（比喩や感想のこもった言葉）が多く、Bは明示的・説明的記述が多い。従って「story related characteristics」のような抽象ラベルはAと強く対応する。

- 抽象概念や間接表現の存在
  - Aは直接に"story"という語を使う例が複数あり、間接的表現（"the tears won't stop"＝感動の強さ）も豊富。物語的特徴の抽象化（没入、感動、プロット進行の記述）が顕著。
  - Bは具体的な欠点・機能名の列挙で、間接表現は少ない。したがって抽象概念抽出がしやすいのはA側。

3. 正解ラベルとの比較
- 正解ラベル："story related characteristics"
- LLM（gpt-4o-mini）が生成した対比因子：実験出力が空欄（あるいは評価側へ適切に渡されなかった）ため、BERTスコア・BLEUが0.0000になっていると推察
  - もしLLMが何も出力しなかった／出力が空文字列であれば、BLEU・BERTともに0になり得る（評価実装による）。
  - もう一つの可能性は「出力が参照とまったく語彙的・埋め込み的に一致しない」だが、BERTScoreが完全0は極めて異常で、実装エラー（参照/候補のミスマッチ、言語指定ミス、エンコーディング不一致等）の可能性が高い。

- 一致している部分と不一致部分
  - 一致想定部分：データの内容から見るに、正解ラベルはA群の主要特徴（物語性）を良く表している。理想的な対比因子は "story-related characteristics" や "narrative/emotional content" になるはずであり、これはAのサンプルと合致する。
  - 不一致の可能性：
    - LLMが長い叙述や例示的な説明（文）で出力してしまい、評価側が短い標準ラベルと比較したことでBLEU等で低評価になった（ただしBERTScoreはある程度の意味類似を検出するはず）。
    - LLMが出力を生成したが評価スクリプトが参照（正解ラベル）と候補の照合を正しく行えなかった（言語・正規化の不一致、余剰空白、全角半角、マルチバイト文字の問題など）。
    - LLMが別の観点（例：hardware performance, bugs）を差分として返してしまった場合、正解ラベルとは概念的にズレる。

- BERTスコアとBLEUスコアの乖離（今回では両方0）原因考察
  - 両方0は通常の生成品質の指標としては異常値であり、評価実装に問題がある可能性が高い。具体的には：
    - 参照ラベルと生成テキストのどちらかが空（空文字列やnull）である。
    - 評価に渡す文字エンコーディングの不一致（文字化けで全トークンが無効化）。
    - 生成言語が予期と異なる（例えば日本語参照に対し英語出力で、かつBERTScore実装が言語設定しておらず埋め込みがゼロ化される等）—ただし通常BERTScoreはゼロにはならない。
    - 評価用スクリプトのバグ（参照・候補のインデックスずれ、ファイル読み込み失敗）→ 全件0。
  - 実際の意味的乖離が原因であれば、BERTScoreはある程度の類似度を示すはずなので、実装／入出力パイプラインのチェックを優先すべき。

4. 実験設定の影響
- Few-shot（1-shot）の影響
  - 1-shotはモデルに出力スタイルのヒントを与えるが、タスク要求（「集合差分を短いラベルで要約」）を明確に誘導するには不十分なことがある。
  - 1-shotだと例示依存が高く、示した例の文体や長さに強く影響される。もし例が長文の説明だった場合、モデルは同様に長文で出力する傾向があり、短い名詞句ラベルが必要な本タスクには不向き。
  - したがって、"一意に特定する語彙"（名詞句）を出力させたいなら、複数（3-shot以上）の短いラベル例を与え、フォーマット（例："label: story-related"）を強制する方が安定する。

- グループサイズやデータセット特性の影響
  - 本実験はA/B各150件で明瞭な傾向が見えているが、ノイズ（HTMLタグ、テンプレート化されたスコア表記、ハードウェア言及）が混在している点が妨げになる。
  - group_sizeを小さくすると偶発的サンプルに引きずられやすくなり、ラベル安定性が落ちる。逆に大きくすると真の差分は出やすいが、多様性によりモデルが抽象化しにくい場合がある（特にFew-shotで誘導しているとき）。
  - 150件は中間的だが、"代表性のあるサブサンプル抽出"（高tf-idf差分語抽出など）を行わずにそのまま渡すと、モデルはサンプルの雑多さに惑わされる可能性がある。

- データの品質・前処理の影響
  - HTMLタグやスコア表現（[h1]等）がそのまま存在すると、モデルはそれらを重要な差分手がかりとみなすことがある（特にBにタグが多ければ、モデルは"structured reviews"を差分と解釈する可能性）。
  - スペルミス（例："Resdient"）や略称もノイズとなるが、頻出語は影響を及ぼすため正規化や小文字化が望ましい。

5. 改善の示唆（具体的手法と運用上の提案）
- 評価パイプラインの検証（最優先）
  - BERTScore/BLEUが0という異常値から、まず評価実装の入出力（参照と生成の存在確認、エンコーディング、言語設定、空文字チェック）を点検する。
  - 正解ラベルが単一フレーズである場合、評価は「短いフレーズ vs 生成文」を扱えるよう正規化（小文字化・句読点除去）や意味尺度（埋め込みコサイン類似）を併用する。

- プロンプト改善
  - 例示数を増やす（3-shot以上）し、全て「短い単語／名詞句」を出力例として示す（例: "story-related", "gameplay/controls", "technical issues"）。これによりモデルは出力フォーマットを学習しやすい。
  - 明確な指示を与える："Output a single short noun-phrase label (2–4 words) summarizing the primary difference between A and B." と明示する。
  - 前置処理として、グループごとの代表語（上位差分キーワード）を提示してからラベリングさせる（例：差分ワード上位10個を提示→その要約語を生成させる）。

- 前処理・証拠提示の強化
  - 自動で差分語を抽出（差分TF-IDF、log-odds ratio、chi-squareなど）して、その上位語をLLMに渡す。例：「A top words: story, tears, sequel, narrative; B top words: gameplay, crashes, achievement, bugs. Summarize the primary contrast in a short label.」
  - ノイズ除去：HTMLタグ・テンプレート除去、数値メタデータ（スコア）を正規化。ハードウェアやFPSのような混在ノイズはフィルタリングするか別チャンネルで扱う。

- モデル・デコーディング戦略
  - 温度低め・トップpを下げるなど出力の確定性を高める（短いlabelタスクではdeterministic出力が望ましい）。
  - 生成後に候補を複数出させ（n-best）、それらを埋め込みベースで正解ラベルに最も類似するものを選ぶ（ポストフィルタリング）。

- 評価指標の改善
  - 短いラベル評価にはBLEUは不適切。BLEURT、BARTScore、Sentence-BERT cosine similarity などの意味ベース指標を採用する。単一フレーズでもSBERTコサインが高ければ意味的に一致していると判断できる。
  - 人手評価（少数サンプルを用いた）を並行して行い、学習ベース指標との相関を確認する。

- 実験設計と群サイズの運用
  - group_size探索は続けるべきだが、各サイズで複数回サンプリングして分散を測る（再現性の確認）。単一ランでは偶然に左右される。
  - 代表サンプル抽出（クラスタリング→各クラスタから代表を）を併用すると、ノイズに引きずられない要約が得られやすい。

- 監査可能性の観点
  - LLMの出力に対して「差分キーワードの根拠」を同時に出力させる（例："Label: story-related characteristics. Evidence words: story, tears, sequel, narrative"). これにより説明の忠実性を検証しやすくなる。

まとめ（要点）
- データから見てグループAは明確に「物語／感情的体験」に関する表現が多く、正解ラベル "story related characteristics" と高い整合性がある。一方グループBは技術的・構造化された記述が多い。
- 実験でBERT/BLEUが0となったのは、モデル出力が空／評価パイプラインの不具合／言語・正規化のミスマッチ等、実装上の問題の影響が大きい可能性が高い。まずは評価パイプラインのデバッグを推奨する。
- モデル側の改善としては、Few-shotを3-shot以上に増やし「短い名詞句」を強制するプロンプト設計、差分キーワードの事前抽出・提示、ノイズ除去、出力のポストフィルタリング（埋め込み類似で選択）を組み合わせると実用性が高まる。
- 評価はBLEUではなく意味ベース指標（BLEURT / BARTScore / SBERTコサイン等）と人手評価の併用が必須。

必要であれば、
- 実際に差分TF-IDFを計算して「A優位の上位n語」を抽出する具体処理例（コマンドや擬似コード）、
- 改善プロンプトのテンプレート（3-shot例含む）、
- 評価パイプラインチェックリスト（検証項目）
を提示します。どれを優先して示すか教えてください。

## steam_group_sizeカテゴリ全体の考察

以下は「steam_group_size」カテゴリ（Steamレビューの群比較での対比因子自動生成実験群）に対する、与えられた個別実験考察ログ（20件）を踏まえたカテゴリ全体の総合考察です。問題点の要約、観察された共通パターン、設定要因の影響、今後の示唆を優先度付きで整理しました。

1. カテゴリ全体の傾向（共通パターン）
- 出力欠落／評価ゼロが支配的
  - 多くの実験で「LLM生成対比因子」が実質的に空で、BERT/BLEU が共に 0.0 になっている。これは単なる性能低下ではなく「生成または評価パイプラインの欠陥（出力保存ミス、エンコード/前処理の不整合、タイムアウト／トランケーション等）」を強く示唆する。
- データ側の確度は概ね高い（ラベル妥当性）
  - 代表サンプル観察では、各カテゴリ（gameplay/visual/story/audio）に対応する語彙が群のどちらかに確かに偏在しているケースが多い（例：gameplay→controls/combat、visual→graphics/artstyle、story→narrative/characters、audio→soundtrack/voice）。つまり「正解ラベル自体は妥当」であり、問題はLLMの出力取得・整合化にあることが多い。
- ノイズ・トピック混在が顕著
  - Steamレビューは長文・罵倒・編集タグ（[h1],[b]等）や固有名詞、複数トピック（アート/音楽/操作/価格/サーバ）が混在するため、集合レベルの差分は「単一軸」ではなく複合的になりがち。これがラベル化の難しさを増している。

2. パフォーマンスの特徴（スコア分布と傾向）
- スコア分布
  - ログ上は多くが BERT/BLEU = 0.0。出力が存在すればBLEUは語彙一致により低めになりがち、BERTScore は通常一定の非ゼロ値を示すはずだが今回はゼロが多発しているため「評価不能（出力欠落/処理ミス）」が主因。
- 高スコア実験の共通特徴（観察からの仮説）
  - （観察が限られるが）高評価が期待できる条件は、（1）群内で特定トピック語が高頻度に偏在、（2）プロンプトが短ラベル出力を明確に指示、（3）前処理でノイズを除去し差分語を与えた、という組合せ。
- 低スコア（ゼロ）実験の特徴
  - ほとんど全ての実験に共通：Few-shot=1 のまま生テキストを大量投入、出力の生ログ未保存／評価前処理ミス、BLEUのみ依存等。これらが低スコア（あるいは評価不能）を招いている。

3. 設定パラメータの影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力形式（短いラベル vs 長文説明）」の誘導力が弱く、不安定。タスク（集合差分→短い概念ラベル）では 3–5 shot の方が安定性が上がるというログ中の示唆が一貫している。
- グループサイズ（group_size）
  - 小（50）: ノイズや偶発的サンプルに影響されやすく、代表性が不安定。  
  - 中（100–150）: 差分シグナルが比較的安定して抽出しやすいバランス帯。多くの改善案で推奨されているのはこのレンジを基準に試行すること。  
  - 大（200–300）: 多様性が増えシグナルが希薄化する一方で、十分な前処理（クラスタリング・代表抽出）を行えば安定化も可能。だが生データをそのままプロンプトに入れるとトークン制限や情報過多で失敗しやすい。
- モデル（gpt-4o-mini 等）
  - gpt-4o-mini 自体は汎用性が高いが、長文集合比較や厳密なフォーマット出力（短い名詞句ラベル）に対してはプロンプト工夫と前処理が不可欠。モデル変更（より大きなモデル）で改善は見込めるが、まずはパイプライン／プロンプト改善が先決。
- 評価指標の選択
  - BLEU は短いラベル比較に不向き（語彙揺れに敏感）。BERTScoreは意味的に優れるが今回の0多発はパイプライン問題を示す。BLEURT、BARTScore、埋め込みコサイン等の導入と、多参照/同義語辞書の準備が推奨される。

4. 洞察と示唆（主要知見と今後の研究方向）
A. 主要知見（要点）
- 根本問題は「運用（パイプライン）＋設計（プロンプト／前処理／評価）」にあることが最も多くの実験で示唆される。データ自体はラベルに対応するシグナルを持つことが多いが、LLM出力の取得/正規化/評価のいずれかで失敗している。
- 単にモデルを変えるより先に、（1）出力ログ保存、（2）評価パイプラインの前処理整合、（3）出力フォーマット強制、（4）差分語抽出などの前処理ワークフローを整備することが効果的。
- group_size の調整だけでなく「群内部のトピック収束度（視覚語率、音語率など）」を計測し、最適なサンプル数・代表化方法を決めるべき。

B. 優先度付き改善提案（実務的）
1) 最優先（必ず行う）
  - raw LLM 出力（API応答）を全て保存し、出力が空かどうか、トークン上限で切れていないかなどを検証する。出力が空なら直ちにAPIログ／エラー原因を調査。
  - 評価パイプラインの入出力前処理を固定：正解ラベル・生成ラベルともに同一の正規化（小文字化・trim・Unicode正規化・HTML除去）を行い、評価を再実行する。
2) 高効果（次に実施）
  - Prompt engineering：Few-shot を 3–5 ショットに増やし、出力を「1–3語の英語名詞句のみ (no explanation)」に厳格化。成功例 / 失敗例（bad example）を混ぜて示す。
  - 前処理パイプライン導入：TF-IDF / log-odds で A/B の差分キーワード上位を抽出し、そのリストを LLM に与えてラベル命名させる（二段階化）。またはクラスタリングで代表文を抽出して提示する。
  - 評価指標改善：BLEU廃止→BLEURT/BARTScore/embedding cosine を導入し、同義語マップ（許容ラベル群）を作る。
3) 中長期（実験設計 / 研究）
  - group_size 感度実験：50/100/150/200/300 の各サイズで複数ラン（シード）を実行し、ラベル出力の安定度（同一ラベル再現率、embedding類似度分散）を評価して最適サイズを選定。
  - ハイブリッドワークフロー：統計的手法で候補語を自動抽出 → LLM が短ラベルに正規化 → 小規模人手で承認する運用（半自動ラベリング）を構築。
  - 出力の透明化：LLMにラベルと同時に「支持する代表例/キーワード」を出力させ、説明可能性（explainability）を担保する。

C. 研究的示唆
- 集合差分ラベリング（group-level concept discovery）は「ノイズの多いUGC（Steam等）」では直接LLMに大量テキストを渡すだけでは不安定。統計的差分解析（log-oddsなど）とLLMの組合せ（証拠→命名）が有望である。
- 評価手法研究：短ラベル評価に適する自動指標の検証（BLEURT等）と人手評価の少量混入によるキャリブレーションが必要。
- 出力欠落の発生源（API側タイムアウト・filtering・プロンプト長超過など）を定量的にログし、再現性の高い障害モデルを作ると将来的な改善に寄与する。

5. 最後に：短期チェックリスト（実装担当向け）
- 保存ログの確認（raw responses + HTTP status）→ 出力が無ければAPIログを精査。
- 評価パイプラインの単体テスト（既知のref + hyp でBERT/BLEUが期待値を返すか）。
- 簡素な sanity-check プロンプト（3対3の代表テキストを入力、temperature 0、出力1語）で動作確認。
- 差分語の自動抽出を1回実行（各群 top-20）して、LLMに与えてラベル化する方式を試す（短期実験）。

———

要約：現状の失敗は主に「出力欠落／評価パイプライン不備」＋「プロンプト設計と前処理不足」に起因する。データ自体はラベルと整合するシグナルを含む場合が多い（各aspectごとに該当語彙あり）。まずはログ確認と前処理＋プロンプト改善（3–5 shot・短ラベル強制）、差分語抽出を組み合わせた二段階ワークフローを実装し、その上でgroup_size感度テストと評価指標の改善（BLEURT等）を進めることを強く推奨します。必要であれば、（A）代表サンプルからのTF-IDF/log-odds抽出結果、（B）3–5-shotプロンプトテンプレート、（C）評価パイプラインチェックリストの具体案を作成します。どれを先に出しますか？

