# 実験考察レポート: goemotions_disapproval_1_4o-mini_word

## 個別実験の詳細考察

以下は提示いただいた実験（グループA/B各100件、Few‑shot=1、GPT‑4o‑mini使用）結果についての詳細な考察です。特に「単語レベル」の差分分析を中心に、意味・文脈、評価スコアの異常、実験設定の影響、および改善策を具体的に示します。

要点（結論の概観）
- グループAは「否定・非難・軽蔑・批判」を示す語彙・表現が濃厚であり、正解ラベル「disapproval related characteristics」と内容的に高い整合性を持つ。
- グループBは「肯定・感謝・中立的共感」的語彙が多く、対比は明瞭。
- 出力（LLM生成対比因子）が実際に取得されていない（もしくは評価パイプラインで欠落・ミスマッチしている）可能性が高く、BERTスコア/BLEUともに0となっている点は評価側の運用不備かフォーマット不整合を疑う必要がある。
- Few‑shot=1・プロンプト設計・データノイズ・評価指標選択が結果に大きく影響する。改善策としては（1）単語頻度/差分統計を先に用いて候補語を抽出、（2）LLMに対して「出力形式」を厳格に指定、（3）評価指標の見直し（BLEURT/BARTScore/埋め込み類似度 + 人手評価）を推奨する。

以下、指定の観点ごとに詳細に分析します。

1) 単語レベルでの特徴分析
- 方法論（想定）：まず A/B の代表文から特徴語を抽出（頻度、log‑odds、差分TF‑IDF、共起）し、文脈例を確認して語の感情・機能（批判・否定・質問・依頼など）を注釈する。

- A に特徴的な語・表現（代表例と文脈）
  - 否定/否認の助動詞・否定語：can't, don't, not, never（例: "I can't read Korean.", "We don't."）  
    文脈：相手の行為や存在を否定する、拒絶・断絶の機能。
  - 否定評価・軽蔑語：shit, hate, doubt, bad, problem（例: "It’s shit.", "I hate it", "the idea is bad dude. that's the problem."）  
    感情的側面：強いネガティブ感情（怒り・軽蔑）。直接的評価語が多い点が特徴。
  - 批判的構文・攻撃的断定："You can't do anything about it now, and nobody cares.", "I doubt [NAME] could do make up as well as a drag queen."  
    文脈：主語を攻撃・見下す、断定的な否認（対人攻撃性）。
  - 攻撃的・衝動的表現（過激表現）："Bomb on your chest Do not try this at home"（脅しやショック表現や黒いユーモア）  
    文脈：挑発的／感情喚起を狙った表現。
  - 帰属・責任転嫁："Its allways the kids fault.", "they don't know how markets work"  
    文脈：他者（や集団）を責める語用。
  - 皮肉・嘲笑・煽り："I am lazy so i won't write it down sry", "That's not as much fun."  
    文脈：皮肉や軽視を示す修辞が頻出。

- B に特徴的な語・表現（代表例と文脈）
  - 感謝・肯定："Thanks", "I like this idea.", "I love it", "Oh thanks a lot! :)"  
    文脈：肯定的評価、好意的リアクション。
  - 慰め・共感："Sorry for your loss mate", "At least she had a great experience", "That’s also cute"  
    文脈：共感・慰め・賞賛。
  - 中立的/事実述語："Interesting.", "Which could mean new faces"  
    文脈：観察や推測の述語。
  - カジュアル・フレンドリーな語調："Haha cheers for the kind words mate", "I was in Buckhead waiting to celebrate"  
    文脈：ポジティブな会話参加、社交的語彙。

- 単語のニュアンス（感情的側面）
  - Aは「否定・攻撃・皮肉・強いネガティブ情動」を示すトークンの密度が高く、評定語（shit/hate）と否定構文（don't/can't）がセットで現れている点が特に差異を生む。
  - Bは「肯定・感謝・共感・中立」が中心で、感情はポジティブ〜中立に偏る。皮肉や攻撃性は稀。

- 定量的に確認すべき指標（提案）
  - 各語の差分log‑odds（informative Dirichlet prior）でA固有語を抽出。
  - Sentimentスコア（VADERなど）を各文に付与して群の平均比較（Aは平均が顕著に低くなるはず）。
  - ネガティブ情動語リスト（hate, shit, crap, problem, doubt など）の出現頻度比。

2) 文脈・意味的ニュアンスの考察
- グループAの文脈的特徴（総括）
  - 対人攻撃的／否定的語用が多く、会話における「反論」「侮蔑」「嘲笑」「批判」の機能を持つ発話が優勢。  
  - 言語表現の粒度は直接的（explicit）な評価語に富み、抽象化すると「不賛成／不快／批判」という概念群にまとまる。  
  - ユーモアや皮肉、暴言（過激表現）も含み、破壊的／否定的行動を示唆する発話が混在する。

- グループBとの意味的差異
  - A = ネガティブ評価/敵意/批判、B = ポジティブ/共感/礼儀的表現。両群は情動の極性（polarity）が対照的で、結果として「disapproval（否認・批判）」というラベルは概念的に妥当。
  - Bは会話的な応答（感謝、賞賛、慰め）を含み、対話的な協調性（conversational alignment）が高い。一方Aは対立的発話が多く、対話破壊的な要素が目立つ。

- 抽象化・間接表現の有無
  - Aでは直接的な否定語が多く「間接的な示唆・婉曲表現」よりも明示的評価が主。間接表現（皮肉など）は存在するが、多くは直接的なネガティブ語で表現されるため判別は容易。
  - Bは褒め言葉や感謝などで抽象化が少なく、直接的肯定が主。

3) 正解ラベルとの比較
- 正解ラベル: "disapproval related characteristics"（否定・不賛成に関する特徴）
- LLM生成対比因子: 実験報告には"LLM生成対比因子"のテキストが欠落している（空欄）となっているため、直接的な一致度評価は困難。

- 想定評価（もしLLMが正しく出力していれば）
  - 期待される出力例（高一致）: "disapproval / negative sentiment / critical language / dismissive tone" のような短いラベルまたは名詞句。
  - 一致している部分：Aの語彙・文脈は正解ラベルを強く支持しており、適切に要約すれば高いBERTScoreやBLEU（ただしBLEUは短文に弱い）を得られるはず。

- 実際のスコア（BERT=0, BLEU=0）についての考察
  - 技術的可能性A（最も可能性が高い）：LLM出力が何らかの理由でキャプチャされておらず、評価器に対する入力が空文字列／NULLになっている。評価器が空文字列→スコア0を返した。
  - 技術的可能性B：LLMは日本語や別言語、あるいは非常に長い説明文（人間評価用の段落）を返し、評価スクリプト側で参照ラベルと照合できない形式（例えば参照が短い名詞句で、生成が長文で正規化されていない）だったためスコアが0になった（ただしBERTScoreが完全に0になるのは珍しい）。
  - 技術的可能性C：評価パイプラインのバグ（パス間違い、エンコーディングエラー、生成テキストのトリミング）により実際のモデル出力が評価に渡されていない。
  - さらに、BLEUは短い単語列評価に不適切（1語〜短フレーズではBLEUほぼ0）であるため、BLEU=0自体はありうる。しかし BERTScore=0 は通常発生しにくく、上記の「出力欠落／パイプラインエラー」を示唆する。

4) 実験設定の影響
- Few‑shot設定（1-shot）の影響
  - 1-shotは有効だが安定性に欠く。例示が1つだと「どの粒度（名詞句 vs 説明文）で返すべきか」の誘導が弱く、モデルは多様な応答（長文要約、箇条書き、単語）を返しやすい。  
  - 特に「対比因子ラベル」は短い名詞句を期待するため、出力形式の強制（"Output only a single short label in English, e.g., 'disapproval'."）が必要。

- グループサイズ・データ特性の影響
  - group_size=100は十分なサンプル量で「集合差分」を示すには適切。だがデータが雑多（投稿のジャンル混合、個人名トークン、ノイズ、皮肉やスラング）だとLLMの要約対象がブレる。  
  - ノイズ（例: 暴言、リンク、絵文字、引用）を事前除去/正規化しないと、LLMはフォーカスを失い生成が曖昧になる可能性がある。  
  - データセットが unknown とされていること自体が問題（ドメインごとの語彙分布が評価に影響）。多様なドメインを混ぜると、代表的差分が薄まることがある。

5) 改善の示唆（具体的手順）
- デバッグ（最優先）
  1. 実際のLLM出力をログ保存しているか確認する（stdout/返り値、APIレスポンスJSON）。生成が空でないかをまずチェック。  
  2. 評価パイプラインで生成テキストが正しく読み込まれているか（ファイルパス、文字コード、改行やトリミングの影響）を確認。  
  3. 生成→評価までの中間ファイルを手で比較して既知の単語（"disapproval"等）を投げてみてスコアが変化するか確認（サニティチェック）。

- プロンプト改良（出力の安定化）
  1. 出力形式を厳格に固定：例 "Return a single short English noun or noun phrase (1–3 words). Do not add explanation." という命令を最初に与える。  
  2. Few‑shotを増やす（3‑shot）で、例は「群Aのサンプル→短いラベル」を示す。例は多様な語彙でラベルが常に1語あるいは短い名詞句であることを徹底する。  
  3. Temp=0（決定的応答）にする、top_p低めにして出力のばらつきを抑える。  
  4. 生成前に「重要語（トップNの差分単語）」を提示して、モデルにラベル生成のヒントを与える（例："Important words in A: shit, hate, don't, problem — generate label."）。

- 前処理と特徴抽出（自動候補生成の補助）
  1. A/B それぞれに対して差分log‑odds、chi2、TF‑IDF差分を計算し、上位語（n‑grams含む）を列挙する。これをLLMに渡してラベル生成を促す（"From these keywords, produce a concise label"）。  
  2. Sentiment解析（VADER/BERT‑based）で群ごとの平均極性を出し、ラベルへ反映させる（例："negative sentiment & insulting language"）。  
  3. 共起クラスタリングで「テーマ」ごとのサブ群を抽出し、各サブ群に対し個別にラベルを作らせる（概念多様性に対応）。

- 評価指標の見直し
  1. BLEUは短いラベル評価に不適切。代替として BLEURT、BARTScore、または意味埋め込みのコサイン類似（Sentence‑Transformers）やMoverScore を使用。  
  2. BERTScore は短文にもある程度対応するが、完全な自動評価に頼らずサンプルごとの人手評価（ラベル妥当性、冗長性）を少数でも実施する。  
  3. 出力ラベルが単語レベルの場合、embedding cosine と閾値で一致判定（>=0.75など）を行う運用が現実的。

- 実験設計の改善
  1. Few‑shot のショット数と例の多様性をスイープ（0/1/3/5）し、安定性を評価。  
  2. group_size の感度分析（既に計画のSteamサブ実験）を確実に実行し、大きい群でのノイズ耐性を確認。  
  3. 複数モデル（gpt‑4o‑mini 以外に GPT‑4/other）で比較し、モデル依存性を測る。  
  4. 出力フォーマットの強制（JSONスキーマで label_short, label_long）を採用し、下流評価を安定化。

- 実用的フロー（提案）
  1. 前処理：ノイズ除去（名前置換、URL除去）、トークン正規化。  
  2. 解析：差分統計でトップ20単語抽出。  
  3. LLM入力：トップ単語 + 3ショット例 + 出力形式強制。  
  4. 生成：ラベル（短）、説明（オプション）。  
  5. 評価：埋め込み類似度 + BLEURT + 少数の人手評価。

補足：具体的なキーワード／差分例（与えられた代表サンプルから抽出）
- A上位候補語（手動抜粋・頻出予測）："shit", "hate", "doubt", "problem", "don't", "can't", "not", "leftist", "terrible", "bomb", "angry"  
  → これらは「不賛成／否定／批判」を直接示す語であり、ラベル候補は "disapproval", "negative sentiment / criticism", "dismissive / insulting language" など。
- B上位候補語："thanks", "love", "like", "sorry", "great experience", "cheers", "interesting"  
  → ラベル候補は "positive / appreciative comments", "supportive tone"。

最後に——今回の最も重要な所見
- データ本体（A群の語彙分布）は正解ラベルと整合しており、理論的にはLLMに短い対比因子ラベルを生成させるのは十分可能である。したがって、今回の「スコアが0」という致命的な結果は主に「出力欠落／フォーマットミスマッチ／評価パイプラインの不整合」に起因する可能性が極めて高い。まずはログと中間ファイルを確認して出力が正しく取得・評価に渡されているかを検証することを強く推奨します。その上で、上に示したプロンプト改善・前処理・評価改善策を順次実施すれば、安定して正解ラベル（あるいは高類似のラベル）を得られる見込みが高いです。

## メイン実験全体の考察

以下は「メイン実験（group_size=100 統一）／各データセットに対する対比因子ラベリング実験群（gpt-4o-mini, few-shot=1 を基本）」に関する、個別実験考察ログ（実験1〜37）を総合して導いたカテゴリ全体の分析・洞察です。実務的な原因推定と優先的改善案を含めてまとめます。

要約（結論）
- 最も顕著な事象：多数の個別実験で「LLMの出力が得られていない／評価に回せる形式で取得できていない」ため BERTScore・BLEU が 0.0000 になっている。すなわち「モデルが出力しなかった／出力が消失した／評価パイプラインが壊れている」ことが主要因で、モデル能力の評価は事実上行えていないケースが多い。
- データ側では A 群が典型的に「一貫したドメイン語彙（例：food/service/battery/screen/emotions別語彙）」を示すことが多く、理論上は短い名詞句ラベルで要約可能であるにもかかわらず、実験設定（few-shot=1・入出力前処理・評価方式）の不備で失敗が多発している。

以下、指定観点ごとに整理します。

1. カテゴリ全体の傾向（個別実験から抽出された共通パターン）
共通パターン
- 出力欠落問題が圧倒的に多い
  - 多数の実験で「LLM生成対比因子」が空欄、BERT/BLEU が 0。ログ上は出力欠落（APIエラー／レスポンス保存漏れ／評価入力が空）または評価前処理での消失が最有力。
- A群はドメインに固有の語彙がまとまる
  - semeval/restaurants/laptop/amazon/goemotions 各セットで、A群は明瞭なトピック語（例：food→fresh/menu/taste、service→friendly/attentive、battery→charge/last、screen→resolution/froze、emotion→sad/joy/pride等）に収束している。つまり「集合差分」は存在するケースが多い。
- B群はより分散／雑多／ノイズが多い
  - B群は混在テーマ・技術語・雑談などが多く、対比が単純ではない場合もある（Aの信号を薄める）。
- $T$ や [NAME] のようなプレースホルダ・特殊トークンが入力に多く存在
  - マスク・プレースホルダが意味手がかりを隠すためモデルが迷う、あるいはプレースホルダが雑音となるケースが複数確認された。
- 生成は「短いラベル（名詞句）」の想定なのに、プロンプトやショットが説明文や長文を誘導してしまうケースが目立つ

データセット／アスペクト差
- 感情系（goemotions）
  - A群は明確にその感情カテゴリの語彙（例えば「admiration→amazing/wonderful」「anger→fuck/idiot」）を含む場合が多く、理想的には非常にラベル化しやすい。
- SemEval / Amazon / Laptop 等（属性系）
  - A群は特定アスペクト（food quality, service, battery life, price, delivery 等）を語る語彙で凝縮しており、本来は差分抽出＋命名が実務的に可能。
- 実際の失敗の分布はアスペクト依存より「実験運用（プロンプト・評価・ログ）」に強く依存している

2. パフォーマンスの特徴
スコア分布・傾向
- 並列して報告された多くの実験で BERT/BLEU が 0.0000（完全ゼロ）：実際には「性能低下」より「実験的欠損（出力や評価入力の欠如）」を示す
- 正常に出力が得られていたサブケースがほとんど報告されておらず、スコアの有意な正負分布を評価するデータが不足

高スコアになり得る条件（ログや人手観察からの逆推定）
- A群が語彙的に凝縮（同一アスペクト語彙が高頻度）→ラベリング容易
- プロンプトで出力形式が明示され、few-shot で短い名詞句例を与えている
- 前処理で代表語（top-n tokens by log-odds/TF-IDF）を抽出して入力に含めている（つまり「二段階ワークフロー」）
- 出力検査・フォーマット検証（非空チェック）を行っている

低スコア（今回の大多数）に共通する特徴
- 出力が空、あるいは出力フォーマットが評価用フォーマットと異なる（例：説明文 vs 短いラベル）
- BLEU を単独で評価に使っている（短いラベル評価に BLEU は不適）
- 評価パイプラインで文字コード・トークナイザ・改行などが整合していない

3. 設定パラメータの影響
Few-shot（ショット数）
- 1-shot は不安定要因
  - 多数の事例で「1-shot が出力形式の安定を欠く」→モデルが説明的応答や空応答を返すリスクが高いと示唆
  - 推奨：3～5ショットで例の質（短い名詞句によるラベル例を複数）を確保することで出力安定化

group_size（100）
- group_size=100 は概念抽出には十分
  - ただし「群の純度（A にアスペクトが集中しているか）」が重要。A 内にノイズ（異トピック）が多いと対比が弱まる
  - グループサイズの感度解析は有益（50/100/150/200/300）→既に計画されている通りだが、代表抽出法（クラスタ代表 or top tokens）併用を推奨

モデル（gpt-4o-mini 等）
- gpt-4o-mini は汎用性は高いが、
  - 出力の「短いラベル」を安定的に生成させるにはプロンプトとショット設計が重要
  - 一部の失敗（空出力）はモデルより運用（API/ログ/評価）に起因している可能性が高い
- モデル選択の影響は存在するが、まずはプロンプト・前処理・評価周りを安定化させるのが優先

その他実行パラメータ
- temperature（多様性）→低め(0–0.2)で決定的出力を狙うべき（短いラベル生成は多様性不要）
- max_tokens／stop_sequences：短ラベルを切られないように設定し、長文を強制しない

4. 洞察と示唆（研究・実務への具体的提案）
主要知見（実験群全体から）
1. 実験失敗の主要原因は「運用／パイプライン（出力取得・評価）の欠陥」であり、モデルそのものの性能評価が十分に回収されていないケースが多い。
2. A群は多くの場合、明確な集合的語彙手がかり（discriminative tokens）が存在するため、原理的には自動ラベリングは実現可能である。
3. 単語ベースのみでの判定は限界がある：句・共起・文脈（"at least", "I never knew" 等）を踏まえた処理が必要。
4. BLEU は短いラベル評価に不向き。意味的評価（BERTScore, BLEURT, BARTScore, embedding cosine）が必要。

優先的改善（短期：実験フローを直す）
- A. ログ＆評価健全性の確保（絶対必須）
  1. LLM の raw response（text）を全て保存し、出力の有無を自動チェックして「空なら再試行/アラート」。
  2. 評価前に参照と生成の正規化（UTF-8, strip, lower, NFKC）を行い空入力を弾く。
- B. 出力形式の強制化（プロンプト）
  1. 出力は「1–3語の英語名詞句（short label）」のみと指示し、few-shot で 3–5 例を与える（例は必ず正しい形式）。
  2. 生成候補を n-best で取り、後段で埋め込み類似度で再ランキングする。
- C. 前処理→LLM の二段構成
  1. 統計的差分（log-odds / TF-IDF / chi-square）で上位 discriminative tokens を抽出（A vs B）。
  2. それらトークン＋代表サンプルを LLM に与え、「この語群を要約して短いラベルを1つ出せ」とする。
  - これによりノイズ低減と安定性向上が期待できる。
- D. 評価改善
  1. BLEURT / BARTScore / SBERT cosine を導入（BLEU廃止または補助）。
  2. 多参照（paraphrase set）と人手評価（少量）を用意し、自動指標のキャリブレーションを行う。

中期的研究提案
- 1. アブレーション実験：few-shot数（0/1/3/5）、group_size（50/100/150/200）、前処理有無の 3 因子実験で安定性曲線を描く。
- 2. ハイブリッド法の評価：統計的差分→LLM命名 vs LLM単体（対比）で精度・再現性を比較。
- 3. 出力の根拠提示（explainable output）：LLM にラベルと「根拠トークン3つ」の同時出力を求め、信頼性指標を作る（自動スコア+人手検査で検証）。

運用上の注意点（倫理・安全）
- データに攻撃的／差別語が含まれる場合、モデル応答のフィルタや出力抑制が働く可能性がある（その結果出力が空になるケースあり）。フィルタの有無／ログは必ずチェックすること。
- ラベル化で人を動かす用途（自動モデレーション等）では検出誤差・誤学習に注意。人手介在を残すハイブリッド運用を推奨。

最後に：今すぐ着手すべき実務タスクリスト
1. raw model response の保存／空応答検出を実装（必須）。  
2. 評価前処理の自動チェック（参照・生成が非空であること）を実装。  
3. few-shot を 3-shot に増やしたテンプレートを作り、代表的小実験で安定化（短いラベルを返すかを検証）。  
4. 差分トークン抽出（log-odds）スクリプトを作成して A/B の上位語を確認 → LLM に提示するワークフローを試す。  
5. BLEURT / BARTScore / SBERT-cosine を導入し、少数人手ラベルで指標調整を行う。

もし望まれるなら、次のいずれかをすぐ用意します：
- A. 差分トークン抽出（A/B 各100件）を行う Python スクリプトと上位語リスト（出力形式 csv）  
- B. 改良版 few-shot プロンプト（英語／日本語）テンプレート（3-shot）＋出力検証ロジック（空応答検出）  
- C. 評価パイプライン修正案（BLEURT 並列導入手順・SBERT 電卓）

どれを優先して作成しましょうか？

