# 実験考察レポート: steam_audio_group_size_50_1_4o-mini_word

## 個別実験の詳細考察

以下は与えられた実験結果（Steam レビューのグループ A/B と正解ラベル "audio related characteristics"、LLM 出力が事実上失敗し BERT/BLEU が 0 になっている状況）に対する詳細な考察です。特に単語レベルの差異と文脈的ニュアンスを重視して解析し、失敗原因の推定と改善案を具体的に示します。

1) 単語レベルでの特徴分析
- 手法（前提）
  - 提示サンプル（各群 20 件ずつ代表）から頻出語・目立つキーワードを抽出し、出現文脈を確認しました（ストップワードは除外）。なお全 50 件の完全リストは無いので代表例に基づく推定です。

- グループ A（発火群）に特徴的な単語・表現（代表例と文脈）
  - overhyped, let down, boring, unresponsive, disappointed
    - 文脈: 「過度に持ち上げられていた」「期待外れ」など、総括的な不満を短く示すレビュー冒頭で用いられる。否定的評価の凝縮表現。
  - controls, platforming, slide jumping, performance, launcher, microstutter
    - 文脈: 操作性・入力応答・パフォーマンス（フレーム落ち、ランチャーの問題）に関する具体的不満。「controls are mostly on point」など操作に言及する例もある。
  - art style, pixel artstyle, cutscenes visual
    - 文脈: 視覚表現（ドット絵、カットシーン）が肯定的に言及される箇所。これはビジュアル面の評価語彙。
  - story, endearing main cast, world building, satisfying finale
    - 文脈: 物語性やキャラクターに関する肯定的記述。
  - modding, modding heaven
    - 文脈: 拡張性・MOD に関する肯定的な称賛語。
  - DO NOT BUY, Seriously, I hate to do this
    - 文脈: 強い否定の呼びかけや感情的な断定語。

  感情的側面: A は否定的・感情的語彙（overhyped, DO NOT BUY）と具体的問題指摘（controls, performance）が混在。視覚・物語に関する肯定語もあるが、総じて主観的で感情彩度が高い。

- グループ B（非発火群）に特徴的な単語・表現（代表例と文脈）
  - Soundtrack, soundtrack is 10/10, Voice actors, audio, sound
    - 文脈: 明確に「音」に言及しており、肯定的評価（最高評価）とセットで出現。これが正解ラベル「audio related characteristics」を直接支持する重要語。
  - Pros:, Cons:, TL;DR, Overview, [h1]
    - 文脈: 構造化されたレビュー（見出し・箇条）や要約を取るタイプ。レビューのフォーマット性が高い。
  - movement/mechanics, easy to pickup, gameplay, gameplay: 9/10, graphic/gfx, beautiful, atmospheric, stunning gfx
    - 文脈: ゲームプレイ・操作性・グラフィックに関する具体評価。肯定的で体系的な評価語彙。
  - bugs, critical bugs, early access, devs worked hard
    - 文脈: バグや開発者対応に関する事実記述。問題点の指摘はあるが語り口は説明的。
  - community, playing with friends, competitiveness & balance
    - 文脈: マルチ要素・コミュニティに触れる行。

  感情的側面: B は A に比べて説明的・評価的（Pros/Cons のような構造）で、音響関連語（soundtrack, voice actors）が明瞭に存在する。全体としてトピックが安定している印象。

- 単語頻度差から見えること（総括）
  - audio 関連語（soundtrack, voice, voice actors, audio, sfx 等）は明確にグループ B に集中している。
  - group A は「不満・操作・パフォーマンス・ビジュアル・物語」といった語彙群に偏っており、audio 語彙はほとんど見当たらない。
  - したがって、語彙ベースでは「audio related characteristics」はグループ B の特徴であり、もしラベルが A に対応しているなら群の入れ替え・選択ミスが発生している可能性が高い。

2) 文脈・意味的ニュアンスの考察
- グループ A の文脈的特徴
  - 感情的・主観的な表現が頻出（「overhyped」「DO NOT BUY」「I hate to do this」など）。個人の感情や極端な評価に重心がある。
  - 問題指摘が個別技術面（controls, performance, launcher）に偏る。つまり「何が駄目か」を列挙するタイプのレビューが多い。
  - 視覚や物語（art style, story）を肯定する文もあり、トピックが散在（操作性・性能・グラフィック・ストーリーが混在）。

- グループ B の文脈的特徴
  - 構造化（Pros/Cons、見出し）された記述が多く、対称的な評価（長所・短所）を整理している。結果、特定の側面（音響、グラフィック、ゲームプレイ）に言及しやすい。
  - 「soundtrack is 10/10」「Voice actors are high quality」のように音響面を直接かつ肯定的に述べる文が存在し、集合全体として音に関する評価の密度が高い。
  - 文章のトーンは A に比べ客観的・説明的。

- 意味的・概念的差異
  - A は「感情/体験の強調＋技術的不満」、B は「機能別の評価（音・映像・操作）を整理して提示する傾向」。つまり A は「主観的な総評（好き/嫌い）＋不満の具体的指摘」、B は「側面別の評価（音は良い、グラフィックは良いなど）」に向いている。
  - 抽象概念・間接表現：A は暗示的・間接的（「this remains to be one of the best games」「it taught me to hate them all」など感情移入や風刺的表現）を含む。一方 B はより直接的に評価対象（soundtrack, voice actors）を名指しするため、対比因子の抽出が容易。

3) 正解ラベルとの比較
- 与えられた正解ラベル: "audio related characteristics"
  - これは「音響に関する特徴（soundtrack/voice/audio/etc.）」を要約する語句。

- LLM が生成した対比因子: （提示では空白／無出力／失敗）
  - 出力がない、あるいは期待する語彙（audio 等）と一致しないため、評価指標が BERT=0.0000、BLEU=0.0000 となっています。

- 一致している部分と不一致の部分
  - 一致点: 実際には LLM 出力が無いため一致点はない。だが入力データを観察すると、グループ B が正解ラベルに一致する内部証拠（"Soundtrack is 10/10", "Voice actors are high quality"）を持つため、正解ラベル自体はデータの一群に十分根拠あり。
  - 不一致点: グループ A の方が「発火群」として与えられている点（もしラベルが A に対応するなら）で根本的な不一致。要するに「発火群に 'audio' の証拠が少ない」こと、そして LLM がそれを拾えなかった点が不一致を生んでいます。

- BERTScore と BLEU の極端な低さについて（原因考察）
  - 出力が空、もしくは評価用の比較文字列（gold）とモデル出力が完全に一致しない・語彙的重なりがゼロであると、BLEU は 0 に近くなるのは説明可能。ただし BERTScore が 0.0 は異常で、通常は語彙的に無関係でも小さな埋め込み類似度が出るはずです。考えられる原因は以下：
    1. モデル出力が空文字列（評価器は空を埋め込みに変換できず 0 を返した）。
    2. 評価実装のバグ（比較対象のテキストエンコーダが異常、言語タグミス、あるいはトークナイザでフィルタされる）。
    3. gold と predicted の言語が異なる（例：gold 英語、predicted 日本語）で評価器が対応していない設定。
    4. 文字エンコーディングの問題（非表示文字のみ、改行だけ等）。
  - したがって 0.0 は LLM の内容的失敗に加え、評価パイプラインの確認が必要であることを示唆します。

4) 実験設定の影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力スタイルの誘導」にはある程度効くが、ラベリングという曖昧で集合差分を要約するタスクではショット数が不足すると安定性が低い。
  - 具体的影響:
    - 例示が出力のフォーマット（短い語句 vs 長い説明）を決める力はあるが、示した一例が代表的でないと LLM はその一例を過度に一般化して誤った着地をする危険がある（例：一例が「visual-related」なら出力が視覚寄りになる）。
    - 1-shot だと「まずキーワード抽出→要約」という分解プロセスを LLM に期待するよりも、直接的にラベルを挙げることを強制されるため、誤出力や空出力になりやすい。
  - 対策: 3–5 shot に増やして「キーワード抽出→要約→1語ラベル」など逐次プロンプト手順を示すほうが安定する。

- グループサイズ（今回 50）とデータ特性の影響
  - 小規模（50 件）は標本バラツキが大きい：あるゲームの特定トピック（例：あるタイトルの音声称賛）が少数混入するとノイズとなる。安定した「グループ差」を抽出するにはより大きい group_size が望ましい。
  - 今回のサンプルではグループ間トピックの偏り（B に audio 明示、A に audio ほぼ無し）が大きく、もし実験設定が「A が発火群」と指定されていたならデータ抽出ミス（ラベル付けミス）か、群のサンプリング基準が不適切である可能性が高い。
  - またレビューの書式（Pros/Cons ヘッダや [h1] タグなど）が B に多い点は、LLM が「構造的レビュー＝特定特徴の記載が多い」と学習済みであれば B を容易に識別できるはずだが、少数ショットではそのルールを学ばせきれない。

5) 改善の示唆（具体的手順）
- データ前処理・解析段階（モデルプロンプトを与える前）
  1. 集合差分の事前統計解析を導入する
     - 各群について TF-IDF、log-odds ratio、chi-square、頻度比を計算し、上位 N 単語（unigram/bigram）を自動抽出する。今回なら「soundtrack」「voice actors」「sound」が B に高頻度で偏ることが数値的に確認できるはず。
     - これをモデルへの入力（「A の上位語: ...; B の上位語: ...」）として与えることで LLM の出力精度が飛躍的に上がる。
  2. ノイズ除去
     - HTML タグ（[h1] 等）や特殊記号、スクレイプによる混入文を正規化する。感情的な極端表現が多いサンプルは別バケット化し、主題抽出時に重みを下げるなど。
  3. 表式化
     - まず「差分キーワードの抽出（上位 10）」→「そのキーワードに基づく説明の生成」→「短いタグ（1–4語）への圧縮」という分段プロセスを設計する（マルチステッププロンプト）。

- プロンプト設計（実務的具体例）
  1. ステップ1（診断）: 「A と B の上位 15 単語は以下です。A: ... B: ...。これらを見て A に特徴的なトピックを 3 つ、B に特徴的なトピックを 3 つ挙げよ（根拠となるキーワードを併記）。」
  2. ステップ2（要約→命名）: 「B の主要トピックの一つは [soundtrack, voice actors, audio] です。これを人間が理解しやすい短いラベル（例: 'audio related characteristics'）で 1 つだけ命名せよ。理由を 1 文で述べよ。」
  3. 出力フォーマットの厳格化: 「出力: <label> || <1-sentence-justification> の形式のみを返す。不要な説明は書かない。」
  4. Few-shot の与え方: 上記ステップを 3 程度の多様な例で示す（例：visual-related, performance-related, community-related の mapping を示す）。

- モデル・プロセス改良
  - シードショットを増やす（3–5 shot）、ショットは各トピック（audio/visual/performance）からバランス良く選ぶ。
  - 温度を低め（0–0.3）にして決定性を確保する。
  - 複数生成＋再ランキング: LLM に複数候補ラベルを生成させ、TF-IDF 差分や埋め込み類似度で再選択する。
  - チェックポイント: 出力が空にならないように「必ず 1 語以上出力する」指示を入れる。

- 評価指標の改善
  - BLEU は短いラベル評価に不適切。提案指標:
    - BLEURT / BARTScore：人手参照を学習したスコアで短いフレーズの意味的一致を評価しやすい。
    - MoverScore / BERTScore（正しく計算）: 埋め込みベースで語彙差を吸収。
    - 意味的クラスタリング評価: gold ラベルと生成ラベルの意味的距離（埋め込みコサイン）を計測し閾値で合否判定。
    - 最終的には人手アノテータによる一致度（複数アノテータの多数決）を用いた精度計測を併用する。
  - また評価時は許容同義語リスト（audio-related / sound-related / soundtrack）を用いた緩和評価も考える。

- 実験デザインの改善案（group_size 等）
  - group_size を増やして（100, 150, 300）で安定性を確認。多くのサンプルで語彙分布が安定するので抽出ノイズが減るはず。
  - さらに「同一ゲーム内での A/B 比較」ではなく「横断的に複数タイトルを混ぜて一般性を評価」する場合は、各ゲームから等数サンプリングして偏りを抑える。
  - gpt-5.1 での検証（既に計画あり）を行う価値はあるが、まずプロンプトと前処理を改善し確度を上げてからモデル比較を行うのが順序として妥当。

- デバッグ提案（今回の 0 スコアに対して）
  1. 評価パイプライン確認：predicted が空文字列になっていないか、言語・エンコード・トークナイザの整合性をチェック。
  2. LLM の raw 出力ログを保存して、何が返ってきたか（エラー・制限・長さ切り捨て等）を確認する。
  3. 同じ入力で簡易プロンプト（「A と B の違いを 3 単語以内で」）を与え手動で出力確認し、モデルが返答を止める条件を特定する。

総括（結論）
- 統計的・語彙的観点からは、正解ラベル "audio related characteristics" は提示されたデータ群のうち「グループ B」に明確に根拠が存在する。グループ A は主に操作性・パフォーマンス・物語・ビジュアルに関する語彙を持ち、audio 語彙はほとんど見られない。
- 現状で LLM が有効な対比因子を生成できなかった主因は以下の混合だと推定される：
  1. 入力群と正解ラベルのミスマッチ（集合選択ミスまたはラベリングミス）
  2. Few-shot（1-shot）が不十分で、プロンプトが差分抽出の手順を明示していない
  3. 評価パイプラインの問題（出力の取りこぼしやエンコード不整合）により BERTScore が 0 になった可能性
  4. group_size が小さくてノイズが大きい（標本誤差）

- 優先的な改善アクション（短期）
  1. 評価ログと出力の生ログを確認して「出力が存在するか」確定する（デバッグ）。
  2. TF-IDF / log-odds による差分キーワード抽出を事前処理として導入し、その結果をプロンプトに与える。
  3. Few-shot を 3–5 に増やし「段階的処理（抽出→要約→命名）」を示すプロンプトを採用。
  4. BLEURT/BARTScore 等、意味的類似性を測る評価指標を追加し再評価する。

これらを実施すれば、LLM による「集合差分の自然言語ラベリング（対比因子ラベル）」は高い確度で実現可能であり、今回のような評価ゼロの事象は再発しにくくなると考えます。必要であれば、上記の TF-IDF 差分抽出スクリプトや具体的な多段プロンプト（3-shot の完全テンプレート）を作成して提供します。必要に応じてお申し付けください。

## steam_group_sizeカテゴリ全体の考察

以下は「steam_group_size」カテゴリ（Steamレビューの群比較での対比因子自動生成実験群）に対する、与えられた個別実験考察ログ（20件）を踏まえたカテゴリ全体の総合考察です。問題点の要約、観察された共通パターン、設定要因の影響、今後の示唆を優先度付きで整理しました。

1. カテゴリ全体の傾向（共通パターン）
- 出力欠落／評価ゼロが支配的
  - 多くの実験で「LLM生成対比因子」が実質的に空で、BERT/BLEU が共に 0.0 になっている。これは単なる性能低下ではなく「生成または評価パイプラインの欠陥（出力保存ミス、エンコード/前処理の不整合、タイムアウト／トランケーション等）」を強く示唆する。
- データ側の確度は概ね高い（ラベル妥当性）
  - 代表サンプル観察では、各カテゴリ（gameplay/visual/story/audio）に対応する語彙が群のどちらかに確かに偏在しているケースが多い（例：gameplay→controls/combat、visual→graphics/artstyle、story→narrative/characters、audio→soundtrack/voice）。つまり「正解ラベル自体は妥当」であり、問題はLLMの出力取得・整合化にあることが多い。
- ノイズ・トピック混在が顕著
  - Steamレビューは長文・罵倒・編集タグ（[h1],[b]等）や固有名詞、複数トピック（アート/音楽/操作/価格/サーバ）が混在するため、集合レベルの差分は「単一軸」ではなく複合的になりがち。これがラベル化の難しさを増している。

2. パフォーマンスの特徴（スコア分布と傾向）
- スコア分布
  - ログ上は多くが BERT/BLEU = 0.0。出力が存在すればBLEUは語彙一致により低めになりがち、BERTScore は通常一定の非ゼロ値を示すはずだが今回はゼロが多発しているため「評価不能（出力欠落/処理ミス）」が主因。
- 高スコア実験の共通特徴（観察からの仮説）
  - （観察が限られるが）高評価が期待できる条件は、（1）群内で特定トピック語が高頻度に偏在、（2）プロンプトが短ラベル出力を明確に指示、（3）前処理でノイズを除去し差分語を与えた、という組合せ。
- 低スコア（ゼロ）実験の特徴
  - ほとんど全ての実験に共通：Few-shot=1 のまま生テキストを大量投入、出力の生ログ未保存／評価前処理ミス、BLEUのみ依存等。これらが低スコア（あるいは評価不能）を招いている。

3. 設定パラメータの影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力形式（短いラベル vs 長文説明）」の誘導力が弱く、不安定。タスク（集合差分→短い概念ラベル）では 3–5 shot の方が安定性が上がるというログ中の示唆が一貫している。
- グループサイズ（group_size）
  - 小（50）: ノイズや偶発的サンプルに影響されやすく、代表性が不安定。  
  - 中（100–150）: 差分シグナルが比較的安定して抽出しやすいバランス帯。多くの改善案で推奨されているのはこのレンジを基準に試行すること。  
  - 大（200–300）: 多様性が増えシグナルが希薄化する一方で、十分な前処理（クラスタリング・代表抽出）を行えば安定化も可能。だが生データをそのままプロンプトに入れるとトークン制限や情報過多で失敗しやすい。
- モデル（gpt-4o-mini 等）
  - gpt-4o-mini 自体は汎用性が高いが、長文集合比較や厳密なフォーマット出力（短い名詞句ラベル）に対してはプロンプト工夫と前処理が不可欠。モデル変更（より大きなモデル）で改善は見込めるが、まずはパイプライン／プロンプト改善が先決。
- 評価指標の選択
  - BLEU は短いラベル比較に不向き（語彙揺れに敏感）。BERTScoreは意味的に優れるが今回の0多発はパイプライン問題を示す。BLEURT、BARTScore、埋め込みコサイン等の導入と、多参照/同義語辞書の準備が推奨される。

4. 洞察と示唆（主要知見と今後の研究方向）
A. 主要知見（要点）
- 根本問題は「運用（パイプライン）＋設計（プロンプト／前処理／評価）」にあることが最も多くの実験で示唆される。データ自体はラベルに対応するシグナルを持つことが多いが、LLM出力の取得/正規化/評価のいずれかで失敗している。
- 単にモデルを変えるより先に、（1）出力ログ保存、（2）評価パイプラインの前処理整合、（3）出力フォーマット強制、（4）差分語抽出などの前処理ワークフローを整備することが効果的。
- group_size の調整だけでなく「群内部のトピック収束度（視覚語率、音語率など）」を計測し、最適なサンプル数・代表化方法を決めるべき。

B. 優先度付き改善提案（実務的）
1) 最優先（必ず行う）
  - raw LLM 出力（API応答）を全て保存し、出力が空かどうか、トークン上限で切れていないかなどを検証する。出力が空なら直ちにAPIログ／エラー原因を調査。
  - 評価パイプラインの入出力前処理を固定：正解ラベル・生成ラベルともに同一の正規化（小文字化・trim・Unicode正規化・HTML除去）を行い、評価を再実行する。
2) 高効果（次に実施）
  - Prompt engineering：Few-shot を 3–5 ショットに増やし、出力を「1–3語の英語名詞句のみ (no explanation)」に厳格化。成功例 / 失敗例（bad example）を混ぜて示す。
  - 前処理パイプライン導入：TF-IDF / log-odds で A/B の差分キーワード上位を抽出し、そのリストを LLM に与えてラベル命名させる（二段階化）。またはクラスタリングで代表文を抽出して提示する。
  - 評価指標改善：BLEU廃止→BLEURT/BARTScore/embedding cosine を導入し、同義語マップ（許容ラベル群）を作る。
3) 中長期（実験設計 / 研究）
  - group_size 感度実験：50/100/150/200/300 の各サイズで複数ラン（シード）を実行し、ラベル出力の安定度（同一ラベル再現率、embedding類似度分散）を評価して最適サイズを選定。
  - ハイブリッドワークフロー：統計的手法で候補語を自動抽出 → LLM が短ラベルに正規化 → 小規模人手で承認する運用（半自動ラベリング）を構築。
  - 出力の透明化：LLMにラベルと同時に「支持する代表例/キーワード」を出力させ、説明可能性（explainability）を担保する。

C. 研究的示唆
- 集合差分ラベリング（group-level concept discovery）は「ノイズの多いUGC（Steam等）」では直接LLMに大量テキストを渡すだけでは不安定。統計的差分解析（log-oddsなど）とLLMの組合せ（証拠→命名）が有望である。
- 評価手法研究：短ラベル評価に適する自動指標の検証（BLEURT等）と人手評価の少量混入によるキャリブレーションが必要。
- 出力欠落の発生源（API側タイムアウト・filtering・プロンプト長超過など）を定量的にログし、再現性の高い障害モデルを作ると将来的な改善に寄与する。

5. 最後に：短期チェックリスト（実装担当向け）
- 保存ログの確認（raw responses + HTTP status）→ 出力が無ければAPIログを精査。
- 評価パイプラインの単体テスト（既知のref + hyp でBERT/BLEUが期待値を返すか）。
- 簡素な sanity-check プロンプト（3対3の代表テキストを入力、temperature 0、出力1語）で動作確認。
- 差分語の自動抽出を1回実行（各群 top-20）して、LLMに与えてラベル化する方式を試す（短期実験）。

———

要約：現状の失敗は主に「出力欠落／評価パイプライン不備」＋「プロンプト設計と前処理不足」に起因する。データ自体はラベルと整合するシグナルを含む場合が多い（各aspectごとに該当語彙あり）。まずはログ確認と前処理＋プロンプト改善（3–5 shot・短ラベル強制）、差分語抽出を組み合わせた二段階ワークフローを実装し、その上でgroup_size感度テストと評価指標の改善（BLEURT等）を進めることを強く推奨します。必要であれば、（A）代表サンプルからのTF-IDF/log-odds抽出結果、（B）3–5-shotプロンプトテンプレート、（C）評価パイプラインチェックリストの具体案を作成します。どれを先に出しますか？

