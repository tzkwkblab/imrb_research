# 実験考察レポート: steam_visual_group_size_150_1_4o-mini_word

## 個別実験の詳細考察

以下は提示いただいた実験データ（Group A/B の代表サンプル、正解ラベル「visual related characteristics」、LLM（gpt-4o-mini）出力が事実上空になり BERT/BLEU が 0 となった結果）に基づく詳細な考察です。論点ごとに分け、単語レベルの具体例・原因推定・改善案を示します。

1. 単語レベルでの特徴分析
- 手法（前提）  
  - 本解析は代表サンプル群を目視＋語彙抽出（頻出語・目立つ固有表現・否定語・感嘆表現など）で比較したものです。正式な統計（TF/TF-IDF、χ2、log-likelihood 等）を併用するとより確実ですが、以下はサンプルから明瞭に見える語彙差です。

- Group A（発火群）に特徴的な単語・表現（具体例）
  - ビジネス／運営関連語: "ads", "DLC", "microtransactions", "no denuvo", "no unnecessary launcher", "epic exclusivity", "stupid business decisions", "piracy"
  - アップデート／不具合関連: "update that literally breaks the game", "breaks", "bug", "V6.0 Infantry Combat Overhaul", "delete the intro", "fence that wasn't fully s..."
  - 強い感情表現・一人称の私的エピソード: "My wife left and took the kids", "I was impatient", "honest edit", "lmao", "I love this game", "completely in love"
  - カジュアル／スラング・強烈表現: "heroin-tier addictive", "eyegasm", "lmao", "stupid"
  - メタ表現・分類語: "confessional", "Early Access", "launch controversy"

- Group B（非発火群）に特徴的な単語・表現（具体例）
  - 視覚・芸術表現: "visuals are stunning", "graphics", "pixel artstyle", "stunning", "soundtrack"
  - 推薦・評価語: "recommend", "definitely recommend", "excellent game", "one of the best games", "great game"
  - ゲーム性・ジャンル説明: "parry bosses", "final fantasy tactics", "story-rich", "multiplayer"
  - 穏やかな称賛／説明語: "sincerity", "joyful", "outstanding", "memes", "funniest"

- 文脈での使用と意味的ニュアンス
  - Group A の "ads", "microtransactions", "piracy" 等は「運営方針への批判→ユーザーの反応（否定・逃避）」を示す。例: "They shadow dropped an update that literally breaks the game and DLC, and for what? Ads.... ads for RS+" は「アップデート→破壊→目的は広告」という因果的・批判的文脈。
  - Group A の "heroin-tier addictive" や "eyegasm" は誇張的・感情的メタファー。肯定的に用いられることもある（強い称賛）一方、語用論上は感情の激しさや非形式性を示す。 "My wife left and took the kids" のような私的告白はレビュー本文が個人的ストーリー寄りであることを示す（ノイズ的）。
  - Group B の "visuals are stunning", "pixel artstyle", "soundtrack" は感覚依存の肯定語で、視覚的特徴の記述に直結する。語義は具体的で客観的評価に落とし込みやすい。

- 感情的側面
  - Group A は「怒り／不満」「強烈な賛美（中毒性）」「個人的告白（悲嘆）」といった幅の広い感情表現を含み、語彙は強度が高く分散している（ノイズが多い）。
  - Group B は「穏やかな称賛」「描写的形容（visual, soundtrack）」が多く、感情は比較的ポジティブかつ説明的で安定している。

2. 文脈・意味的ニュアンスの考察
- Group A の共通文脈的特徴
  - 運営／アップデート／課金関連に対する批判やトラブルの報告が多い（"breaks the game", "ads", "microtransactions", "launcher"）。これは「製品の外的要因（運営）や技術的不具合」が語られる割合が高いことを示す。
  - 個人感情や体験（confessional, "My wife left..." など）を交えた叙述が目立ち、レビューが個人的物語・立場表明になりやすい。
  - 文体が口語的・スラングを含みメタファーも多く、抽象的な概念表現よりも反応的・感情表現に富む。
- Group B と比較した意味的・概念的差異
  - Group B は主にプロダクト属性（視覚・音響・ゲームデザイン）に焦点を当てた説明が多く、観察的・評価的な語彙（visuals, soundtrack, pixel artstyle）が安定している。したがって「視覚的特徴（visual related characteristics）」という正解ラベルに合致しやすい。
  - Group A は「運営・政策・バグ・感情表現」という属性が混在しており、視覚的特徴が中心ではない。よって集合差分としては「運営/感情的反応に富む群 vs 視覚的／描写的評価に富む群」という対比が想定される。
- 抽象的概念や間接表現
  - Group A には比喩（"heroin-tier", "eyegasm"）や誇張が多く、直接的に「何が視覚的に優れている」とはわかりにくい。間接的表現・皮肉も多く、これが自動要約／命名を難しくしている。
  - Group B は抽象表現は相対的に少なく、直接的な属性語（visual 等）が使われやすい。

3. 正解ラベルとの比較（LLM 出力が空／不一致である点を含める）
- LLM の出力状況（推定）  
  - 実測の評価スコア（BERT: 0.0000、BLEU: 0.0000）から推定すると、LLM が返した生成が「空文字列」か、評価ルーチンが参照できないフォーマット（例：HTMLタグのみ、言語が混在、特殊文字のみ）であった可能性が高いです。通常単語列があれば BERTScore は 0 より大きい値になるため、完全に出力がないかスコア計算が失敗したと考えられます。
- LLM生成対比因子と正解ラベルの一致度
  - 実際の生成が空であるため一致なし。ただし期待される良い生成は「visual related characteristics（視覚に関連する記述／語彙が Group B 側に多い）」であり、Group B の語彙分布が正解を支持する。
- 一致／不一致の具体指摘
  - 一致する可能性のある側面: Group B のサンプルは明示的に "visuals", "graphics", "pixel artstyle" など視覚に関する記述があり、正解ラベルと一致する根拠が存在する。
  - 不一致の原因（LLM 側）: 生成が空であったこと自体が最大の不一致。さらに、Group A の語彙分布（多様でノイズが多い）により、LLM が「どの集合が視覚関連か」を見誤った可能性もある（例：グループAに "Looks great regardless" のような視覚表現が散在しているため混乱）。
- BERT/BLEU スコアの乖離（およびゼロ化）の原因考察
  - 出力が空または非常に短いフォーマット（非語彙）であった。  
  - スコア計算時の前処理（小文字化／トークナイズ）の不一致、あるいは参照ラベルのフォーマット（単語群 vs 文）と生成の形式のミスマッチ。  
  - BLEU がゼロになるのは生成語と参照語のn-gram一致が全くない場合だが、BERTScore は意味的類似を埋め込みで測るため通常 0 にはならない。したがって BERTScore = 0 は「出力が存在しない／無効値が代入された」エラーの可能性が高い。

4. 実験設定の影響
- Few-shot (1-shot) の影響
  - 1-shot は出力スタイルをある程度誘導できるが、示し方（例の質）に非常に依存する。今回のように Group A/B がノイズ混在・語彙重複がある場合、1-shot では「対比ラベルの形式（短い名詞句 vs 説明文）」や「ラベル候補の語彙範囲」を十分に伝えきれない。  
  - 例えば示した例が「説明的な一文」だった場合、モデルは長文要約を返す癖を残す。逆に「単語で答えよ」と明確に示さないと、LLM は自由記述になるため評価との不整合が生じる。
- グループサイズ・データセット特性の影響
  - 実データは両群とも 150 件ずつ（代表サンプルは雑多）。group_size が小さいと（例：50）ばらつきにより特徴抽出が不安定、極端な発言（例：「My wife left」）の影響が大きくなる。今回 150 は中規模であるが、レビュー内にノイズ・メタデータ（HTML タグ、編集マーク、断片的テキスト）が多く、集合特徴の抽出を困難にしている。  
  - グループ間で共通フレーズ（"One of the best games I have ever played" が両側に存在）があるため、単純な頻度差だけでは差分を検出しにくい。  
  - 「正解ラベルが抽象的（visual related characteristics）」である一方、入力集合は多様な観点（運営批判、個人的告白、技術評価、視覚称賛）を混ぜているため、集合レベルの明確な差分が弱ければ LLM は曖昧な出力を返すか出力失敗する。

5. 改善の示唆（実践的対処）
- データ前処理（必須）
  - HTML タグや見出しマーク [h1] 等の除去、文の正規化（省略符号の展開、改行の統合）、極端な個人エピソード/非レビュー的行のフィルタ（"My wife left" のようにゲーム内容と無関係なもの）を除外することでノイズを減らす。  
  - 重複句（テンプレ句）を検出してダウンサンプリングすることで、偏った n-gram が支配するのを防ぐ。
- 統計的な「候補語抽出」→ LLM に選ばせる二段構成
  - ステップ1: χ2 / log-likelihood / TF-IDF で A/B の差分語（上位 20–50 単語）を自動抽出する。  
  - ステップ2: その語群を LLM に与え、「この語群の集合差は何か、一語または短い句で表せ」と命令する。  
  - 理由: 生テキストから直接自然言語要約させるより、語彙候補を与えることで LLM が不要な語彙空間探索をせずに済み、安定した短いラベルを生成しやすい。
- プロンプト改良（実効的）
  - 明確な出力形式を強制する（例: 「1–3語の名詞句のみで答えよ。句以外は評価で無効となる」）。  
  - 例示（few-shot）は「Steam ドメインかつ短いラベル」を複数（3-shot）与える。例示は必ず成功例（視覚ラベルや運営批判ラベル）を入れる。  
  - 温度を低め（例: 0〜0.2）にし、max_tokens を少なく設定して短い出力を誘導する。
- 評価指標の改善
  - BLEU は語彙一致を重視するため本タスク不適。BERTScore は意味的比較に有利だが、出力ゼロ化には無力。まずは出力欠損を検出しログを確認（raw outputs）すること。  
  - 提案指標: BLEURT（人手評価学習済み）、BARTScore（生成確率ベース）、MoverScore。さらに、生成ラベルと正解ラベルの語彙類似度を埋め込みコサインで測る単純指標（embedding similarity）を導入すると良い。  
  - 人手評価も併用して「ラベルとして妥当か」を確認する（特に抽象概念ラベルは自動評価が難しい）。
- 実験設計の改善
  - group_size を変える実験は続けるべきだが、各 group_size でノイズの程度を同様に揃える（前処理を同一に）こと。  
  - グループ分けの基準（どのニューロン発火からサンプルを取ったか等）が偏ると語彙差が反映されないため、サンプリングの再現性を確保する。  
  - gpt-4o-mini 以外のモデル（より大きい gpt-4 系や指示従順性の高いモデル）での比較も有用。モデル間で出力の安定性が変わる可能性が高い。
- 実例的ワークフロー（推奨）
  1. 生データのクリーニング（タグ除去、短文フィルタ、重複削除）  
  2. A/B の差分語を χ2 で抽出（上位 30）  
  3. 抽出語を LLM に「上位語を見て A を特徴づける短いラベルを 1–3 語で作れ」と与える（3-shot、低温度）  
  4. 生成結果を埋め込み類似度＋BLEURT でスコアリング、トップを採用／人手検査  
  5. 上位候補をさらに LLM に統合・正規化させる（シノニムの統一など）

まとめ（要点）
- 観察: Group B は視覚関連語が明確に多く、「visual related characteristics」が妥当なラベルと言える。一方 Group A は運営・不具合・強烈な感情表現などが多く、視覚ラベルとは異なる語彙分布を示す。  
- 問題点: 実験で得られた LLM 出力が空／不正であったため評価がゼロになった。原因はプロンプト設計・前処理不足・出力形式強制の欠如、あるいは評価パイプライン側の入力不整合のいずれか／複合と考えられる。  
- 改善: データのクレンジング、統計的語彙候補抽出→LLM の再利用、明確な出力フォーマット指示、より適切な自動評価指標（BLEURT 等）を組み合わせることで有意義な対比因子ラベル自動生成の実現可能性は高められる。

必要であれば、上記ワークフローの具体的なプロンプト例（few-shot の例示含む）、χ2 による語彙差分抽出のコードスニペット、あるいは代表 150 件全体に対する簡易 TF-IDF/χ2 集計をこちらで実行して詳細な候補語リストを作成することも可能です。どの改善案を優先して進めたいか指示ください。

## steam_group_sizeカテゴリ全体の考察

以下は「steam_group_size」カテゴリ（Steamレビューの群比較での対比因子自動生成実験群）に対する、与えられた個別実験考察ログ（20件）を踏まえたカテゴリ全体の総合考察です。問題点の要約、観察された共通パターン、設定要因の影響、今後の示唆を優先度付きで整理しました。

1. カテゴリ全体の傾向（共通パターン）
- 出力欠落／評価ゼロが支配的
  - 多くの実験で「LLM生成対比因子」が実質的に空で、BERT/BLEU が共に 0.0 になっている。これは単なる性能低下ではなく「生成または評価パイプラインの欠陥（出力保存ミス、エンコード/前処理の不整合、タイムアウト／トランケーション等）」を強く示唆する。
- データ側の確度は概ね高い（ラベル妥当性）
  - 代表サンプル観察では、各カテゴリ（gameplay/visual/story/audio）に対応する語彙が群のどちらかに確かに偏在しているケースが多い（例：gameplay→controls/combat、visual→graphics/artstyle、story→narrative/characters、audio→soundtrack/voice）。つまり「正解ラベル自体は妥当」であり、問題はLLMの出力取得・整合化にあることが多い。
- ノイズ・トピック混在が顕著
  - Steamレビューは長文・罵倒・編集タグ（[h1],[b]等）や固有名詞、複数トピック（アート/音楽/操作/価格/サーバ）が混在するため、集合レベルの差分は「単一軸」ではなく複合的になりがち。これがラベル化の難しさを増している。

2. パフォーマンスの特徴（スコア分布と傾向）
- スコア分布
  - ログ上は多くが BERT/BLEU = 0.0。出力が存在すればBLEUは語彙一致により低めになりがち、BERTScore は通常一定の非ゼロ値を示すはずだが今回はゼロが多発しているため「評価不能（出力欠落/処理ミス）」が主因。
- 高スコア実験の共通特徴（観察からの仮説）
  - （観察が限られるが）高評価が期待できる条件は、（1）群内で特定トピック語が高頻度に偏在、（2）プロンプトが短ラベル出力を明確に指示、（3）前処理でノイズを除去し差分語を与えた、という組合せ。
- 低スコア（ゼロ）実験の特徴
  - ほとんど全ての実験に共通：Few-shot=1 のまま生テキストを大量投入、出力の生ログ未保存／評価前処理ミス、BLEUのみ依存等。これらが低スコア（あるいは評価不能）を招いている。

3. 設定パラメータの影響
- Few-shot（1-shot）の影響
  - 1-shot は「出力形式（短いラベル vs 長文説明）」の誘導力が弱く、不安定。タスク（集合差分→短い概念ラベル）では 3–5 shot の方が安定性が上がるというログ中の示唆が一貫している。
- グループサイズ（group_size）
  - 小（50）: ノイズや偶発的サンプルに影響されやすく、代表性が不安定。  
  - 中（100–150）: 差分シグナルが比較的安定して抽出しやすいバランス帯。多くの改善案で推奨されているのはこのレンジを基準に試行すること。  
  - 大（200–300）: 多様性が増えシグナルが希薄化する一方で、十分な前処理（クラスタリング・代表抽出）を行えば安定化も可能。だが生データをそのままプロンプトに入れるとトークン制限や情報過多で失敗しやすい。
- モデル（gpt-4o-mini 等）
  - gpt-4o-mini 自体は汎用性が高いが、長文集合比較や厳密なフォーマット出力（短い名詞句ラベル）に対してはプロンプト工夫と前処理が不可欠。モデル変更（より大きなモデル）で改善は見込めるが、まずはパイプライン／プロンプト改善が先決。
- 評価指標の選択
  - BLEU は短いラベル比較に不向き（語彙揺れに敏感）。BERTScoreは意味的に優れるが今回の0多発はパイプライン問題を示す。BLEURT、BARTScore、埋め込みコサイン等の導入と、多参照/同義語辞書の準備が推奨される。

4. 洞察と示唆（主要知見と今後の研究方向）
A. 主要知見（要点）
- 根本問題は「運用（パイプライン）＋設計（プロンプト／前処理／評価）」にあることが最も多くの実験で示唆される。データ自体はラベルに対応するシグナルを持つことが多いが、LLM出力の取得/正規化/評価のいずれかで失敗している。
- 単にモデルを変えるより先に、（1）出力ログ保存、（2）評価パイプラインの前処理整合、（3）出力フォーマット強制、（4）差分語抽出などの前処理ワークフローを整備することが効果的。
- group_size の調整だけでなく「群内部のトピック収束度（視覚語率、音語率など）」を計測し、最適なサンプル数・代表化方法を決めるべき。

B. 優先度付き改善提案（実務的）
1) 最優先（必ず行う）
  - raw LLM 出力（API応答）を全て保存し、出力が空かどうか、トークン上限で切れていないかなどを検証する。出力が空なら直ちにAPIログ／エラー原因を調査。
  - 評価パイプラインの入出力前処理を固定：正解ラベル・生成ラベルともに同一の正規化（小文字化・trim・Unicode正規化・HTML除去）を行い、評価を再実行する。
2) 高効果（次に実施）
  - Prompt engineering：Few-shot を 3–5 ショットに増やし、出力を「1–3語の英語名詞句のみ (no explanation)」に厳格化。成功例 / 失敗例（bad example）を混ぜて示す。
  - 前処理パイプライン導入：TF-IDF / log-odds で A/B の差分キーワード上位を抽出し、そのリストを LLM に与えてラベル命名させる（二段階化）。またはクラスタリングで代表文を抽出して提示する。
  - 評価指標改善：BLEU廃止→BLEURT/BARTScore/embedding cosine を導入し、同義語マップ（許容ラベル群）を作る。
3) 中長期（実験設計 / 研究）
  - group_size 感度実験：50/100/150/200/300 の各サイズで複数ラン（シード）を実行し、ラベル出力の安定度（同一ラベル再現率、embedding類似度分散）を評価して最適サイズを選定。
  - ハイブリッドワークフロー：統計的手法で候補語を自動抽出 → LLM が短ラベルに正規化 → 小規模人手で承認する運用（半自動ラベリング）を構築。
  - 出力の透明化：LLMにラベルと同時に「支持する代表例/キーワード」を出力させ、説明可能性（explainability）を担保する。

C. 研究的示唆
- 集合差分ラベリング（group-level concept discovery）は「ノイズの多いUGC（Steam等）」では直接LLMに大量テキストを渡すだけでは不安定。統計的差分解析（log-oddsなど）とLLMの組合せ（証拠→命名）が有望である。
- 評価手法研究：短ラベル評価に適する自動指標の検証（BLEURT等）と人手評価の少量混入によるキャリブレーションが必要。
- 出力欠落の発生源（API側タイムアウト・filtering・プロンプト長超過など）を定量的にログし、再現性の高い障害モデルを作ると将来的な改善に寄与する。

5. 最後に：短期チェックリスト（実装担当向け）
- 保存ログの確認（raw responses + HTTP status）→ 出力が無ければAPIログを精査。
- 評価パイプラインの単体テスト（既知のref + hyp でBERT/BLEUが期待値を返すか）。
- 簡素な sanity-check プロンプト（3対3の代表テキストを入力、temperature 0、出力1語）で動作確認。
- 差分語の自動抽出を1回実行（各群 top-20）して、LLMに与えてラベル化する方式を試す（短期実験）。

———

要約：現状の失敗は主に「出力欠落／評価パイプライン不備」＋「プロンプト設計と前処理不足」に起因する。データ自体はラベルと整合するシグナルを含む場合が多い（各aspectごとに該当語彙あり）。まずはログ確認と前処理＋プロンプト改善（3–5 shot・短ラベル強制）、差分語抽出を組み合わせた二段階ワークフローを実装し、その上でgroup_size感度テストと評価指標の改善（BLEURT等）を進めることを強く推奨します。必要であれば、（A）代表サンプルからのTF-IDF/log-odds抽出結果、（B）3–5-shotプロンプトテンプレート、（C）評価パイプラインチェックリストの具体案を作成します。どれを先に出しますか？

