# 実験結果：テキスト類似度評価

## 実施日時
2025年05月27日

## 実験概要
前回の実験で出力されたGPTレスポンスと元の特徴記述の類似度をBERTとBLEUを用いて評価しました。

## 対象データ
- 対象特徴: Feature 2, 3, 4, 5
- GPTレスポンス: baseline_feature{N}_20250523_154828.json ファイルの"gpt_response"フィールド
- 元の特徴記述: review_features.csvの各特徴記述

## 評価結果

### 特徴別結果
| Feature | GPTレスポンス | 元の特徴記述 | BERT類似度 | BLEU類似度 |
|---------|---------------|--------------|------------|------------|
| 2 | "Mentions detailed technical features and usage examples" | "Includes technical details or specifications" | 0.8186 | 0.0360 |
| 3 | "Contains detailed personal experience and specific product comparisons" | "Contains comparisons with competing products" | 0.7675 | 0.0351 |
| 4 | "Mentions detailed product features and user experience" | "Mentions long-term usage experience" | 0.7461 | 0.0451 |
| 5 | "Contains detailed technical usage and installation advice" | "Explains specific use cases or purposes" | 0.7052 | 0.0000 |

### 統計サマリー
- **BERT類似度平均**: 0.7593 (標準偏差: 0.0409)
- **BLEU類似度平均**: 0.0291 (標準偏差: 0.0172)

## 考察

### BERT類似度について
- 平均0.7593と良好な結果
- Feature 2が最も高い類似度（0.8186）
- 意味的な類似性は概ね保たれている

### BLEU類似度について
- 全体的に低い結果（平均0.0291）
- Feature 5は完全に0
- GPTが異なる語彙・表現を使用していることを示す

## 結論
1. **意味的類似性は保持**: BERTスコアが示すように、GPTは元の特徴の意味を適切に捉えている
2. **表現の多様性**: BLEUスコアの低さは、GPTがより詳細で説明的な表現に言い換えていることを示す
3. **改善の余地**: より元の表現に近い生成のためにプロンプト調整が可能

## 生成ファイル
- `text_similarity_evaluator.py`: 類似度評価スクリプト
- `analysis_report_generator.py`: 詳細分析レポート生成スクリプト
- `similarity_evaluation_results.json`: 評価結果（JSON形式）
- `similarity_evaluation_results.csv`: 評価結果（CSV形式）
- `detailed_analysis_report.md`: 詳細分析レポート
- `similarity_comparison.png`: 類似度比較グラフ
- `bert_vs_bleu_scatter.png`: BERT vs BLEU散布図
- `requirements.txt`: 必要ライブラリ一覧

## 今後の課題
1. 他の評価指標（ROUGE-L、語彙一致度など）の追加実装
2. プロンプト改善による表現の近似性向上
3. より多くの特徴での評価実施