#!/usr/bin/env python3
"""
çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®Ÿé¨“ä¾‹

DatasetManagerã‚’ä½¿ã£ãŸç°¡æ½”ãªå®Ÿé¨“å®Ÿè£…ä¾‹
å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±ä¸€çš„ã«æ‰±ã„ã€æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“å®Ÿè¡Œ
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# Utilsçµ±åˆ
utils_dir = Path(__file__).parent.parent.parent.parent / "utils"
sys.path.append(str(utils_dir))

from dataset_manager import DatasetManager
from contrast_factor_analyzer import ContrastFactorAnalyzer


class UnifiedExperimentExample:
    """çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½¿ç”¨ã®å®Ÿé¨“ä¾‹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.dataset_manager = DatasetManager()
        self.analyzer = ContrastFactorAnalyzer(debug=True)  # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹
        self.results_dir = Path(__file__).parent / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        print("ğŸ“Š çµ±ä¸€å®Ÿé¨“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    def demo_quick_experiment(self):
        """1è¡Œã§å®Ÿé¨“å®Ÿè¡Œãƒ‡ãƒ¢"""
        print("\nğŸš€ ã‚¯ã‚¤ãƒƒã‚¯å®Ÿé¨“ãƒ‡ãƒ¢")
        print("=" * 50)
        
        # åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª
        datasets = self.dataset_manager.list_available_datasets()
        print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {list(datasets.keys())}")
        
        # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç°¡å˜ãªå®Ÿé¨“
        results = []
        
        for dataset_id in ["steam", "semeval"]:  # å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿
            print(f"\nğŸ” {dataset_id}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“")
            
            try:
                # å®Ÿé¨“è¨­å®šã‚’å–å¾—
                config = self.dataset_manager.get_experiment_config(dataset_id)
                print(f"  åˆ©ç”¨å¯èƒ½ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {config['aspects'][:3]}...")  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
                
                # æœ€åˆã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§å®Ÿé¨“
                first_aspect = config['aspects'][0]
                print(f"  ğŸ¯ {first_aspect}ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§å®Ÿé¨“å®Ÿè¡Œ")
                
                # 1è¡Œã§ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
                if dataset_id == "steam":
                    splits = self.dataset_manager.get_binary_splits(
                        dataset_id, aspect=first_aspect, group_size=50, split_type="binary_label"
                    )
                else:
                    splits = self.dataset_manager.get_binary_splits(
                        dataset_id, aspect=first_aspect, group_size=50, split_type="aspect_vs_others"
                    )
                
                # Few-shotä¾‹é¡Œå–å¾—
                examples = self.dataset_manager.create_examples(dataset_id, first_aspect, shot_count=1)
                
                print(f"    ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: A={len(splits.group_a)}, B={len(splits.group_b)}")
                print(f"    ä¾‹é¡Œæ•°: {len(examples)}")
                print(f"    æ­£è§£æ–‡: {splits.correct_answer}")  # æ­£è§£æ–‡è¡¨ç¤º
                
                # å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œ
                result = self.analyzer.analyze(
                    group_a=splits.group_a,
                    group_b=splits.group_b,
                    correct_answer=splits.correct_answer,
                    examples=examples,
                    output_dir=str(self.results_dir)
                )
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                if result:
                    result.update(splits.metadata)
                    result['examples_count'] = len(examples)
                    results.append(result)
                    
                    # è©³ç´°ãªçµæœè¡¨ç¤º
                    bert_score = result.get('evaluation', {}).get('bert_score', 0)
                    bleu_score = result.get('evaluation', {}).get('bleu_score', 0)
                    llm_response = result.get('process', {}).get('llm_response', 'N/A')
                    print(f"    LLMå¿œç­”: {llm_response}")
                    print(f"    âœ… BERT: {bert_score:.6f}")
                    print(f"    âœ… BLEU: {bleu_score:.6f}")
                else:
                    print(f"    âŒ å®Ÿé¨“å¤±æ•—")
                    
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±
        
        return results
    
    def demo_multi_aspect_experiment(self, dataset_id: str = "steam", max_aspects: int = 3):
        """è¤‡æ•°ã‚¢ã‚¹ãƒšã‚¯ãƒˆå®Ÿé¨“ãƒ‡ãƒ¢"""
        print(f"\nğŸ® {dataset_id}è¤‡æ•°ã‚¢ã‚¹ãƒšã‚¯ãƒˆå®Ÿé¨“ãƒ‡ãƒ¢")
        print("=" * 50)
        
        try:
            # å®Ÿé¨“è¨­å®šå–å¾—
            config = self.dataset_manager.get_experiment_config(dataset_id)
            aspects = config['aspects'][:max_aspects]  # æœ€åˆã®Nå€‹ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆ
            
            print(f"å¯¾è±¡ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspects}")
            print(f"äºˆæƒ³å®Ÿé¨“æ•°: {len(aspects)} Ã— {len(config['shot_settings'])} = {len(aspects) * len(config['shot_settings'])}")
            
            all_results = []
            
            for aspect in aspects:
                for shot_count in config['shot_settings']:
                    print(f"  ğŸ¯ {aspect} - {shot_count}shot")
                    
                    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
                    split_type = "binary_label" if dataset_id == "steam" else "aspect_vs_others"
                    splits = self.dataset_manager.get_binary_splits(
                        dataset_id, aspect=aspect, group_size=30, split_type=split_type  # å°ã•ãªã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
                    )
                    
                    # Few-shotä¾‹é¡Œå–å¾—
                    examples = self.dataset_manager.create_examples(dataset_id, aspect, shot_count)
                    
                    # å®Ÿé¨“å®Ÿè¡Œ
                    result = self.analyzer.analyze(
                        group_a=splits.group_a,
                        group_b=splits.group_b,
                        correct_answer=splits.correct_answer,
                        examples=examples,
                        output_dir=str(self.results_dir),
                        experiment_name=f"{dataset_id}_{aspect}_{shot_count}shot"
                    )
                    
                    # çµæœè¨˜éŒ²
                    if result:
                        result.update(splits.metadata)
                        result['shot_count'] = shot_count
                        all_results.append(result)
                        
                        bert_score = result.get('evaluation', {}).get('bert_score', 0)
                        print(f"    âœ… BERT: {bert_score:.3f}")
                    else:
                        print(f"    âŒ å¤±æ•—")
            
            # çµæœä¿å­˜
            if all_results:
                self._save_experiment_results(all_results, f"{dataset_id}_multi_aspect")
                print(f"\nğŸ“ˆ å®Ÿé¨“å®Œäº†: {len(all_results)}çµæœ")
            
            return all_results
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def demo_cross_dataset_comparison(self):
        """ã‚¯ãƒ­ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¯”è¼ƒãƒ‡ãƒ¢"""
        print(f"\nğŸ”„ ã‚¯ãƒ­ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¯”è¼ƒãƒ‡ãƒ¢")
        print("=" * 50)
        
        # å…±é€šã‚¢ã‚¹ãƒšã‚¯ãƒˆã§ã®æ¯”è¼ƒï¼ˆä¾¡æ ¼é–¢é€£ï¼‰
        comparison_configs = [
            {"dataset": "steam", "aspect": "price", "split_type": "binary_label"},
            {"dataset": "semeval", "aspect": "price", "split_type": "aspect_vs_others"},
            {"dataset": "amazon", "aspect": "price", "split_type": "aspect_vs_others"}
        ]
        
        comparison_results = []
        
        for config in comparison_configs:
            dataset_id = config["dataset"]
            aspect = config["aspect"]
            split_type = config["split_type"]
            
            print(f"  ğŸ¯ {dataset_id} - {aspect}")
            
            try:
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
                splits = self.dataset_manager.get_binary_splits(
                    dataset_id, aspect=aspect, group_size=50, split_type=split_type
                )
                
                # 1-shotä¾‹é¡Œ
                examples = self.dataset_manager.create_examples(dataset_id, aspect, shot_count=1)
                
                # å®Ÿé¨“å®Ÿè¡Œ
                result = self.analyzer.analyze(
                    group_a=splits.group_a,
                    group_b=splits.group_b,
                    correct_answer=splits.correct_answer,
                    examples=examples,
                    output_dir=str(self.results_dir),
                    experiment_name=f"cross_{dataset_id}_{aspect}"
                )
                
                if result:
                    result.update(splits.metadata)
                    result['comparison_type'] = 'cross_dataset_price'
                    comparison_results.append(result)
                    
                    bert_score = result.get('evaluation', {}).get('bert_score', 0)
                    bleu_score = result.get('evaluation', {}).get('bleu_score', 0)
                    print(f"    âœ… BERT: {bert_score:.3f}, BLEU: {bleu_score:.3f}")
                else:
                    print(f"    âŒ å¤±æ•—")
                    
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¯”è¼ƒçµæœä¿å­˜
        if comparison_results:
            self._save_experiment_results(comparison_results, "cross_dataset_comparison")
            
            # ç°¡å˜ãªæ¯”è¼ƒåˆ†æ
            print(f"\nğŸ“Š ã‚¯ãƒ­ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¯”è¼ƒçµæœ:")
            print("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | BERTã‚¹ã‚³ã‚¢ | BLEUã‚¹ã‚³ã‚¢ |")
            print("|------------|-----------|----------|")
            for result in comparison_results:
                dataset = result.get('dataset_id', 'N/A')
                bert = result.get('bert_score', 0)
                bleu = result.get('bleu_score', 0)
                print(f"| {dataset} | {bert:.3f} | {bleu:.3f} |")
        
        return comparison_results
    
    def _save_experiment_results(self, results, experiment_name):
        """å®Ÿé¨“çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{experiment_name}_results_{timestamp}.json"
        output_path = self.results_dir / filename
        
        summary = {
            "experiment_name": experiment_name,
            "timestamp": timestamp,
            "total_experiments": len(results),
            "results": results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çµæœä¿å­˜: {filename}")
    
    def run_all_demos(self):
        """å…¨ãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print("ğŸª çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å…¨ãƒ‡ãƒ¢å®Ÿè¡Œ")
        print("=" * 60)
        
        # ãƒ‡ãƒ¢1: ã‚¯ã‚¤ãƒƒã‚¯å®Ÿé¨“
        quick_results = self.demo_quick_experiment()
        
        # ãƒ‡ãƒ¢2: è¤‡æ•°ã‚¢ã‚¹ãƒšã‚¯ãƒˆå®Ÿé¨“
        multi_results = self.demo_multi_aspect_experiment("steam", max_aspects=2)
        
        # ãƒ‡ãƒ¢3: ã‚¯ãƒ­ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¯”è¼ƒ
        cross_results = self.demo_cross_dataset_comparison()
        
        # ç·åˆçµæœ
        total_experiments = len(quick_results) + len(multi_results) + len(cross_results)
        print(f"\nğŸ‰ å…¨ãƒ‡ãƒ¢å®Œäº†!")
        print(f"ç·å®Ÿé¨“æ•°: {total_experiments}")
        print(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.results_dir}")
        
        return {
            "quick_results": quick_results,
            "multi_results": multi_results,
            "cross_results": cross_results
        }


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ ãƒ‡ãƒ¢")
    
    experiment = UnifiedExperimentExample()
    results = experiment.run_all_demos()
    
    return results


if __name__ == "__main__":
    main() 