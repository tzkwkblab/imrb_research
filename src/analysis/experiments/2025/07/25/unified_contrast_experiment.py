#!/usr/bin/env python3
"""
çµ±åˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ï¼šæ–°ã—ã„utilsçµ±åˆãƒ„ãƒ¼ãƒ«æ´»ç”¨ç‰ˆ

ã€å®Ÿè£…å†…å®¹ã€‘
- DatasetManager.from_config()ã«ã‚ˆã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•
- ContrastFactorAnalyzerã«ã‚ˆã‚‹çµ±åˆåˆ†æ
- 3ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿé¨“ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿æº–å‚™â†’å®Ÿé¨“å®Ÿè¡Œâ†’çµæœåˆ†æï¼‰
- 3ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆÃ—ã‚¢ã‚¹ãƒšã‚¯ãƒˆÃ—Few-shotè¨­å®šã®åŒ…æ‹¬çš„å®Ÿé¨“
- JSONãƒ»Markdownå½¢å¼ã§ã®çµæœå‡ºåŠ›

ã€è©•ä¾¡æŒ‡æ¨™ã€‘
- BERTã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³é¡ä¼¼åº¦ãƒ»ä¸»è¦æŒ‡æ¨™ï¼‰
- BLEUã‚¹ã‚³ã‚¢ï¼ˆè¡¨å±¤ä¸€è‡´åº¦ãƒ»ä¸»è¦æŒ‡æ¨™ï¼‰
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm

# Utilsçµ±åˆãƒ‘ã‚¹è¨­å®š
utils_dir = Path(__file__).parent.parent.parent / "utils"
sys.path.append(str(utils_dir))

# æ–°ã—ã„utilså®Œå…¨æ´»ç”¨
from dataset_manager import DatasetManager
from contrast_factor_analyzer import ContrastFactorAnalyzer
from config import DatasetConfig, ConfigValidator

# å®Ÿé¨“å›ºæœ‰è¨­å®š
from experiment_config import CONFIG

# åŸºæœ¬è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class UnifiedContrastExperiment:
    """çµ±åˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config=CONFIG):
        """
        åˆæœŸåŒ–
        Args:
            config: å®Ÿé¨“è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµæœä¿å­˜ç”¨
        self.results = []
        self.summary_stats = {}
        self.errors = []
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰
        self.dataset_manager = None
        self.contrast_analyzer = None
    
    def _initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰"""
        if self.dataset_manager is None:
            logger.info("DatasetManageråˆæœŸåŒ–ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ï¼‰")
            self.dataset_manager = DatasetManager.from_config()
        
        if self.contrast_analyzer is None:
            logger.info("ContrastFactorAnalyzeråˆæœŸåŒ–")
            self.contrast_analyzer = ContrastFactorAnalyzer(debug=self.config.debug_mode)
    
    def phase1_data_preparation(self) -> Dict:
        """Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        logger.info("=== Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ ===")
        
        self._initialize_components()
        
        # 1. è¨­å®šæ¤œè¨¼
        logger.info("Step 1: è¨­å®šæ¤œè¨¼")
        validation_result = self.dataset_manager.validate_configuration()
        logger.info(f"è¨­å®šæ¤œè¨¼çµæœ: {validation_result['status']}")
        
        if validation_result['warnings']:
            for warning in validation_result['warnings']:
                logger.warning(f"è¨­å®šè­¦å‘Š: {warning}")
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
        logger.info("Step 2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ©ç”¨å¯èƒ½æ€§ç¢ºèª")
        available_datasets = self.dataset_manager.list_available_datasets()
        
        accessible_datasets = []
        for dataset_id in self.config.target_datasets:
            if dataset_id in available_datasets:
                info = available_datasets[dataset_id]
                if info.get('accessible', False):
                    accessible_datasets.append(dataset_id)
                    logger.info(f"âœ… {dataset_id}: ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½")
                else:
                    logger.warning(f"âŒ {dataset_id}: ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ - {info.get('warnings', ['ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'])[0]}")
            else:
                logger.error(f"âŒ {dataset_id}: è¨­å®šãªã—")
        
        # 3. å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±å–å¾—
        logger.info("Step 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆæƒ…å ±å–å¾—")
        dataset_stats = {}
        for dataset_id in accessible_datasets:
            try:
                stats = self.dataset_manager.get_data_statistics(dataset_id)
                dataset_stats[dataset_id] = stats
                logger.info(f"{dataset_id}: {stats.get('total_samples', 'N/A')}ã‚µãƒ³ãƒ—ãƒ«")
            except Exception as e:
                logger.error(f"{dataset_id}çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 4. å®Ÿé¨“ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ç”Ÿæˆ
        logger.info("Step 4: å®Ÿé¨“ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ç”Ÿæˆ")
        experiment_matrix = []
        
        for exp_def in self.config.get_experiment_matrix():
            dataset_id = exp_def['dataset_id']
            if dataset_id in accessible_datasets:
                experiment_matrix.append(exp_def)
            else:
                logger.warning(f"å®Ÿé¨“ã‚¹ã‚­ãƒƒãƒ—: {exp_def['experiment_id']} (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ©ç”¨ä¸å¯)")
        
        phase1_result = {
            "validation": validation_result,
            "accessible_datasets": accessible_datasets,
            "dataset_stats": dataset_stats,
            "experiment_matrix": experiment_matrix,
            "total_experiments": len(experiment_matrix)
        }
        
        logger.info(f"Phase 1å®Œäº†: {len(experiment_matrix)}å®Ÿé¨“ã‚’å®Ÿè¡Œäºˆå®š")
        return phase1_result
    
    def phase2_contrast_experiment(self, phase1_result: Dict) -> List[Dict]:
        """Phase 2: å¯¾æ¯”å®Ÿé¨“å®Ÿè¡Œ"""
        logger.info("=== Phase 2: å¯¾æ¯”å®Ÿé¨“å®Ÿè¡Œé–‹å§‹ ===")
        
        experiment_matrix = phase1_result['experiment_matrix']
        results = []
        
        with tqdm(total=len(experiment_matrix), desc="å®Ÿé¨“é€²è¡Œ") as pbar:
            for i, exp_def in enumerate(experiment_matrix):
                pbar.set_description(f"å®Ÿé¨“ {i+1}/{len(experiment_matrix)}: {exp_def['experiment_id']}")
                
                try:
                    result = self._execute_single_experiment(exp_def)
                    results.append(result)
                    
                    if self.config.save_intermediate:
                        self._save_intermediate_result(result, exp_def['experiment_id'])
                    
                except Exception as e:
                    error_msg = f"å®Ÿé¨“å¤±æ•— {exp_def['experiment_id']}: {e}"
                    logger.error(error_msg)
                    self.errors.append({
                        "experiment_id": exp_def['experiment_id'],
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
                
                pbar.update(1)
        
        logger.info(f"Phase 2å®Œäº†: {len(results)}å®Ÿé¨“æˆåŠŸ, {len(self.errors)}å®Ÿé¨“å¤±æ•—")
        return results
    
    def _execute_single_experiment(self, exp_def: Dict) -> Dict:
        """å˜ä¸€å®Ÿé¨“å®Ÿè¡Œ"""
        dataset_id = exp_def['dataset_id']
        domain = exp_def['domain']
        aspect = exp_def['aspect']
        shot_setting = exp_def['shot_setting']
        split_type = exp_def['split_type']
        group_size = exp_def['group_size']
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
        if domain:
            splits = self.dataset_manager.get_binary_splits(
                dataset_id=dataset_id,
                aspect=aspect,
                group_size=group_size,
                split_type=split_type,
                domain=domain
            )
        else:
            splits = self.dataset_manager.get_binary_splits(
                dataset_id=dataset_id,
                aspect=aspect,
                group_size=group_size,
                split_type=split_type
            )
        
        # Few-shotä¾‹é¡Œä½œæˆ
        examples = None
        if shot_setting > 0:
            try:
                examples = self.dataset_manager.create_examples(
                    dataset_id=dataset_id,
                    aspect=aspect,
                    shot_count=shot_setting,
                    domain=domain
                )
            except Exception as e:
                logger.warning(f"Few-shotä¾‹é¡Œä½œæˆå¤±æ•— {exp_def['experiment_id']}: {e}")
                examples = None
        
        # å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œ
        result = self.contrast_analyzer.analyze(
            group_a=splits.group_a,
            group_b=splits.group_b,
            correct_answer=splits.correct_answer,
            examples=examples,
            output_dir=self.config.output_dir,
            experiment_name=exp_def['experiment_id']
        )
        
        # å®Ÿé¨“å®šç¾©ã‚’çµæœã«è¿½åŠ 
        result['experiment_definition'] = exp_def
        
        return result
    
    def _save_intermediate_result(self, result: Dict, experiment_id: str):
        """ä¸­é–“çµæœä¿å­˜"""
        output_path = Path(self.config.output_dir) / f"{experiment_id}_{self.timestamp}.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    def phase3_results_analysis(self, phase1_result: Dict, phase2_results: List[Dict]) -> Dict:
        """Phase 3: çµæœä¿å­˜ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("=== Phase 3: çµæœåˆ†æãƒ»ä¿å­˜é–‹å§‹ ===")
        
        # 1. åŒ…æ‹¬çš„çµæœæ§‹é€ åŒ–
        comprehensive_results = {
            "experiment_info": {
                "experiment_name": self.config.experiment_name,
                "timestamp": self.timestamp,
                "total_experiments": len(phase2_results),
                "successful_experiments": len(phase2_results),
                "failed_experiments": len(self.errors),
                "target_datasets": self.config.target_datasets,
                "accessible_datasets": phase1_result['accessible_datasets'],
                "configuration": {
                    "group_size": self.config.group_size,
                    "shot_settings": self.config.shot_settings,
                    "random_seed": self.config.random_seed
                }
            },
            "data_preparation": phase1_result,
            "results": phase2_results,
            "errors": self.errors,
            "summary": self._generate_summary_statistics(phase2_results)
        }
        
        # 2. JSONçµæœä¿å­˜
        json_path = Path(self.config.output_dir) / f"unified_contrast_experiment_results_{self.timestamp}.json"
        json_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSONçµæœä¿å­˜: {json_path}")
        
        # 3. Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if self.config.generate_report:
            report_content = self._generate_markdown_report(comprehensive_results)
            report_path = Path(self.config.output_dir) / f"experiment_report_{self.timestamp}.md"
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
            comprehensive_results['report_path'] = str(report_path)
        
        comprehensive_results['json_path'] = str(json_path)
        logger.info("Phase 3å®Œäº†: çµæœåˆ†æãƒ»ä¿å­˜çµ‚äº†")
        
        return comprehensive_results
    
    def _generate_summary_statistics(self, results: List[Dict]) -> Dict:
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        if not results:
            return {"total_experiments": 0, "success_rate": 0.0}
        
        bert_scores = []
        bleu_scores = []
        dataset_stats = {}
        shot_stats = {}
        
        for result in results:
            # ã‚¹ã‚³ã‚¢åé›†
            eval_data = result.get('evaluation', {})
            bert_score = eval_data.get('bert_score')
            bleu_score = eval_data.get('bleu_score')
            
            if bert_score is not None:
                bert_scores.append(bert_score)
            if bleu_score is not None:
                bleu_scores.append(bleu_score)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµ±è¨ˆ
            exp_def = result.get('experiment_definition', {})
            dataset_id = exp_def.get('dataset_id', 'unknown')
            shot_setting = exp_def.get('shot_setting', 0)
            
            if dataset_id not in dataset_stats:
                dataset_stats[dataset_id] = []
            dataset_stats[dataset_id].append(result)
            
            if shot_setting not in shot_stats:
                shot_stats[shot_setting] = []
            shot_stats[shot_setting].append(result)
        
        # çµ±è¨ˆè¨ˆç®—
        summary = {
            "total_experiments": len(results),
            "success_rate": len(results) / (len(results) + len(self.errors)) if self.errors else 1.0,
            "score_statistics": {
                "bert_score": {
                    "count": len(bert_scores),
                    "mean": sum(bert_scores) / len(bert_scores) if bert_scores else 0.0,
                    "min": min(bert_scores) if bert_scores else 0.0,
                    "max": max(bert_scores) if bert_scores else 0.0
                },
                "bleu_score": {
                    "count": len(bleu_scores),
                    "mean": sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0,
                    "min": min(bleu_scores) if bleu_scores else 0.0,
                    "max": max(bleu_scores) if bleu_scores else 0.0
                }
            },
            "dataset_breakdown": {
                dataset_id: len(results_list) 
                for dataset_id, results_list in dataset_stats.items()
            },
            "shot_breakdown": {
                f"{shot}shot": len(results_list)
                for shot, results_list in shot_stats.items()
            }
        }
        
        return summary
    
    def _generate_markdown_report(self, comprehensive_results: Dict) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        exp_info = comprehensive_results['experiment_info']
        summary = comprehensive_results['summary']
        results = comprehensive_results['results']
        
        report = f"""# çµ±åˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿé¨“æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿé¨“å | {exp_info['experiment_name']} |
| å®Ÿè¡Œæ—¥æ™‚ | {exp_info['timestamp']} |
| ç·å®Ÿé¨“æ•° | {exp_info['total_experiments']} |
| æˆåŠŸå®Ÿé¨“æ•° | {exp_info['successful_experiments']} |
| å¤±æ•—å®Ÿé¨“æ•° | {exp_info['failed_experiments']} |
| æˆåŠŸç‡ | {summary['success_rate']:.1%} |

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | å®Ÿé¨“æ•° | åˆ©ç”¨å¯èƒ½ |
|-------------|-------|----------|
"""
        
        for dataset_id in exp_info['target_datasets']:
            count = summary['dataset_breakdown'].get(dataset_id, 0)
            accessible = "âœ…" if dataset_id in exp_info['accessible_datasets'] else "âŒ"
            report += f"| {dataset_id} | {count} | {accessible} |\n"
        
        report += f"""
## è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼

### BERTã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³é¡ä¼¼åº¦ãƒ»ä¸»è¦æŒ‡æ¨™ï¼‰
| çµ±è¨ˆé‡ | å€¤ |
|--------|-----|
| å¹³å‡ | {summary['score_statistics']['bert_score']['mean']:.4f} |
| æœ€å° | {summary['score_statistics']['bert_score']['min']:.4f} |
| æœ€å¤§ | {summary['score_statistics']['bert_score']['max']:.4f} |
| æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•° | {summary['score_statistics']['bert_score']['count']} |

### BLEUã‚¹ã‚³ã‚¢ï¼ˆè¡¨å±¤ä¸€è‡´åº¦ãƒ»ä¸»è¦æŒ‡æ¨™ï¼‰
| çµ±è¨ˆé‡ | å€¤ |
|--------|-----|
| å¹³å‡ | {summary['score_statistics']['bleu_score']['mean']:.4f} |
| æœ€å° | {summary['score_statistics']['bleu_score']['min']:.4f} |
| æœ€å¤§ | {summary['score_statistics']['bleu_score']['max']:.4f} |
| æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•° | {summary['score_statistics']['bleu_score']['count']} |

## Few-shotè¨­å®šåˆ¥çµæœ

| Shotè¨­å®š | å®Ÿé¨“æ•° |
|----------|-------|
"""
        
        for shot_key, count in summary['shot_breakdown'].items():
            report += f"| {shot_key} | {count} |\n"
        
        report += f"""
## å®Ÿé¨“çµæœè©³ç´°

| å®Ÿé¨“ID | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | Shotè¨­å®š | BERTã‚¹ã‚³ã‚¢ | BLEUã‚¹ã‚³ã‚¢ |
|--------|-------------|----------|----------|-----------|-----------|
"""
        
        for result in results[:20]:  # æœ€åˆã®20ä»¶ã®ã¿è¡¨ç¤º
            exp_def = result.get('experiment_definition', {})
            eval_data = result.get('evaluation', {})
            
            experiment_id = exp_def.get('experiment_id', 'N/A')
            dataset_id = exp_def.get('dataset_id', 'N/A')
            aspect = exp_def.get('aspect', 'N/A')
            shot_setting = exp_def.get('shot_setting', 'N/A')
            bert_score = eval_data.get('bert_score', 0.0)
            bleu_score = eval_data.get('bleu_score', 0.0)
            
            report += f"| {experiment_id} | {dataset_id} | {aspect} | {shot_setting}shot | {bert_score:.4f} | {bleu_score:.4f} |\n"
        
        if len(results) > 20:
            report += f"\n*ä»–{len(results)-20}å®Ÿé¨“ã®çµæœã¯çœç•¥*\n"
        
        if comprehensive_results['errors']:
            report += f"""
## ã‚¨ãƒ©ãƒ¼ãƒ»å¤±æ•—ã‚±ãƒ¼ã‚¹

| å®Ÿé¨“ID | ã‚¨ãƒ©ãƒ¼å†…å®¹ |
|--------|-----------|
"""
            for error in comprehensive_results['errors'][:10]:  # æœ€åˆã®10ä»¶ã®ã¿
                report += f"| {error['experiment_id']} | {error['error']} |\n"
        
        report += f"""
## å®Ÿé¨“çµè«–

ã“ã®å®Ÿé¨“ã§ã¯ã€æ–°ã—ã„utilsçµ±åˆãƒ„ãƒ¼ãƒ«ï¼ˆDatasetManagerãƒ»ContrastFactorAnalyzerï¼‰ã‚’æ´»ç”¨ã—ã¦ã€
{exp_info['total_experiments']}ã®å¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚

**ä¸»è¦æŒ‡æ¨™çµæœ:**
- **BERTã‚¹ã‚³ã‚¢å¹³å‡**: {summary['score_statistics']['bert_score']['mean']:.4f} (æ„å‘³é¡ä¼¼åº¦)
- **BLEUã‚¹ã‚³ã‚¢å¹³å‡**: {summary['score_statistics']['bleu_score']['mean']:.4f} (è¡¨å±¤ä¸€è‡´åº¦)

**æŠ€è¡“çš„æˆæœ:**
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†ã®å®Ÿç¾
- 3ãƒ•ã‚§ãƒ¼ã‚ºæ§‹é€ ã«ã‚ˆã‚‹çµ„ç¹”åŒ–ã•ã‚ŒãŸå®Ÿé¨“ãƒ•ãƒ­ãƒ¼
- è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¨ªæ–­ã§ã®çµ±ä¸€çš„åˆ†æ
- è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«ã‚ˆã‚‹çµæœå¯è¦–åŒ–

å®Ÿé¨“ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: `{comprehensive_results.get('json_path', 'N/A')}`
"""
        
        return report
    
    def run_full_experiment(self) -> Dict:
        """å®Œå…¨å®Ÿé¨“å®Ÿè¡Œ"""
        logger.info(f"çµ±åˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“é–‹å§‹: {self.config.experiment_name}")
        logger.info(f"æ¨å®šå®Ÿè¡Œæ™‚é–“: {self.config.get_estimated_time()}")
        
        try:
            # Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™
            phase1_result = self.phase1_data_preparation()
            
            # Phase 2: å¯¾æ¯”å®Ÿé¨“å®Ÿè¡Œ
            phase2_results = self.phase2_contrast_experiment(phase1_result)
            
            # Phase 3: çµæœåˆ†æãƒ»ä¿å­˜
            final_results = self.phase3_results_analysis(phase1_result, phase2_results)
            
            logger.info("=== å®Ÿé¨“å®Œäº† ===")
            logger.info(f"ç·å®Ÿé¨“æ•°: {final_results['experiment_info']['total_experiments']}")
            logger.info(f"æˆåŠŸå®Ÿé¨“æ•°: {final_results['experiment_info']['successful_experiments']}")
            logger.info(f"å¹³å‡BERTã‚¹ã‚³ã‚¢: {final_results['summary']['score_statistics']['bert_score']['mean']:.4f}")
            logger.info(f"å¹³å‡BLEUã‚¹ã‚³ã‚¢: {final_results['summary']['score_statistics']['bleu_score']['mean']:.4f}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"å®Ÿé¨“å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== çµ±åˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ï¼šæ–°ã—ã„utilsçµ±åˆãƒ„ãƒ¼ãƒ«æ´»ç”¨ç‰ˆ ===")
    
    # å®Ÿé¨“è¨­å®šç¢ºèª
    print(f"\nğŸ“‹ å®Ÿé¨“è¨­å®š:")
    print(f"  å®Ÿé¨“å: {CONFIG.experiment_name}")
    print(f"  å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {CONFIG.target_datasets}")
    print(f"  ç·å®Ÿé¨“æ•°: {CONFIG.get_total_experiments()}")
    print(f"  æ¨å®šå®Ÿè¡Œæ™‚é–“: {CONFIG.get_estimated_time()}")
    
    # å®Ÿé¨“å®Ÿè¡Œ
    experiment = UnifiedContrastExperiment(CONFIG)
    
    try:
        results = experiment.run_full_experiment()
        
        print(f"\nâœ… å®Ÿé¨“å®Œäº†!")
        print(f"  æˆåŠŸå®Ÿé¨“æ•°: {results['experiment_info']['successful_experiments']}")
        print(f"  å¹³å‡BERTã‚¹ã‚³ã‚¢: {results['summary']['score_statistics']['bert_score']['mean']:.4f}")
        print(f"  å¹³å‡BLEUã‚¹ã‚³ã‚¢: {results['summary']['score_statistics']['bleu_score']['mean']:.4f}")
        print(f"  çµæœãƒ•ã‚¡ã‚¤ãƒ«: {results.get('json_path', 'N/A')}")
        
        if 'report_path' in results:
            print(f"  ãƒ¬ãƒãƒ¼ãƒˆ: {results['report_path']}")
        
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“å¤±æ•—: {e}")
        logger.exception("å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 