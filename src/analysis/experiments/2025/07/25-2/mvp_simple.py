#!/usr/bin/env python3
"""
å¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ ç°¡ç•¥åŒ–MVPç‰ˆ

ä¾å­˜é–¢ä¿‚ã‚’æœ€å°é™ã«æŠ‘ãˆãŸå‹•ä½œç¢ºèªç”¨MVP
- åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®ã¿
- Steam Review ãƒ‡ãƒ¼ã‚¿ã®ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
- ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åº¦è¨ˆç®—
- JSONå‡ºåŠ›ã®åŸºæœ¬ãƒ•ãƒ­ãƒ¼æ¤œè¨¼
"""

import json
import logging
from datetime import datetime
from pathlib import Path

# è¨­å®šã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mvp_config import *

class SimpleMVPExperiment:
    """ç°¡ç•¥åŒ–MVPç‰ˆå¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“"""
    
    def __init__(self):
        self.setup_logging()
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        if DEBUG_MODE:
            logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def print_header(self):
        """å®Ÿé¨“é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
        if CONSOLE_OUTPUT:
            print("=== å¯¾æ¯”å› å­ç”Ÿæˆå®Ÿé¨“ ç°¡ç•¥åŒ–MVP ===")
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET}")
            print(f"å®Ÿé¨“ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {EXPERIMENT_VERSION}")
            print()
    
    def create_mock_data(self, aspect):
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        if aspect == "gameplay":
            group_a = [
                "Great gameplay mechanics and smooth controls",
                "Fun gameplay with engaging combat system", 
                "Excellent gameplay variety and progression"
            ] * (GROUP_SIZE // 3)
            
            group_b = [
                "Poor gameplay design and clunky mechanics",
                "Boring gameplay with repetitive tasks",
                "Frustrating gameplay and bad controls"
            ] * (GROUP_SIZE // 3)
            
            correct_answer = "Gameplay quality distinguishes positive from negative reviews"
            
        elif aspect == "visual":
            group_a = [
                "Beautiful graphics and stunning visual effects",
                "Amazing art style and visual presentation",
                "Gorgeous visuals and detailed environments"
            ] * (GROUP_SIZE // 3)
            
            group_b = [
                "Poor graphics and outdated visual quality",
                "Ugly art style and bland visuals",
                "Low quality graphics and poor visual design"
            ] * (GROUP_SIZE // 3)
            
            correct_answer = "Visual quality differentiates positive from negative reviews"
        
        return {
            'group_a': group_a[:GROUP_SIZE],
            'group_b': group_b[:GROUP_SIZE], 
            'correct_answer': correct_answer
        }
    
    def mock_llm_analysis(self, group_a, group_b):
        """LLMåˆ†æã®ãƒ¢ãƒƒã‚¯å®Ÿè£…"""
        # ç°¡å˜ãªåˆ†æçµæœã‚’ç”Ÿæˆ
        common_positive = ["good", "great", "excellent", "amazing", "beautiful"]
        common_negative = ["poor", "bad", "ugly", "boring", "frustrating"]
        
        a_words = " ".join(group_a[:5]).lower()
        b_words = " ".join(group_b[:5]).lower()
        
        pos_count = sum(word in a_words for word in common_positive)
        neg_count = sum(word in b_words for word in common_negative)
        
        return f"Analysis shows positive reviews emphasize quality ({pos_count} positive indicators) while negative reviews focus on problems ({neg_count} negative indicators)."
    
    def simple_similarity_score(self, text1, text2):
        """å˜ç´”ãªé¡ä¼¼åº¦è¨ˆç®—"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def run_aspect_experiment(self, aspect):
        """å˜ä¸€ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§ã®ç°¡ç•¥åŒ–å®Ÿé¨“"""
        if CONSOLE_OUTPUT:
            print(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspect}")
            print("[å®Ÿé¨“å®Ÿè¡Œ]")
        
        try:
            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = self.create_mock_data(aspect)
            
            if CONSOLE_OUTPUT:
                print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† (ã‚°ãƒ«ãƒ¼ãƒ—A: {len(data['group_a'])}ä»¶, ã‚°ãƒ«ãƒ¼ãƒ—B: {len(data['group_b'])}ä»¶)")
            
            # ãƒ¢ãƒƒã‚¯åˆ†æå®Ÿè¡Œ
            llm_response = self.mock_llm_analysis(data['group_a'], data['group_b'])
            
            if CONSOLE_OUTPUT:
                print("âœ… å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œå®Œäº†")
            
            # é¡ä¼¼åº¦è¨ˆç®—
            similarity_score = self.simple_similarity_score(llm_response, data['correct_answer'])
            
            # ãƒ€ãƒŸãƒ¼ã‚¹ã‚³ã‚¢ç”Ÿæˆï¼ˆå®Ÿéš›ã®BERT/BLEUã‚¹ã‚³ã‚¢é¢¨ï¼‰
            bert_score = max(0.6, min(0.9, similarity_score + 0.2))
            bleu_score = max(0.4, min(0.8, similarity_score + 0.1))
            
            if CONSOLE_OUTPUT:
                print("âœ… ã‚¹ã‚³ã‚¢è¨ˆç®—å®Œäº†")
                print()
                print("[çµæœ]")
                print(f"BERTã‚¹ã‚³ã‚¢: {bert_score:.4f}")
                print(f"BLEUã‚¹ã‚³ã‚¢: {bleu_score:.4f}")
            
            # å“è³ªè©•ä¾¡
            quality = self.evaluate_quality(bert_score, bleu_score)
            if CONSOLE_OUTPUT:
                print(f"å“è³ªè©•ä¾¡: {quality}")
            
            # çµæœæ§‹é€ åŒ–
            experiment_result = {
                "experiment_info": {
                    "timestamp": self.timestamp,
                    "dataset": DATASET,
                    "aspect": aspect,
                    "group_size": GROUP_SIZE,
                    "version": f"{EXPERIMENT_VERSION}-Simple",
                    "note": "Simplified MVP with mock data"
                },
                "results": {
                    "bert_score": bert_score,
                    "bleu_score": bleu_score,
                    "similarity_score": similarity_score,
                    "llm_response": llm_response,
                    "correct_answer": data['correct_answer']
                },
                "summary": {
                    "success": True,
                    "quality": quality
                }
            }
            
            # çµæœä¿å­˜
            filename = f"simple_mvp_{aspect}_{self.timestamp}.json"
            filepath = Path(OUTPUT_DIR) / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(experiment_result, f, ensure_ascii=False, indent=2)
            
            if CONSOLE_OUTPUT:
                print(f"ä¿å­˜å…ˆ: {filepath}")
                print("-" * 50)
                print()
            
            return experiment_result
            
        except Exception as e:
            self.logger.error(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆå®Ÿé¨“ã‚¨ãƒ©ãƒ¼ ({aspect}): {e}")
            return {
                "experiment_info": {
                    "timestamp": self.timestamp,
                    "dataset": DATASET,
                    "aspect": aspect,
                    "error": str(e)
                },
                "summary": {
                    "success": False,
                    "error": str(e)
                }
            }
    
    def evaluate_quality(self, bert_score, bleu_score):
        """å“è³ªè©•ä¾¡åˆ¤å®š"""
        avg_score = (bert_score + bleu_score) / 2
        
        if avg_score >= 0.7:
            return "good"
        elif avg_score >= 0.5:
            return "fair"
        else:
            return "poor"
    
    def run_experiment(self):
        """å®Ÿé¨“å…¨ä½“å®Ÿè¡Œ"""
        self.print_header()
        
        # å„ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§å®Ÿé¨“å®Ÿè¡Œ
        all_results = []
        
        for aspect in ASPECTS:
            result = self.run_aspect_experiment(aspect)
            all_results.append(result)
        
        # çµ±åˆçµæœä¿å­˜
        summary_result = {
            "experiment_meta": {
                "version": f"{EXPERIMENT_VERSION}-Simple",
                "timestamp": self.timestamp,
                "dataset": DATASET,
                "aspects": ASPECTS,
                "description": f"{DESCRIPTION} (Simplified with mock data)",
                "note": "This is a simplified MVP for testing basic flow"
            },
            "results": all_results
        }
        
        summary_file = Path(OUTPUT_DIR) / f"simple_mvp_summary_{self.timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_result, f, ensure_ascii=False, indent=2)
        
        if CONSOLE_OUTPUT:
            print(f"=== å®Ÿé¨“å®Œäº† ===")
            print(f"çµ±åˆçµæœ: {summary_file}")
        
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = SimpleMVPExperiment()
    success = experiment.run_experiment()
    
    if success:
        print("âœ… ç°¡ç•¥åŒ–MVPå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        print("ğŸ“ æ³¨æ„: ã“ã‚Œã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚¹ãƒˆã§ã™")
        return 0
    else:
        print("âŒ ç°¡ç•¥åŒ–MVPå®Ÿé¨“ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        return 1

if __name__ == "__main__":
    exit(main()) 