#!/usr/bin/env python3
"""
çµ±ä¸€å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œã®å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ã§ä¿å®ˆæ€§ã®é«˜ã„è¨­è¨ˆ
"""

import sys
import json
import yaml
import logging
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# utilsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
EXPERIMENTS_DIR = SCRIPT_DIR.parent.parent.parent
sys.path.insert(0, str(EXPERIMENTS_DIR))

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.datasetManager.dataset_manager import DatasetManager
from utils.cfGenerator.contrast_factor_analyzer import ContrastFactorAnalyzer
from utils.scores.get_score import calculate_scores


class ExperimentPipeline:
    """çµ±ä¸€å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_path: str, debug: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.debug = debug
        self.setup_logging()
        
        self.dataset_manager = None
        self.results = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = self._derive_run_name()
        self.run_dir: Optional[Path] = None

    def _derive_run_name(self) -> str:
        """å®Ÿè¡Œåã‚’æ±ºå®šï¼ˆconfigã®run_name > è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰"""
        try:
            name_from_config = self.config.get('run_name')
            if isinstance(name_from_config, str) and name_from_config.strip():
                return name_from_config.strip()
        except Exception:
            pass
        return self.config_path.stem
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_level = logging.INFO if self.debug else logging.WARNING
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def _attach_file_logger(self) -> None:
        """å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼ã‚’å–ã‚Šä»˜ã‘ã‚‹"""
        if self.run_dir is None:
            return
        log_dir = self.run_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        from logging import FileHandler, Formatter
        fh = FileHandler(log_dir / "python.log", encoding="utf-8")
        fh.setLevel(logging.DEBUG if self.debug else logging.INFO)
        fmt = Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
        fh.setFormatter(fmt)
        root = logging.getLogger()
        # é‡è¤‡è¿½åŠ é˜²æ­¢
        if not any(getattr(h, 'baseFilename', '').endswith("python.log") for h in root.handlers):
            root.addHandler(fh)
    
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def validate_config(self) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼"""
        self.logger.info("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ä¸­...")
        
        # å¿…é ˆã‚­ãƒ¼ã®ç¢ºèª
        required_keys = ['experiments', 'output', 'llm']
        for key in required_keys:
            if key not in self.config:
                self.logger.error(f"å¿…é ˆã‚­ãƒ¼ '{key}' ãŒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ã‚Šã¾ã›ã‚“")
                return False
        
        # å®Ÿé¨“è¨­å®šã®ç¢ºèª
        if not self.config['experiments']:
            self.logger.error("å®Ÿé¨“è¨­å®šãŒç©ºã§ã™")
            return False
        
        self.logger.info("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†")
        return True
    
    def setup_dataset_manager(self):
        """DatasetManageråˆæœŸåŒ–"""
        self.logger.info("DatasetManagerã‚’åˆæœŸåŒ–ä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’æŒ‡å®š
            data_root = Path("/Users/seinoshun/imrb_research/data/external")
            self.dataset_manager = DatasetManager(data_root=data_root)
            
            self.logger.info("âœ… DatasetManageråˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DatasetManageråˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def run_single_experiment(
        self, 
        dataset: str, 
        aspect: str, 
        group_size: int,
        split_type: str = "aspect_vs_others",
        output_dir: Optional[Path] = None
    ) -> Optional[Dict]:
        """
        å˜ä¸€å®Ÿé¨“å®Ÿè¡Œ
        
        Args:
            dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            aspect: ã‚¢ã‚¹ãƒšã‚¯ãƒˆå
            group_size: ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º
            split_type: åˆ†å‰²ã‚¿ã‚¤ãƒ—
            
        Returns:
            å®Ÿé¨“çµæœè¾æ›¸
        """
        experiment_id = f"{dataset}_{aspect}_{self.timestamp}"
        
        self.logger.info(f"\n{'='*50}")
        self.logger.info(f"å®Ÿé¨“é–‹å§‹: {experiment_id}")
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset}")
        self.logger.info(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspect}")
        self.logger.info(f"ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º: {group_size}")
        self.logger.info(f"åˆ†å‰²ã‚¿ã‚¤ãƒ—: {split_type}")
        
        try:
            # [1/3] ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
            self.logger.info(f"\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ç¢ºèª
            try:
                records = self.dataset_manager.load_dataset(dataset)
                self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {len(records)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ†å¸ƒç¢ºèª
                aspect_records = [r for r in records if r.aspect == aspect]
                self.logger.info(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆ '{aspect}' ã®ãƒ¬ã‚³ãƒ¼ãƒ‰: {len(aspect_records)}ä»¶")
                
            except Exception as e:
                self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                raise
            
            splits = self.dataset_manager.split_dataset(
                dataset_id=dataset,
                aspect=aspect,
                group_size=group_size,
                split_type=split_type
            )
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº† (A: {len(splits.group_a)}ä»¶, B: {len(splits.group_b)}ä»¶)")
            self.logger.info(f"æ­£è§£ãƒ©ãƒ™ãƒ«: {splits.correct_answer}")
            
            # [2/3] å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œ
            self.logger.info(f"\n[2/3] å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œä¸­...")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆå®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ï¼‰
            out_dir = output_dir if output_dir is not None else Path(self.config['output']['directory'])
            out_dir.mkdir(parents=True, exist_ok=True)
            
            analyzer = ContrastFactorAnalyzer(debug=self.debug)
            
            result = analyzer.analyze(
                group_a=splits.group_a,
                group_b=splits.group_b,
                correct_answer=splits.correct_answer,
                output_dir=str(out_dir),
                experiment_name=experiment_id
            )
            
            self.logger.info("âœ… LLMå¿œç­”å–å¾—å®Œäº†")
            
            # [3/3] ã‚¹ã‚³ã‚¢ç¢ºèªï¼ˆanalyzersã§æ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
            self.logger.info(f"\n[3/3] ã‚¹ã‚³ã‚¢ç¢ºèªä¸­...")
            
            bert_score = result['evaluation']['bert_score']
            bleu_score = result['evaluation']['bleu_score']
            llm_response = result['process']['llm_response']
            
            self.logger.info("âœ… ã‚¹ã‚³ã‚¢ç¢ºèªå®Œäº†")
            
            # çµæœè¡¨ç¤º
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"=== çµæœ ===")
            self.logger.info(f"BERTã‚¹ã‚³ã‚¢: {bert_score:.4f}")
            self.logger.info(f"BLEUã‚¹ã‚³ã‚¢: {bleu_score:.4f}")
            self.logger.info(f"LLMå¿œç­”: {llm_response}")
            self.logger.info(f"å“è³ªè©•ä¾¡: {result['summary']['quality_assessment']['overall_quality']}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            result['experiment_info']['dataset'] = dataset
            result['experiment_info']['aspect'] = aspect
            result['experiment_info']['group_size'] = group_size
            result['experiment_info']['split_type'] = split_type
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ å®Ÿé¨“å¤±æ•— ({experiment_id}): {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "experiment_info": {
                    "experiment_id": experiment_id,
                    "dataset": dataset,
                    "aspect": aspect,
                    "error": str(e)
                },
                "summary": {
                    "success": False,
                    "error": str(e)
                }
            }
    
    def run_batch_experiments(self) -> List[Dict]:
        """ãƒãƒƒãƒå®Ÿé¨“å®Ÿè¡Œ"""
        self.logger.info("=" * 60)
        self.logger.info("ãƒãƒƒãƒå®Ÿé¨“é–‹å§‹")
        self.logger.info("=" * 60)
        
        all_results = []
        
        # å®Ÿè¡Œç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆç›´ä¸‹ã®resultsã«æ™‚åˆ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆï¼ˆexperiments/{YYYY}/{MM}/{DD}/results/æ™‚åˆ»ï¼‰
        base_output_dir = SCRIPT_DIR / "results"
        base_output_dir.mkdir(parents=True, exist_ok=True)
        self.run_dir = base_output_dir / f"{self.timestamp}"
        self.run_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.run_dir}")
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼å–ã‚Šä»˜ã‘
        self._attach_file_logger()

        # å„å®Ÿé¨“è¨­å®šã‚’å®Ÿè¡Œ
        for exp_config in self.config['experiments']:
            dataset = exp_config['dataset']
            aspects = exp_config['aspects']
            group_size = exp_config.get('group_size', 100)
            split_type = exp_config.get('split_type', 'aspect_vs_others')
            
            self.logger.info(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset}")
            self.logger.info(f"å¯¾è±¡ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspects}")
            
            # å„ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§å®Ÿé¨“å®Ÿè¡Œ
            for aspect in aspects:
                result = self.run_single_experiment(
                    dataset=dataset,
                    aspect=aspect,
                    group_size=group_size,
                    split_type=split_type,
                    output_dir=self.run_dir
                )
                
                if result:
                    all_results.append(result)
        
        self.results = all_results
        return all_results
    
    def save_results(self, results: Optional[List[Dict]] = None) -> str:
        """çµæœä¿å­˜"""
        if results is None:
            results = self.results
        
        if not results:
            self.logger.warning("ä¿å­˜ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return ""
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå®Ÿè¡Œç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ï¼‰
        if self.run_dir is None:
            base_output_dir = SCRIPT_DIR / "results"
            self.run_dir = base_output_dir / f"{self.timestamp}"
            self.run_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±åˆçµæœä½œæˆ
        summary = {
            "experiment_meta": {
                "timestamp": self.timestamp,
                "config_file": str(self.config_path),
                "total_experiments": len(results),
                "successful_experiments": sum(
                    1 for r in results if r.get('summary', {}).get('success', False)
                ),
                "run_name": self.run_name,
                "output_dir": str(self.run_dir)
            },
            "results": results
        }
        
        # ä¿å­˜
        filename = f"batch_experiment_{self.timestamp}.json"
        filepath = self.run_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"\nğŸ“ çµæœä¿å­˜: {filepath}")

        # å®Ÿè¡Œæ™‚è¨­å®šã®ä¿å­˜ï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰
        self._save_run_configuration()

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆï¼ˆè©³ç´°ï¼‰
        self._write_markdown_summary(summary)
        # ãƒ«ãƒ¼ãƒˆresultsã«æ¦‚è¦ã‚’ä½œæˆ
        self._write_root_overview(summary)
        
        return str(filepath)

    def _save_run_configuration(self) -> None:
        """å®Ÿè¡Œæ™‚è¨­å®šï¼ˆæœ‰åŠ¹å€¤ï¼‰ã‚’ä¿å­˜"""
        if self.run_dir is None:
            return
        # è¨­å®šã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆå…ƒYAMLï¼‰
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                original_yaml = f.read()
            with open(self.run_dir / 'pipeline_config_snapshot.yaml', 'w', encoding='utf-8') as f:
                f.write(original_yaml)
        except Exception:
            pass
        # å®Ÿè¡Œãƒ¡ã‚¿ï¼ˆJSONï¼‰
        effective = {
            "timestamp": self.timestamp,
            "run_name": self.run_name,
            "output_dir": str(self.run_dir),
            "config_path": str(self.config_path),
            "experiments": self.config.get('experiments', []),
            "llm": self.config.get('llm', {}),
            "general": self.config.get('general', {})
        }
        with open(self.run_dir / 'run_effective_config.json', 'w', encoding='utf-8') as f:
            json.dump(effective, f, ensure_ascii=False, indent=2)

    def _write_markdown_summary(self, summary_data: Dict) -> None:
        """è¨­å®šå€¤ã¨çµæœæ¦‚è¦ã®Markdownã‚’ä½œæˆ"""
        if self.run_dir is None:
            return
        lines = []
        meta = summary_data.get('experiment_meta', {})
        results = summary_data.get('results', [])
        lines.append(f"# å®Ÿé¨“ã‚µãƒãƒªãƒ¼: {self.run_name}")
        lines.append("")
        lines.append(f"- å®Ÿè¡Œæ™‚åˆ»: {meta.get('timestamp', '')}")
        lines.append(f"- å‡ºåŠ›å…ˆ: {meta.get('output_dir', '')}")
        lines.append(f"- ç·å®Ÿé¨“æ•°: {meta.get('total_experiments', 0)}")
        lines.append(f"- æˆåŠŸæ•°: {meta.get('successful_experiments', 0)}")
        lines.append("")
        # è¨­å®š
        lines.append("## è¨­å®š")
        lines.append("")
        lines.append("```yaml")
        try:
            with open(self.run_dir / 'pipeline_config_snapshot.yaml', 'r', encoding='utf-8') as f:
                lines.append(f.read())
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç°¡æ˜“è¨­å®š
            lines.append(yaml.safe_dump(self.config, allow_unicode=True))
        lines.append("```")
        lines.append("")
        # çµæœä¸€è¦§
        lines.append("## çµæœæ¦‚è¦")
        lines.append("")
        lines.append("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | BERT | BLEU | å“è³ª | å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« |")
        lines.append("| --- | --- | ---:| ---:| --- | --- |")
        for r in results:
            if not r.get('summary', {}).get('success', False):
                continue
            info = r.get('experiment_info', {})
            evals = r.get('evaluation', {})
            out_file = r.get('output_file', '')
            lines.append(
                f"| {info.get('dataset','')} | {info.get('aspect','')} | "
                f"{evals.get('bert_score',0):.4f} | {evals.get('bleu_score',0):.4f} | "
                f"{r.get('summary',{}).get('quality_assessment',{}).get('overall_quality','')} | "
                f"{Path(out_file).name if out_file else ''} |"
            )
        lines.append("")
        # ãƒ­ã‚°ãƒªãƒ³ã‚¯
        lines.append("## ãƒ­ã‚°")
        lines.append("")
        try:
            rel_python_log = os.path.relpath(self.run_dir / 'logs/python.log', self.run_dir)
            rel_cli_log = os.path.relpath(self.run_dir / 'logs/cli_run.log', self.run_dir)
            lines.append(f"- Pythonãƒ­ã‚°: {rel_python_log}")
            lines.append(f"- CLIãƒ­ã‚°: {rel_cli_log}")
        except Exception:
            pass
        lines.append("")
        # ä¿å­˜
        md_path = self.run_dir / 'summary.md'
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))

    def _write_root_overview(self, summary_data: Dict) -> None:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆresults/ã«æ¦‚è¦Markdownã‚’ä¿å­˜ã—ã€è©³ç´°ã¸ã®ãƒ‘ã‚¹ã‚’è¨˜è¼‰"""
        try:
            root_dir = SCRIPT_DIR.parents[5] / 'results'
        except Exception:
            return
        root_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨æ„ï¼ˆãªã‘ã‚Œã°ç”Ÿæˆï¼‰
        template_dir = root_dir / 'template'
        template_dir.mkdir(parents=True, exist_ok=True)
        template_path = template_dir / 'root_overview_template.md'
        if not template_path.exists():
            default_tpl = []
            # èª¬æ˜ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆç”Ÿæˆç‰©ã«ã¯å«ã‚ãªã„ï¼‰
            default_tpl.append("<!-- ãƒ†ãƒ³ãƒ—ãƒ¬èª¬æ˜: ä¸‹è¨˜ã®è¨˜å·ã¯å®Ÿè¡Œæ™‚ã«ç½®æ›ã•ã‚Œã¾ã™ï¼ˆã“ã®ã‚³ãƒ¡ãƒ³ãƒˆã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ï¼‰ -->")
            default_tpl.append("<!-- {{TIMESTAMP}}: å®Ÿè¡Œæ™‚åˆ»(YYYYMMDD_HHMMSS) -->")
            default_tpl.append("<!-- {{RUN_NAME}}: å®Ÿé¨“åï¼ˆconfigã®run_nameã€‚æœªè¨­å®šæ™‚ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ -->")
            default_tpl.append("<!-- {{DETAIL_DIR_PATH}}: è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ç›¸å¯¾ãƒ‘ã‚¹ -->")
            default_tpl.append("<!-- {{DETAIL_SUMMARY_PATH}}: è©³ç´°summary.mdã¸ã®ç›¸å¯¾ãƒ‘ã‚¹ -->")
            default_tpl.append("<!-- {{DETAIL_DIR_MD_LINK}}: [è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª](ç›¸å¯¾ãƒ‘ã‚¹) ã¸ã®ãƒªãƒ³ã‚¯ -->")
            default_tpl.append("<!-- {{DETAIL_SUMMARY_MD_LINK}}: [è©³ç´°ã‚µãƒãƒªãƒ¼](ç›¸å¯¾ãƒ‘ã‚¹) ã¸ã®ãƒªãƒ³ã‚¯ -->")
            default_tpl.append("<!-- {{TOTAL_EXPERIMENTS}}: ç·å®Ÿé¨“æ•° / {{SUCCESSFUL_EXPERIMENTS}}: æˆåŠŸæ•° -->")
            default_tpl.append("<!-- {{RESULTS_TABLE}}: å…ˆé ­æ•°ä»¶ã®çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚¢ã‚¹ãƒšã‚¯ãƒˆ/BERT/BLEUï¼‰ -->")
            # è¿½åŠ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼ˆä»»æ„ã§ä½¿ç”¨å¯ï¼‰
            default_tpl.append("<!-- è¿½åŠ : {{DATASET_LIST}}ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{ASPECT_LIST}}ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¢ã‚¹ãƒšã‚¯ãƒˆã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{DETAIL_DIR_ABS}}ï¼ˆè©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{CONFIG_PATH}}ï¼ˆä½¿ç”¨ã—ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{RUN_DIR_NAME}}ï¼ˆè©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®ã¿ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{LLM_MODEL}}ï¼ˆè¨­å®šã®llm.modelï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{RESULT_JSON_PATH}}ï¼ˆãƒãƒƒãƒçµæœJSONã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰ -->\n")
            default_tpl.append("# å®Ÿé¨“æ¦‚è¦ {{TIMESTAMP}}")
            default_tpl.append("")
            default_tpl.append("- å®Ÿé¨“å: {{RUN_NAME}}")
            default_tpl.append("- {{DETAIL_DIR_MD_LINK}}")
            default_tpl.append("- {{DETAIL_SUMMARY_MD_LINK}}")
            default_tpl.append("- ç·å®Ÿé¨“æ•°: {{TOTAL_EXPERIMENTS}} / æˆåŠŸ: {{SUCCESSFUL_EXPERIMENTS}}\n")
            default_tpl.append("- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {{DATASET_LIST}}")
            default_tpl.append("- ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {{ASPECT_LIST}}\n")
            default_tpl.append("## çµæœæ¦‚è¦")
            default_tpl.append("{{RESULTS_TABLE}}")
            with open(template_path, 'w', encoding='utf-8') as tf:
                tf.write("\n".join(default_tpl))

        # ç½®æ›ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        meta = summary_data.get('experiment_meta', {})
        results = summary_data.get('results', [])
        rel_detail_dir = os.path.relpath(self.run_dir, root_dir) if self.run_dir else ''
        rel_detail_summary = os.path.relpath(self.run_dir / 'summary.md', root_dir) if self.run_dir else ''
        results_table = self._build_results_table(results, limit=5)
        # è¿½åŠ å¤‰æ•°ã‚’æ§‹ç¯‰
        datasets = sorted({(r.get('experiment_info') or {}).get('dataset', '') for r in results if r.get('experiment_info')})
        aspects = sorted({(r.get('experiment_info') or {}).get('aspect', '') for r in results if r.get('experiment_info')})
        dataset_list = ", ".join([d for d in datasets if d])
        aspect_list = ", ".join([a for a in aspects if a])
        detail_dir_abs = str(self.run_dir) if self.run_dir else ''
        config_path = str(self.config_path)
        run_dir_name = self.run_dir.name if self.run_dir else ''
        llm_model = (self.config.get('llm') or {}).get('model', '')
        result_json_rel = os.path.relpath(self.run_dir / f"batch_experiment_{meta.get('timestamp','')}.json", root_dir) if self.run_dir else ''
        rel_log_dir = os.path.relpath(self.run_dir / 'logs', root_dir) if self.run_dir else ''
        rel_cli_log = os.path.relpath(self.run_dir / 'logs/cli_run.log', root_dir) if self.run_dir else ''

        with open(template_path, 'r', encoding='utf-8') as tf:
            template = tf.read()

        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ç½®æ›
        rendered = template
        rendered = rendered.replace('{{TIMESTAMP}}', str(meta.get('timestamp', '')))
        rendered = rendered.replace('{{RUN_NAME}}', str(self.run_name))
        rendered = rendered.replace('{{DETAIL_DIR_PATH}}', rel_detail_dir)
        rendered = rendered.replace('{{DETAIL_SUMMARY_PATH}}', rel_detail_summary)
        rendered = rendered.replace('{{DETAIL_DIR_MD_LINK}}', f"[è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª]({rel_detail_dir})")
        rendered = rendered.replace('{{DETAIL_SUMMARY_MD_LINK}}', f"[è©³ç´°ã‚µãƒãƒªãƒ¼]({rel_detail_summary})")
        rendered = rendered.replace('{{TOTAL_EXPERIMENTS}}', str(meta.get('total_experiments', 0)))
        rendered = rendered.replace('{{SUCCESSFUL_EXPERIMENTS}}', str(meta.get('successful_experiments', 0)))
        rendered = rendered.replace('{{RESULTS_TABLE}}', results_table)
        # è¿½åŠ ç½®æ›
        rendered = rendered.replace('{{DATASET_LIST}}', dataset_list)
        rendered = rendered.replace('{{ASPECT_LIST}}', aspect_list)
        rendered = rendered.replace('{{DETAIL_DIR_ABS}}', detail_dir_abs)
        rendered = rendered.replace('{{CONFIG_PATH}}', config_path)
        rendered = rendered.replace('{{RUN_DIR_NAME}}', run_dir_name)
        rendered = rendered.replace('{{LLM_MODEL}}', llm_model)
        rendered = rendered.replace('{{RESULT_JSON_PATH}}', result_json_rel)
        rendered = rendered.replace('{{LOG_DIR_PATH}}', rel_log_dir)
        rendered = rendered.replace('{{CLI_LOG_PATH}}', rel_cli_log)
        rendered = rendered.replace('{{CLI_LOG_MD_LINK}}', f"[CLIãƒ­ã‚°]({rel_cli_log})")

        # å‡ºåŠ›
        overview_path = root_dir / f"summary_{meta.get('timestamp','')}.md"
        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œ(<!-- -->)ã¯æ¦‚è¦ã«ã¯å«ã‚ãªã„
        rendered_lines = [ln for ln in rendered.splitlines() if not ln.strip().startswith('<!--')]
        with open(overview_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(rendered_lines))

    def _build_results_table(self, results: List[Dict], limit: int = 5) -> str:
        """çµæœãƒ†ãƒ¼ãƒ–ãƒ«Markdownã‚’ä½œæˆ"""
        lines = []
        lines.append("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | BERT | BLEU |")
        lines.append("| --- | --- | ---:| ---:|")
        for r in results[:limit]:
            info = r.get('experiment_info', {})
            evals = r.get('evaluation', {})
            lines.append(
                f"| {info.get('dataset','')} | {info.get('aspect','')} | {evals.get('bert_score',0):.4f} | {evals.get('bleu_score',0):.4f} |"
            )
        return "\n".join(lines)
    
    def print_summary(self):
        """å®Ÿé¨“ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not self.results:
            self.logger.info("å®Ÿé¨“çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("=== å®Ÿé¨“ã‚µãƒãƒªãƒ¼ ===")
        self.logger.info("=" * 60)
        
        total = len(self.results)
        successful = sum(1 for r in self.results if r.get('summary', {}).get('success', False))
        
        self.logger.info(f"ç·å®Ÿé¨“æ•°: {total}")
        self.logger.info(f"æˆåŠŸ: {successful}")
        self.logger.info(f"å¤±æ•—: {total - successful}")
        
        # ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼
        self.logger.info("\n=== ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼ ===")
        for result in self.results:
            if result.get('summary', {}).get('success', False):
                exp_info = result['experiment_info']
                evaluation = result['evaluation']
                
                dataset = exp_info.get('dataset', 'N/A')
                aspect = exp_info.get('aspect', 'N/A')
                bert = evaluation.get('bert_score', 0)
                bleu = evaluation.get('bleu_score', 0)
                
                self.logger.info(f"{dataset:10s} {aspect:15s} BERT: {bert:.4f}  BLEU: {bleu:.4f}")
    
    def run(self) -> bool:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            # è¨­å®šæ¤œè¨¼
            if not self.validate_config():
                return False
            
            # DatasetManageråˆæœŸåŒ–
            if not self.setup_dataset_manager():
                return False
            
            # ãƒãƒƒãƒå®Ÿé¨“å®Ÿè¡Œ
            results = self.run_batch_experiments()
            
            # çµæœä¿å­˜
            self.save_results(results)
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self.print_summary()
            
            self.logger.info("\nâœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    default_config = Path(__file__).parent / "pipeline_config.yaml"
    
    pipeline = ExperimentPipeline(str(default_config), debug=True)
    success = pipeline.run()
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())

