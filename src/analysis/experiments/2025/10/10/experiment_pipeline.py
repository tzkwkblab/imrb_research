#!/usr/bin/env python3
"""
çµ±ä¸€å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œã®å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•ã§ä¿å®ˆæ€§ã®é«˜ã„è¨­è¨ˆ
"""

import sys
import json
import yaml
import logging
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# utilsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
EXPERIMENTS_DIR = SCRIPT_DIR.parent.parent.parent
sys.path.insert(0, str(EXPERIMENTS_DIR))

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.datasetManager.dataset_manager import DatasetManager
from utils.cfGenerator.contrast_factor_analyzer import ContrastFactorAnalyzer
from utils.scores.get_score import calculate_scores
from utils.coco_image_url_converter import convert_coco_path_to_url


class ExperimentPipeline:
    """çµ±ä¸€å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_path: str, debug: bool = True, silent: bool = False):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()

        general_cfg = self.config.setdefault('general', {}) or {}

        self.debug = bool(debug)
        general_cfg['debug_mode'] = self.debug

        self.silent = bool(silent or general_cfg.get('silent_mode', False))
        general_cfg['silent_mode'] = self.silent

        console_flag = general_cfg.get('console_output')
        self.console_output = False if self.silent else bool(console_flag if console_flag is not None else True)
        general_cfg['console_output'] = self.console_output

        output_cfg = self.config.setdefault('output', {}) or {}
        if self.silent:
            output_cfg['save_intermediate'] = False

        self.setup_logging()
        
        self.dataset_manager = None
        self.results = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = self._derive_run_name()
        self.run_dir: Optional[Path] = None
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆèª¬æ˜CSVã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ {csv_path: {aspect: description}}
        self._desc_cache: Dict[str, Dict[str, str]] = {}
        # ä¾‹é¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ {path: List[Dict]}
        self._examples_cache: Dict[str, List[Dict]] = {}

    def _get_dated_results_base(self) -> Path:
        """æ—¥ä»˜(YYYY/MM/DD)ã«åŸºã¥ãçµæœãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™ã€‚
        ä¾‹: src/analysis/experiments/2025/10/21/results
        """
        try:
            y = self.timestamp[0:4]
            m = self.timestamp[4:6]
            d = self.timestamp[6:8]
        except Exception:
            from datetime import datetime as _dt
            now = _dt.now()
            y = f"{now.year:04d}"
            m = f"{now.month:02d}"
            d = f"{now.day:02d}"
        return EXPERIMENTS_DIR / y / m / d / "results"

    def _derive_run_name(self) -> str:
        """å®Ÿè¡Œåã‚’æ±ºå®šï¼ˆconfigã®run_name > è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰"""
        try:
            name_from_config = self.config.get('run_name')
            if isinstance(name_from_config, str) and name_from_config.strip():
                return name_from_config.strip()
        except Exception:
            pass
        return self.config_path.stem
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_level = logging.INFO if self.debug else logging.WARNING
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def _attach_file_logger(self) -> None:
        """å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼ã‚’å–ã‚Šä»˜ã‘ã‚‹"""
        if self.run_dir is None or self.silent:
            return
        log_dir = self.run_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        from logging import FileHandler, Formatter
        fh = FileHandler(log_dir / "python.log", encoding="utf-8")
        fh.setLevel(logging.DEBUG if self.debug else logging.INFO)
        fmt = Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
        fh.setFormatter(fmt)
        root = logging.getLogger()
        # é‡è¤‡è¿½åŠ é˜²æ­¢
        if not any(getattr(h, 'baseFilename', '').endswith("python.log") for h in root.handlers):
            root.addHandler(fh)
    
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def validate_config(self) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼"""
        self.logger.info("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ä¸­...")
        
        # å¿…é ˆã‚­ãƒ¼ã®ç¢ºèª
        required_keys = ['experiments', 'output', 'llm']
        for key in required_keys:
            if key not in self.config:
                self.logger.error(f"å¿…é ˆã‚­ãƒ¼ '{key}' ãŒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ã‚Šã¾ã›ã‚“")
                return False
        
        # å®Ÿé¨“è¨­å®šã®ç¢ºèª
        if not self.config['experiments']:
            self.logger.error("å®Ÿé¨“è¨­å®šãŒç©ºã§ã™")
            return False
        
        self.logger.info("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†")
        return True
    
    def setup_dataset_manager(self):
        """DatasetManageråˆæœŸåŒ–"""
        self.logger.info("DatasetManagerã‚’åˆæœŸåŒ–ä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’æŒ‡å®š
            data_root = Path("/Users/seinoshun/imrb_research/data/external")
            self.dataset_manager = DatasetManager(data_root=data_root)
            
            self.logger.info("âœ… DatasetManageråˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DatasetManageråˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def run_single_experiment(
        self, 
        dataset: str, 
        aspect: str, 
        group_size: int,
        split_type: str = "aspect_vs_others",
        output_dir: Optional[Path] = None
    ) -> Optional[Dict]:
        """
        å˜ä¸€å®Ÿé¨“å®Ÿè¡Œ
        
        Args:
            dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            aspect: ã‚¢ã‚¹ãƒšã‚¯ãƒˆå
            group_size: ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º
            split_type: åˆ†å‰²ã‚¿ã‚¤ãƒ—
            
        Returns:
            å®Ÿé¨“çµæœè¾æ›¸
        """
        experiment_id = f"{dataset}_{aspect}_{self.timestamp}"
        
        self.logger.info(f"\n{'='*50}")
        self.logger.info(f"å®Ÿé¨“é–‹å§‹: {experiment_id}")
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset}")
        self.logger.info(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspect}")
        self.logger.info(f"ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º: {group_size}")
        self.logger.info(f"åˆ†å‰²ã‚¿ã‚¤ãƒ—: {split_type}")
        
        try:
            # [1/3] ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å–å¾—
            self.logger.info(f"\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ç¢ºèª
            try:
                records = self.dataset_manager.load_dataset(dataset)
                self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {len(records)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ†å¸ƒç¢ºèª
                aspect_records = [r for r in records if r.aspect == aspect]
                self.logger.info(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆ '{aspect}' ã®ãƒ¬ã‚³ãƒ¼ãƒ‰: {len(aspect_records)}ä»¶")
                
            except Exception as e:
                self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                raise
            
            splits = self.dataset_manager.split_dataset(
                dataset_id=dataset,
                aspect=aspect,
                group_size=group_size,
                split_type=split_type
            )
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº† (A: {len(splits.group_a)}ä»¶, B: {len(splits.group_b)}ä»¶)")
            self.logger.info(f"æ­£è§£ãƒ©ãƒ™ãƒ«: {splits.correct_answer}")
            
            # [2/3] å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œ
            self.logger.info(f"\n[2/3] å¯¾æ¯”å› å­åˆ†æå®Ÿè¡Œä¸­...")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆå®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ï¼‰
            out_dir = None
            if output_dir is not None:
                out_dir = Path(output_dir)
                out_dir.mkdir(parents=True, exist_ok=True)
            elif not self.silent:
                out_dir = Path(self.config['output']['directory'])
                out_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸€èˆ¬è¨­å®šã‹ã‚‰èª¬æ˜æ–‡åˆ©ç”¨ãƒ•ãƒ©ã‚°ã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
            general_cfg = self.config.get('general', {}) or {}
            use_desc = bool(general_cfg.get('use_aspect_descriptions', False))
            desc_file = general_cfg.get('aspect_descriptions_file')
            # ä¾‹é¡Œè¨­å®š
            use_examples = bool(general_cfg.get('use_examples', False))
            examples_file = general_cfg.get('examples_file')
            max_examples = general_cfg.get('max_examples')
            if isinstance(max_examples, str):
                try:
                    max_examples = int(max_examples) if max_examples.strip() else None
                except Exception:
                    max_examples = None
            
            # è©•ä¾¡è¨­å®šã‹ã‚‰LLMè©•ä¾¡è¨­å®šã‚’å–å¾—
            evaluation_cfg = self.config.get('evaluation', {}) or {}
            use_llm_eval = bool(evaluation_cfg.get('use_llm_score', False))
            llm_eval_model = evaluation_cfg.get('llm_evaluation_model', 'gpt-4o-mini')
            llm_eval_temp = float(evaluation_cfg.get('llm_evaluation_temperature', 0.0))

            analyzer = ContrastFactorAnalyzer(
                debug=self.debug, 
                use_aspect_descriptions=use_desc,
                use_llm_evaluation=use_llm_eval,
                llm_evaluation_model=llm_eval_model,
                llm_evaluation_temperature=llm_eval_temp
            )
            
            # ä¾‹é¡Œèª­ã¿è¾¼ã¿ï¼ˆå¿…è¦æ™‚ï¼‰
            examples_payload: Optional[List[Dict]] = None
            if use_examples and examples_file:
                examples_all = self._load_examples_file(str(examples_file))
                if isinstance(max_examples, int) and max_examples > 0:
                    examples_payload = examples_all[:max_examples]
                else:
                    examples_payload = examples_all

            # ç”»åƒURLæƒ…å ±ã‚’å–å¾—ï¼ˆretrieved_conceptsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã®ã¿ï¼‰
            group_a_top5_image_urls = None
            group_b_top5_image_urls = None
            if dataset == "retrieved_concepts" and splits.metadata:
                # additional_metadataã¯ç›´æ¥metadataã«ãƒãƒ¼ã‚¸ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç›´æ¥å–å¾—
                group_a_top5_image_urls = splits.metadata.get("group_a_top5_image_urls")
                group_b_top5_image_urls = splits.metadata.get("group_b_top5_image_urls")
            
            result = analyzer.analyze(
                group_a=splits.group_a,
                group_b=splits.group_b,
                correct_answer=splits.correct_answer,
                output_dir=str(out_dir) if out_dir else "",
                experiment_name=experiment_id,
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®èª¬æ˜æ–‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿æ¨™æº–ã®descriptions.csvï¼‰
                dataset_path=str(self.dataset_manager.data_root / 'steam-review-aspect-dataset' / 'current') if dataset == 'steam' else None,
                aspect_descriptions_file=desc_file,
                examples=examples_payload,
                group_a_top5_image_urls=group_a_top5_image_urls,
                group_b_top5_image_urls=group_b_top5_image_urls
            )
            
            self.logger.info("âœ… LLMå¿œç­”å–å¾—å®Œäº†")
            
            # [3/3] ã‚¹ã‚³ã‚¢ç¢ºèªï¼ˆanalyzersã§æ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
            self.logger.info(f"\n[3/3] ã‚¹ã‚³ã‚¢ç¢ºèªä¸­...")
            
            bert_score = result['evaluation']['bert_score']
            bleu_score = result['evaluation']['bleu_score']
            llm_score = result['evaluation'].get('llm_score')
            llm_response = result['process']['llm_response']
            
            self.logger.info("âœ… ã‚¹ã‚³ã‚¢ç¢ºèªå®Œäº†")
            
            # çµæœè¡¨ç¤º
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"=== çµæœ ===")
            self.logger.info(f"BERTã‚¹ã‚³ã‚¢: {bert_score:.4f}")
            self.logger.info(f"BLEUã‚¹ã‚³ã‚¢: {bleu_score:.4f}")
            if llm_score is not None:
                self.logger.info(f"LLMã‚¹ã‚³ã‚¢: {llm_score:.4f}")
                llm_reasoning = result['evaluation'].get('llm_evaluation_reasoning')
                if llm_reasoning:
                    self.logger.info(f"LLMè©•ä¾¡ç†ç”±: {llm_reasoning}")
            self.logger.info(f"LLMå¿œç­”: {llm_response}")
            self.logger.info(f"å“è³ªè©•ä¾¡: {result['summary']['quality_assessment']['overall_quality']}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            result['experiment_info']['dataset'] = dataset
            result['experiment_info']['aspect'] = aspect
            result['experiment_info']['group_size'] = group_size
            result['experiment_info']['split_type'] = split_type
            # é¸æŠãƒ¢ãƒ¼ãƒ‰/CSVãƒ‘ã‚¹ï¼ˆanalyzerå´ã§æ—¢ã«å«ã‚ã‚‹ãŒæ˜ç¤ºçš„ã«ä¿æŒï¼‰
            result['experiment_info']['use_aspect_descriptions'] = bool(use_desc)
            result['experiment_info']['aspect_descriptions_file'] = desc_file or ''
            # ä¾‹é¡Œãƒ¡ã‚¿æƒ…å ±
            result['experiment_info']['use_examples'] = bool(use_examples)
            result['experiment_info']['examples_file'] = examples_file or ''
            result['experiment_info']['examples_count_used'] = len(examples_payload or [])
            # LLMè©•ä¾¡ãƒ¡ã‚¿æƒ…å ±
            result['experiment_info']['use_llm_evaluation'] = bool(use_llm_eval)
            result['experiment_info']['llm_evaluation_model'] = llm_eval_model
            result['experiment_info']['llm_evaluation_temperature'] = llm_eval_temp
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ å®Ÿé¨“å¤±æ•— ({experiment_id}): {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "experiment_info": {
                    "experiment_id": experiment_id,
                    "dataset": dataset,
                    "aspect": aspect,
                    "error": str(e)
                },
                "summary": {
                    "success": False,
                    "error": str(e)
                }
            }
    
    def run_batch_experiments(self) -> List[Dict]:
        """ãƒãƒƒãƒå®Ÿé¨“å®Ÿè¡Œ"""
        self.logger.info("=" * 60)
        self.logger.info("ãƒãƒƒãƒå®Ÿé¨“é–‹å§‹")
        self.logger.info("=" * 60)
        
        all_results = []
        
        # å®Ÿè¡Œç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™
        # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã«æ™‚åˆ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆï¼ˆexperiments/{YYYY}/{MM}/{DD}/results/æ™‚åˆ»ï¼‰
        base_output_dir = None
        if not self.silent:
            base_output_dir = self._get_dated_results_base()
            base_output_dir.mkdir(parents=True, exist_ok=True)
            self.run_dir = base_output_dir / f"{self.timestamp}"
            self.run_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.run_dir}")
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼å–ã‚Šä»˜ã‘
            self._attach_file_logger()

        # å„å®Ÿé¨“è¨­å®šã‚’å®Ÿè¡Œ
        for exp_config in self.config['experiments']:
            dataset = exp_config['dataset']
            aspects = exp_config['aspects']
            group_size = exp_config.get('group_size', 100)
            split_type = exp_config.get('split_type', 'aspect_vs_others')
            
            self.logger.info(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset}")
            self.logger.info(f"å¯¾è±¡ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {aspects}")
            
            # å„ã‚¢ã‚¹ãƒšã‚¯ãƒˆã§å®Ÿé¨“å®Ÿè¡Œ
            for aspect in aspects:
                result = self.run_single_experiment(
                    dataset=dataset,
                    aspect=aspect,
                    group_size=group_size,
                    split_type=split_type,
                    output_dir=self.run_dir if not self.silent else None
                )
                
                if result:
                    all_results.append(result)
        
        self.results = all_results
        return all_results
    
    def save_results(self, results: Optional[List[Dict]] = None) -> str:
        """çµæœä¿å­˜"""
        if results is None:
            results = self.results
        
        if self.silent:
            return ""

        if not results:
            self.logger.warning("ä¿å­˜ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return ""
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå®Ÿè¡Œç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ï¼‰
        if self.run_dir is None:
            base_output_dir = self._get_dated_results_base()
            self.run_dir = base_output_dir / f"{self.timestamp}"
            self.run_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±åˆçµæœä½œæˆ
        summary = {
            "experiment_meta": {
                "timestamp": self.timestamp,
                "config_file": str(self.config_path),
                "total_experiments": len(results),
                "successful_experiments": sum(
                    1 for r in results if r.get('summary', {}).get('success', False)
                ),
                "run_name": self.run_name,
                "output_dir": str(self.run_dir)
            },
            "results": results
        }
        
        # ä¿å­˜
        filename = f"batch_experiment_{self.timestamp}.json"
        filepath = self.run_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"\nğŸ“ çµæœä¿å­˜: {filepath}")

        # å®Ÿè¡Œæ™‚è¨­å®šã®ä¿å­˜ï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰
        self._save_run_configuration()
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆï¼ˆè©³ç´°ï¼‰
        self._write_markdown_summary(summary)
        # ãƒ«ãƒ¼ãƒˆresultsã«æ¦‚è¦ã‚’ä½œæˆ
        self._write_root_overview(summary)
        
        return str(filepath)

    def _save_run_configuration(self) -> None:
        """å®Ÿè¡Œæ™‚è¨­å®šï¼ˆæœ‰åŠ¹å€¤ï¼‰ã‚’ä¿å­˜"""
        if self.silent or self.run_dir is None:
            return
        # è¨­å®šã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆå…ƒYAMLï¼‰
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                original_yaml = f.read()
            with open(self.run_dir / 'pipeline_config_snapshot.yaml', 'w', encoding='utf-8') as f:
                f.write(original_yaml)
        except Exception:
            pass
        # å®Ÿè¡Œãƒ¡ã‚¿ï¼ˆJSONï¼‰
        effective = {
            "timestamp": self.timestamp,
            "run_name": self.run_name,
            "output_dir": str(self.run_dir),
            "config_path": str(self.config_path),
            "experiments": self.config.get('experiments', []),
            "llm": self.config.get('llm', {}),
            "general": self.config.get('general', {})
        }
        with open(self.run_dir / 'run_effective_config.json', 'w', encoding='utf-8') as f:
            json.dump(effective, f, ensure_ascii=False, indent=2)

    def _extract_image_id_from_url(self, url: str) -> Optional[str]:
        """
        ç”»åƒURLã‹ã‚‰ç”»åƒIDï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã‚’æŠ½å‡º
        
        Args:
            url: ç”»åƒURLï¼ˆä¾‹: "http://images.cocodataset.org/train2017/000000081860.jpg"ï¼‰
        
        Returns:
            ç”»åƒIDï¼ˆä¾‹: "000000081860.jpg"ï¼‰ã¾ãŸã¯None
        """
        if not url:
            return None
        try:
            # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
            # ä¾‹: "http://images.cocodataset.org/train2017/000000081860.jpg" -> "000000081860.jpg"
            parts = url.split('/')
            if parts:
                filename = parts[-1]
                return filename
        except Exception:
            pass
        return None
    
    def _convert_url_to_image_path(self, url: str) -> Optional[str]:
        """
        ç”»åƒURLã‚’ç”»åƒãƒ‘ã‚¹ã«é€†å¤‰æ›
        
        Args:
            url: ç”»åƒURLï¼ˆä¾‹: "http://images.cocodataset.org/train2017/000000081860.jpg"ï¼‰
        
        Returns:
            ç”»åƒãƒ‘ã‚¹ï¼ˆä¾‹: "data/coco/train2017/000000081860.jpg"ï¼‰ã¾ãŸã¯None
        """
        if not url:
            return None
        try:
            # URLã‹ã‚‰ç›¸å¯¾ãƒ‘ã‚¹ã‚’æŠ½å‡º
            # ä¾‹: "http://images.cocodataset.org/train2017/000000081860.jpg" -> "train2017/000000081860.jpg"
            if "images.cocodataset.org/" in url:
                relative_path = url.split("images.cocodataset.org/", 1)[1]
                return f"data/coco/{relative_path}"
        except Exception:
            pass
        return None
    
    def _find_caption_for_image_url(
        self, 
        image_url: str, 
        captions: List[str],
        output_file: Optional[str] = None
    ) -> Optional[str]:
        """
        ç”»åƒURLã«å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
        
        Args:
            image_url: ç”»åƒURL
            captions: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆ
            output_file: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€è©³ç´°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼‰
        
        Returns:
            å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¾ãŸã¯None
        """
        if not image_url or not captions:
            return None
        
        image_id = self._extract_image_id_from_url(image_url)
        if not image_id:
            return None
        
        # å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        if output_file:
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                input_data = result_data.get('input', {})
                group_a_records = input_data.get('group_a_records', [])
                group_b_records = input_data.get('group_b_records', [])
                
                # ã‚°ãƒ«ãƒ¼ãƒ—Aã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¤œç´¢
                for record in group_a_records:
                    if isinstance(record, dict):
                        metadata = record.get('metadata', {})
                        record_image_path = metadata.get('image_path', '')
                        if record_image_path:
                            record_image_id = self._extract_image_id_from_url(
                                convert_coco_path_to_url(record_image_path)
                            )
                            if record_image_id == image_id:
                                return record.get('text', '')
                
                # ã‚°ãƒ«ãƒ¼ãƒ—Bã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¤œç´¢
                for record in group_b_records:
                    if isinstance(record, dict):
                        metadata = record.get('metadata', {})
                        record_image_path = metadata.get('image_path', '')
                        if record_image_path:
                            record_image_id = self._extract_image_id_from_url(
                                convert_coco_path_to_url(record_image_path)
                            )
                            if record_image_id == image_id:
                                return record.get('text', '')
            except Exception:
                pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é †åºãŒä¸€è‡´ã—ã¦ã„ã‚‹ã¨ä»®å®šã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§å¯¾å¿œä»˜ã‘ã‚‹
        # ã“ã‚Œã¯æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹
        return None

    def _write_markdown_summary(self, summary_data: Dict) -> None:
        """è¨­å®šå€¤ã¨çµæœæ¦‚è¦ã®Markdownã‚’ä½œæˆ"""
        if self.silent or self.run_dir is None:
            return
        lines = []
        meta = summary_data.get('experiment_meta', {})
        results = summary_data.get('results', [])
        lines.append(f"# å®Ÿé¨“ã‚µãƒãƒªãƒ¼: {self.run_name}")
        lines.append("")
        lines.append(f"- å®Ÿè¡Œæ™‚åˆ»: {meta.get('timestamp', '')}")
        lines.append(f"- å‡ºåŠ›å…ˆ: {meta.get('output_dir', '')}")
        lines.append(f"- ç·å®Ÿé¨“æ•°: {meta.get('total_experiments', 0)}")
        lines.append(f"- æˆåŠŸæ•°: {meta.get('successful_experiments', 0)}")
        lines.append("")
        # è¨­å®š
        lines.append("## è¨­å®š")
        lines.append("")
        lines.append("```yaml")
        try:
            with open(self.run_dir / 'pipeline_config_snapshot.yaml', 'r', encoding='utf-8') as f:
                lines.append(f.read())
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç°¡æ˜“è¨­å®š
            lines.append(yaml.safe_dump(self.config, allow_unicode=True))
        lines.append("```")
        lines.append("")
        # çµæœä¸€è¦§
        lines.append("## çµæœæ¦‚è¦")
        lines.append("")
        lines.append("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | ä»¶æ•°(A/B) | ä¾‹é¡Œæ•° | BERT | BLEU | LLM | LLMç†ç”± | å“è³ª | LLMå‡ºåŠ› | å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« |")
        lines.append("| --- | --- | --- | ---:| ---:| ---:| ---:| --- | --- | --- | --- |")
        for r in results:
            if not r.get('summary', {}).get('success', False):
                continue
            info = r.get('experiment_info', {})
            evals = r.get('evaluation', {})
            out_file = r.get('output_file', '')
            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆè¡¨ç¤ºï¼ˆèª¬æ˜æ–‡ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ (å…ƒã‚¢ã‚¹ãƒšã‚¯ãƒˆ) èª¬æ˜æ–‡ï¼‰
            aspect_name = info.get('aspect', '')
            aspect_display = aspect_name
            try:
                if info.get('use_aspect_descriptions') and info.get('aspect_descriptions_file'):
                    csv_path = str(info.get('aspect_descriptions_file'))
                    desc_map = self._load_aspect_descriptions(csv_path)
                    desc_text = desc_map.get(aspect_name, '')
                    if desc_text:
                        aspect_display = f"({aspect_name}) {desc_text}"
            except Exception:
                pass
            # ä»¶æ•°ï¼ˆA/Bï¼‰
            a_count = len(((r.get('input') or {}).get('group_a') or []))
            b_count = len(((r.get('input') or {}).get('group_b') or []))
            counts_display = f"A:{a_count}/B:{b_count}"
            # ä¾‹é¡Œæ•°
            examples_count = int((info.get('examples_count_used') or 0))
            # LLMå‡ºåŠ›ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å‘ã‘ã«æ•´å½¢ãƒ»çŸ­ç¸®ï¼‰
            llm_text = (r.get('process', {}) or {}).get('llm_response', '') or ''
            llm_text = llm_text.replace("\n", " ").replace("|", "ï½œ").strip()
            if len(llm_text) > 160:
                llm_text = llm_text[:157] + "..."
            llm_score = evals.get('llm_score')
            llm_score_display = f"{llm_score:.4f}" if llm_score is not None else "-"
            llm_reasoning = evals.get('llm_evaluation_reasoning', '')
            llm_reasoning_display = llm_reasoning.replace("\n", " ").replace("|", "ï½œ").strip() if llm_reasoning else "-"
            if len(llm_reasoning_display) > 80:
                llm_reasoning_display = llm_reasoning_display[:77] + "..."
            lines.append(
                f"| {info.get('dataset','')} | {aspect_display} | {counts_display} | {examples_count} | "
                f"{evals.get('bert_score',0):.4f} | {evals.get('bleu_score',0):.4f} | {llm_score_display} | {llm_reasoning_display} | "
                f"{r.get('summary',{}).get('quality_assessment',{}).get('overall_quality','')} | "
                f"{llm_text} | "
                f"{Path(out_file).name if out_file else ''} |"
            )
        lines.append("")
        
        # æ¯”è¼ƒå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        lines.append("## æ¯”è¼ƒå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ")
        lines.append("")
        for idx, r in enumerate(results, 1):
            if not r.get('summary', {}).get('success', False):
                continue
            info = r.get('experiment_info', {})
            evals = r.get('evaluation', {})
            dataset = info.get('dataset', '')
            aspect = info.get('aspect', '')
            
            reference_text = evals.get('reference_text', '')
            candidate_text = evals.get('candidate_text', '')
            
            if reference_text or candidate_text:
                lines.append(f"### å®Ÿé¨“ {idx}: {dataset} - {aspect}")
                lines.append("")
                lines.append("**æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆ (Reference):**")
                lines.append("")
                lines.append(f"```")
                lines.append(reference_text)
                lines.append("```")
                lines.append("")
                lines.append("**ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ (Candidate):**")
                lines.append("")
                lines.append(f"```")
                lines.append(candidate_text)
                lines.append("```")
                lines.append("")
        
        # ç”»åƒURLã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼ˆretrieved_conceptsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆï¼‰
        image_sections_added = False
        for r in results:
            if not r.get('summary', {}).get('success', False):
                continue
            info = r.get('experiment_info', {})
            dataset = info.get('dataset', '')
            if dataset == 'retrieved_concepts':
                input_data = r.get('input', {})
                group_a_urls = input_data.get('group_a_top5_image_urls')
                group_b_urls = input_data.get('group_b_top5_image_urls')
                group_a_captions = input_data.get('group_a', [])
                group_b_captions = input_data.get('group_b', [])
                
                if group_a_urls or group_b_urls:
                    if not image_sections_added:
                        lines.append("## ç”»åƒURL")
                        lines.append("")
                        image_sections_added = True
                    
                    aspect_name = info.get('aspect', '')
                    lines.append(f"### {aspect_name}")
                    lines.append("")
                    
                    out_file = r.get('output_file', '')
                    
                    if group_a_urls:
                        lines.append("#### ã‚°ãƒ«ãƒ¼ãƒ—A (Top-5)")
                        lines.append("")
                        lines.append("<p>")
                        for url in group_a_urls[:5]:
                            lines.append(f'  <img src="{url}" width="18%" />')
                        lines.append("</p>")
                        lines.append("")
                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
                        lines.append("<p>")
                        for url in group_a_urls[:5]:
                            # ç”»åƒURLã‹ã‚‰å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                            caption = self._find_caption_for_image_url(url, group_a_captions, out_file)
                            if not caption and group_a_captions:
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é †åºãŒä¸€è‡´ã—ã¦ã„ã‚‹ã¨ä»®å®š
                                url_index = group_a_urls.index(url)
                                if url_index < len(group_a_captions):
                                    caption = group_a_captions[url_index]
                            
                            if caption:
                                # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’çŸ­ç¸®ï¼ˆé•·ã™ãã‚‹å ´åˆï¼‰
                                caption_display = caption if len(caption) <= 80 else caption[:77] + "..."
                                lines.append(f'  <span style="font-size: 0.85em; width: 18%; display: inline-block; vertical-align: top; padding: 0 1%;">{caption_display}</span>')
                            else:
                                lines.append(f'  <span style="font-size: 0.85em; width: 18%; display: inline-block; vertical-align: top; padding: 0 1%;"></span>')
                        lines.append("</p>")
                        lines.append("")
                    
                    if group_b_urls:
                        lines.append("#### ã‚°ãƒ«ãƒ¼ãƒ—B (Bottom-5)")
                        lines.append("")
                        lines.append("<p>")
                        for url in group_b_urls[:5]:
                            lines.append(f'  <img src="{url}" width="18%" />')
                        lines.append("</p>")
                        lines.append("")
                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
                        lines.append("<p>")
                        for url in group_b_urls[:5]:
                            # ç”»åƒURLã‹ã‚‰å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                            caption = self._find_caption_for_image_url(url, group_b_captions, out_file)
                            if not caption and group_b_captions:
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é †åºãŒä¸€è‡´ã—ã¦ã„ã‚‹ã¨ä»®å®š
                                url_index = group_b_urls.index(url)
                                if url_index < len(group_b_captions):
                                    caption = group_b_captions[url_index]
                            
                            if caption:
                                # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’çŸ­ç¸®ï¼ˆé•·ã™ãã‚‹å ´åˆï¼‰
                                caption_display = caption if len(caption) <= 80 else caption[:77] + "..."
                                lines.append(f'  <span style="font-size: 0.85em; width: 18%; display: inline-block; vertical-align: top; padding: 0 1%;">{caption_display}</span>')
                            else:
                                lines.append(f'  <span style="font-size: 0.85em; width: 18%; display: inline-block; vertical-align: top; padding: 0 1%;"></span>')
                        lines.append("</p>")
                        lines.append("")
        
        # ãƒ­ã‚°ãƒªãƒ³ã‚¯
        lines.append("## ãƒ­ã‚°")
        lines.append("")
        try:
            rel_python_log = os.path.relpath(self.run_dir / 'logs/python.log', self.run_dir)
            rel_cli_log = os.path.relpath(self.run_dir / 'logs/cli_run.log', self.run_dir)
            lines.append(f"- Pythonãƒ­ã‚°: {rel_python_log}")
            lines.append(f"- CLIãƒ­ã‚°: {rel_cli_log}")
        except Exception:
            pass
        lines.append("")
        # ä¿å­˜
        md_path = self.run_dir / 'summary.md'
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))

    def _load_examples_file(self, path: str) -> List[Dict]:
        """ä¾‹é¡Œãƒ•ã‚¡ã‚¤ãƒ«(JSON/YAML)ã‚’èª­ã¿è¾¼ã¿ã€ç°¡æ˜“æ¤œè¨¼ã—ã¦è¿”ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šã€‚"""
        if not path:
            return []
        if path in self._examples_cache:
            return self._examples_cache[path]
        ext = (Path(path).suffix or '').lower()
        data = []
        try:
            if ext in ['.yaml', '.yml']:
                import yaml as _yaml
                with open(path, 'r', encoding='utf-8') as f:
                    data = _yaml.safe_load(f) or []
            else:
                import json as _json
                with open(path, 'r', encoding='utf-8') as f:
                    data = _json.load(f) or []
        except Exception as e:
            self.logger.warning(f"ä¾‹é¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {path} ({e})")
            data = []
        # æ¤œè¨¼ãƒ»æ•´å½¢
        valid: List[Dict] = []
        if isinstance(data, list):
            for item in data:
                try:
                    ga = item.get('group_a') if isinstance(item, dict) else None
                    gb = item.get('group_b') if isinstance(item, dict) else None
                    ans = item.get('answer') if isinstance(item, dict) else None
                    if isinstance(ga, list) and isinstance(gb, list) and isinstance(ans, str):
                        valid.append({'group_a': ga, 'group_b': gb, 'answer': ans})
                except Exception:
                    continue
        self._examples_cache[path] = valid
        return valid

    def _write_root_overview(self, summary_data: Dict) -> None:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆresults/ã«æ¦‚è¦Markdownã‚’ä¿å­˜ã—ã€è©³ç´°ã¸ã®ãƒ‘ã‚¹ã‚’è¨˜è¼‰"""
        if self.silent:
            return
        try:
            root_dir = SCRIPT_DIR.parents[5] / 'experiment_summaries'
        except Exception:
            return
        root_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨æ„ï¼ˆãªã‘ã‚Œã°ç”Ÿæˆï¼‰
        template_dir = root_dir / 'template'
        template_dir.mkdir(parents=True, exist_ok=True)
        template_path = template_dir / 'root_overview_template.md'
        if not template_path.exists():
            default_tpl = []
            # èª¬æ˜ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆç”Ÿæˆç‰©ã«ã¯å«ã‚ãªã„ï¼‰
            default_tpl.append("<!-- ãƒ†ãƒ³ãƒ—ãƒ¬èª¬æ˜: ä¸‹è¨˜ã®è¨˜å·ã¯å®Ÿè¡Œæ™‚ã«ç½®æ›ã•ã‚Œã¾ã™ï¼ˆã“ã®ã‚³ãƒ¡ãƒ³ãƒˆã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ï¼‰ -->")
            default_tpl.append("<!-- {{TIMESTAMP}}: å®Ÿè¡Œæ™‚åˆ»(YYYYMMDD_HHMMSS) -->")
            default_tpl.append("<!-- {{RUN_NAME}}: å®Ÿé¨“åï¼ˆconfigã®run_nameã€‚æœªè¨­å®šæ™‚ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ -->")
            default_tpl.append("<!-- {{DETAIL_DIR_PATH}}: è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ç›¸å¯¾ãƒ‘ã‚¹ -->")
            default_tpl.append("<!-- {{DETAIL_SUMMARY_PATH}}: è©³ç´°summary.mdã¸ã®ç›¸å¯¾ãƒ‘ã‚¹ -->")
            default_tpl.append("<!-- {{DETAIL_DIR_MD_LINK}}: [è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª](ç›¸å¯¾ãƒ‘ã‚¹) ã¸ã®ãƒªãƒ³ã‚¯ -->")
            default_tpl.append("<!-- {{DETAIL_SUMMARY_MD_LINK}}: [è©³ç´°ã‚µãƒãƒªãƒ¼](ç›¸å¯¾ãƒ‘ã‚¹) ã¸ã®ãƒªãƒ³ã‚¯ -->")
            default_tpl.append("<!-- {{TOTAL_EXPERIMENTS}}: ç·å®Ÿé¨“æ•° / {{SUCCESSFUL_EXPERIMENTS}}: æˆåŠŸæ•° -->")
            default_tpl.append("<!-- {{RESULTS_TABLE}}: å…ˆé ­æ•°ä»¶ã®çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚¢ã‚¹ãƒšã‚¯ãƒˆ/BERT/BLEUï¼‰ -->")
            # è¿½åŠ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼ˆä»»æ„ã§ä½¿ç”¨å¯ï¼‰
            default_tpl.append("<!-- è¿½åŠ : {{DATASET_LIST}}ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{ASPECT_LIST}}ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¢ã‚¹ãƒšã‚¯ãƒˆã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{DETAIL_DIR_ABS}}ï¼ˆè©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{CONFIG_PATH}}ï¼ˆä½¿ç”¨ã—ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{RUN_DIR_NAME}}ï¼ˆè©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®ã¿ï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{LLM_MODEL}}ï¼ˆè¨­å®šã®llm.modelï¼‰ -->")
            default_tpl.append("<!-- è¿½åŠ : {{RESULT_JSON_PATH}}ï¼ˆãƒãƒƒãƒçµæœJSONã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰ -->\n")
            default_tpl.append("# å®Ÿé¨“æ¦‚è¦ {{TIMESTAMP}}")
            default_tpl.append("")
            default_tpl.append("- å®Ÿé¨“å: {{RUN_NAME}}")
            default_tpl.append("- {{DETAIL_DIR_MD_LINK}}")
            default_tpl.append("- {{DETAIL_SUMMARY_MD_LINK}}")
            default_tpl.append("- ç·å®Ÿé¨“æ•°: {{TOTAL_EXPERIMENTS}} / æˆåŠŸ: {{SUCCESSFUL_EXPERIMENTS}}\n")
            default_tpl.append("- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {{DATASET_LIST}}")
            default_tpl.append("- ã‚¢ã‚¹ãƒšã‚¯ãƒˆ: {{ASPECT_LIST}}\n")
            default_tpl.append("## çµæœæ¦‚è¦")
            default_tpl.append("{{RESULTS_TABLE}}")
            with open(template_path, 'w', encoding='utf-8') as tf:
                tf.write("\n".join(default_tpl))

        # ç½®æ›ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        meta = summary_data.get('experiment_meta', {})
        results = summary_data.get('results', [])
        rel_detail_dir = os.path.relpath(self.run_dir, root_dir) if self.run_dir else ''
        rel_detail_summary = os.path.relpath(self.run_dir / 'summary.md', root_dir) if self.run_dir else ''
        results_table = self._build_results_table(results, limit=5)
        # è¿½åŠ å¤‰æ•°ã‚’æ§‹ç¯‰
        datasets = sorted({(r.get('experiment_info') or {}).get('dataset', '') for r in results if r.get('experiment_info')})
        aspects = sorted({(r.get('experiment_info') or {}).get('aspect', '') for r in results if r.get('experiment_info')})
        dataset_list = ", ".join([d for d in datasets if d])
        aspect_list = ", ".join([a for a in aspects if a])
        detail_dir_abs = str(self.run_dir) if self.run_dir else ''
        config_path = str(self.config_path)
        run_dir_name = self.run_dir.name if self.run_dir else ''
        llm_model = (self.config.get('llm') or {}).get('model', '')
        result_json_rel = os.path.relpath(self.run_dir / f"batch_experiment_{meta.get('timestamp','')}.json", root_dir) if self.run_dir else ''
        rel_log_dir = os.path.relpath(self.run_dir / 'logs', root_dir) if self.run_dir else ''
        rel_cli_log = os.path.relpath(self.run_dir / 'logs/cli_run.log', root_dir) if self.run_dir else ''

        with open(template_path, 'r', encoding='utf-8') as tf:
            template_lines = tf.read().splitlines()

        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ç½®æ›ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ç½®æ›ã—ãªã„ï¼‰
        def apply_replacements(text: str) -> str:
            text = text.replace('{{TIMESTAMP}}', str(meta.get('timestamp', '')))
            text = text.replace('{{RUN_NAME}}', str(self.run_name))
            text = text.replace('{{DETAIL_DIR_PATH}}', rel_detail_dir)
            text = text.replace('{{DETAIL_SUMMARY_PATH}}', rel_detail_summary)
            text = text.replace('{{DETAIL_DIR_MD_LINK}}', f"[è©³ç´°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª]({rel_detail_dir})")
            text = text.replace('{{DETAIL_SUMMARY_MD_LINK}}', f"[è©³ç´°ã‚µãƒãƒªãƒ¼]({rel_detail_summary})")
            text = text.replace('{{TOTAL_EXPERIMENTS}}', str(meta.get('total_experiments', 0)))
            text = text.replace('{{SUCCESSFUL_EXPERIMENTS}}', str(meta.get('successful_experiments', 0)))
            text = text.replace('{{RESULTS_TABLE}}', results_table)
            # è¿½åŠ ç½®æ›
            text = text.replace('{{DATASET_LIST}}', dataset_list)
            text = text.replace('{{ASPECT_LIST}}', aspect_list)
            text = text.replace('{{DETAIL_DIR_ABS}}', detail_dir_abs)
            text = text.replace('{{CONFIG_PATH}}', config_path)
            text = text.replace('{{RUN_DIR_NAME}}', run_dir_name)
            text = text.replace('{{LLM_MODEL}}', llm_model)
            text = text.replace('{{RESULT_JSON_PATH}}', result_json_rel)
            text = text.replace('{{LOG_DIR_PATH}}', rel_log_dir)
            text = text.replace('{{CLI_LOG_PATH}}', rel_cli_log)
            text = text.replace('{{CLI_LOG_MD_LINK}}', f"[CLIãƒ­ã‚°]({rel_cli_log})")
            return text

        rendered_lines_all = []
        for ln in template_lines:
            if ln.strip().startswith('<!--'):
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ãã®ã¾ã¾ä¿æŒï¼ˆå¾Œæ®µã§é™¤å¤–ï¼‰
                rendered_lines_all.append(ln)
            else:
                rendered_lines_all.append(apply_replacements(ln))
        rendered = "\n".join(rendered_lines_all)

        # å‡ºåŠ›
        overview_path = root_dir / f"summary_{meta.get('timestamp','')}.md"
        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œ(<!-- -->)ã¯æ¦‚è¦ã«ã¯å«ã‚ãªã„
        rendered_lines = [ln for ln in rendered.splitlines() if not ln.strip().startswith('<!--')]
        # è¿½è¨˜: ã‚¢ã‚¹ãƒšã‚¯ãƒˆã”ã¨ã®LLMå‡ºåŠ›ä¸€è¦§ãƒ†ãƒ¼ãƒ–ãƒ«
        try:
            outputs_table = self._build_llm_outputs_table(results)
            if outputs_table.strip():
                rendered_lines.append("")
                rendered_lines.append("## LLMå‡ºåŠ›ä¸€è¦§")
                rendered_lines.append("")
                rendered_lines.extend(outputs_table.splitlines())
        except Exception:
            pass
        
        # è¿½è¨˜: ç”»åƒURLã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆretrieved_conceptsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆï¼‰
        try:
            image_section = self._build_image_urls_section(results)
            if image_section.strip():
                rendered_lines.append("")
                rendered_lines.extend(image_section.splitlines())
        except Exception:
            pass
        
        with open(overview_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(rendered_lines))

    def _build_results_table(self, results: List[Dict], limit: int = 5) -> str:
        """çµæœãƒ†ãƒ¼ãƒ–ãƒ«Markdownã‚’ä½œæˆ"""
        lines = []
        lines.append("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | BERT | BLEU | LLM | LLMç†ç”± |")
        lines.append("| --- | --- | ---:| ---:| ---:| --- |")
        for r in results[:limit]:
            info = r.get('experiment_info', {})
            evals = r.get('evaluation', {})
            aspect_name = info.get('aspect', '')
            aspect_display = aspect_name
            try:
                if info.get('use_aspect_descriptions') and info.get('aspect_descriptions_file'):
                    csv_path = str(info.get('aspect_descriptions_file'))
                    desc_map = self._load_aspect_descriptions(csv_path)
                    desc_text = desc_map.get(aspect_name, '')
                    if desc_text:
                        aspect_display = f"({aspect_name}) {desc_text}"
            except Exception:
                pass
            llm_score = evals.get('llm_score')
            llm_score_display = f"{llm_score:.4f}" if llm_score is not None else "-"
            llm_reasoning = evals.get('llm_evaluation_reasoning', '')
            llm_reasoning_display = llm_reasoning.replace("\n", " ").replace("|", "ï½œ").strip() if llm_reasoning else "-"
            if len(llm_reasoning_display) > 60:
                llm_reasoning_display = llm_reasoning_display[:57] + "..."
            lines.append(
                f"| {info.get('dataset','')} | {aspect_display} | {evals.get('bert_score',0):.4f} | {evals.get('bleu_score',0):.4f} | {llm_score_display} | {llm_reasoning_display} |"
            )
        return "\n".join(lines)

    def _build_llm_outputs_table(self, results: List[Dict], limit: Optional[int] = None) -> str:
        """LLMå‡ºåŠ›ã‚’ã‚¢ã‚¹ãƒšã‚¯ãƒˆã”ã¨ã«ä¸€è¦§åŒ–ã™ã‚‹è¡¨ï¼ˆãƒ«ãƒ¼ãƒˆæ¦‚è¦ç”¨ï¼‰"""
        if not results:
            return ""
        rows = results if limit is None else results[:limit]
        lines: List[str] = []
        lines.append("| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚¢ã‚¹ãƒšã‚¯ãƒˆ | LLMå‡ºåŠ› |")
        lines.append("| --- | --- | --- |")
        for r in rows:
            info = r.get('experiment_info', {}) or {}
            dataset = info.get('dataset', '')
            aspect_name = info.get('aspect', '')
            aspect_display = aspect_name
            try:
                if info.get('use_aspect_descriptions') and info.get('aspect_descriptions_file'):
                    csv_path = str(info.get('aspect_descriptions_file'))
                    desc_map = self._load_aspect_descriptions(csv_path)
                    desc_text = desc_map.get(aspect_name, '')
                    if desc_text:
                        aspect_display = f"({aspect_name}) {desc_text}"
            except Exception:
                pass
            llm_text = ((r.get('process') or {}).get('llm_response') or '')
            llm_text = llm_text.replace("\n", " ").replace("|", "ï½œ").strip()
            if len(llm_text) > 200:
                llm_text = llm_text[:197] + "..."
            lines.append(f"| {dataset} | {aspect_display} | {llm_text} |")
        return "\n".join(lines)

    def _build_image_urls_section(self, results: List[Dict]) -> str:
        """ç”»åƒURLã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’Markdownå½¢å¼ã§ä½œæˆï¼ˆretrieved_conceptsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆï¼‰"""
        lines: List[str] = []
        image_sections_added = False
        
        for r in results:
            if not r.get('summary', {}).get('success', False):
                continue
            info = r.get('experiment_info', {})
            dataset = info.get('dataset', '')
            if dataset == 'retrieved_concepts':
                input_data = r.get('input', {})
                group_a_urls = input_data.get('group_a_top5_image_urls')
                group_b_urls = input_data.get('group_b_top5_image_urls')
                
                if group_a_urls or group_b_urls:
                    if not image_sections_added:
                        lines.append("## ç”»åƒURL")
                        lines.append("")
                        image_sections_added = True
                    
                    aspect_name = info.get('aspect', '')
                    lines.append(f"### {aspect_name}")
                    lines.append("")
                    
                    if group_a_urls:
                        lines.append("#### ã‚°ãƒ«ãƒ¼ãƒ—A (Top-5)")
                        lines.append("")
                        lines.append("<p>")
                        for url in group_a_urls[:5]:
                            lines.append(f'  <img src="{url}" width="18%" />')
                        lines.append("</p>")
                        lines.append("")
                    
                    if group_b_urls:
                        lines.append("#### ã‚°ãƒ«ãƒ¼ãƒ—B (Bottom-5)")
                        lines.append("")
                        lines.append("<p>")
                        for url in group_b_urls[:5]:
                            lines.append(f'  <img src="{url}" width="18%" />')
                        lines.append("</p>")
                        lines.append("")
                    
                    # è¿½åŠ ç”»åƒè¡¨ç¤ºã‚³ãƒãƒ³ãƒ‰
                    out_file = r.get('output_file', '')
                    if out_file:
                        file_name = Path(out_file).name
                        rel_result_json = os.path.relpath(out_file, Path(self.run_dir).parents[5] / 'experiment_summaries') if self.run_dir else file_name
                        lines.append("#### è¿½åŠ ç”»åƒã‚’è¦‹ã‚‹ã«ã¯")
                        lines.append("")
                        lines.append("```bash")
                        lines.append(f"python src/analysis/experiments/utils/generate_image_gallery.py \\")
                        lines.append(f"  --result-json {rel_result_json} \\")
                        lines.append(f"  --top-n 10 \\")
                        lines.append(f"  --bottom-n 10")
                        lines.append("```")
                        lines.append("")
                    break  # æœ€åˆã®retrieved_conceptså®Ÿé¨“ã®ã¿è¡¨ç¤º
        
        return "\n".join(lines)

    def _load_aspect_descriptions(self, csv_path: str) -> Dict[str, str]:
        """ã‚¢ã‚¹ãƒšã‚¯ãƒˆèª¬æ˜CSVã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¹æ¯ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
        if not csv_path:
            return {}
        if csv_path in self._desc_cache:
            return self._desc_cache[csv_path]
        mapping: Dict[str, str] = {}
        try:
            import csv
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    aspect = (row.get('aspect') or '').strip()
                    desc = (row.get('description') or '').strip()
                    if aspect:
                        mapping[aspect] = desc
        except Exception:
            mapping = {}
        self._desc_cache[csv_path] = mapping
        return mapping
    
    def print_summary(self):
        """å®Ÿé¨“ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not self.console_output:
            return

        if not self.results:
            self.logger.info("å®Ÿé¨“çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("=== å®Ÿé¨“ã‚µãƒãƒªãƒ¼ ===")
        self.logger.info("=" * 60)
        
        total = len(self.results)
        successful = sum(1 for r in self.results if r.get('summary', {}).get('success', False))
        
        self.logger.info(f"ç·å®Ÿé¨“æ•°: {total}")
        self.logger.info(f"æˆåŠŸ: {successful}")
        self.logger.info(f"å¤±æ•—: {total - successful}")
        
        # ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼
        self.logger.info("\n=== ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼ ===")
        for result in self.results:
            if result.get('summary', {}).get('success', False):
                exp_info = result['experiment_info']
                evaluation = result['evaluation']
                
                dataset = exp_info.get('dataset', 'N/A')
                aspect = exp_info.get('aspect', 'N/A')
                bert = evaluation.get('bert_score', 0)
                bleu = evaluation.get('bleu_score', 0)
                llm = evaluation.get('llm_score')
                
                if llm is not None:
                    self.logger.info(f"{dataset:10s} {aspect:15s} BERT: {bert:.4f}  BLEU: {bleu:.4f}  LLM: {llm:.4f}")
                else:
                    self.logger.info(f"{dataset:10s} {aspect:15s} BERT: {bert:.4f}  BLEU: {bleu:.4f}")
    
    def run(self) -> bool:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            # è¨­å®šæ¤œè¨¼
            if not self.validate_config():
                return False
            
            # DatasetManageråˆæœŸåŒ–
            if not self.setup_dataset_manager():
                return False
            
            # ãƒãƒƒãƒå®Ÿé¨“å®Ÿè¡Œ
            results = self.run_batch_experiments()
            
            # çµæœä¿å­˜
            if not self.silent:
                self.save_results(results)
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self.print_summary()
            
            self.logger.info("\nâœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    default_config = Path(__file__).parent / "pipeline_config.yaml"
    
    pipeline = ExperimentPipeline(str(default_config), debug=True)
    success = pipeline.run()
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())

