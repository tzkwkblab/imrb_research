新しいベースラインの作成に着手する

現状の実装の問題点として、「bert」「bleu」スコアによる評価に関して、スコアが適切でない点が挙げられる

- 人の目で見て明らかに良質な対比因子が低スコア
  - gameplay の時に「this review is plaing game yeah」が非常に低いスコアだったりするがそれはおかしい。めっちゃあってるやん。

これを解決するために、何かしら工夫を施して新しいベースラインとしたい。

思いつくものとして、

- 正解ラベル「gameplay」とかを改善
  - 正解ラベルに関する説明などデータセット作成の github などに載っているので、そちらの文章を比較対象とする。
- bert,bleu に変わる新しい評価方法を追加
  - LLM による評価など使えばより適切にできるのでは？

まず、正解ラベルを単語 → テキストにしてみようと思う。これまでの実装部分を見て、どこを改善すればいいのか要件定義してみる。

【前提改善】

- 理想状態

  - データセット選択、実験変数設定を行えば、スコアが自動的にでてくる

- 現状状態
  - どこで実験を一括で実行するのかパイプラインがどこで実装したか覚えてない、多分 utiils ディレクトリ内で実装しているので確認必要。
  - そもそも実装したパイプラインが正常に動作しているのかわからない。あと、データセット選択とかの部分の実装が中途半端にされていて最後までできていない可能性がある。

---

## 実装完了（2025-10-10）

### 成果

統一実験パイプラインの実装が完了し、理想状態を達成：

✅ **パイプライン実装完了**

- `experiment_pipeline.py`: メインパイプライン実装
- `run_experiment.py`: コマンドライン実行スクリプト
- `pipeline_config.yaml`: 設定ファイル駆動の実験管理

✅ **動作確認完了（Steam データセット）**

```bash
# コマンドライン実行が正常に動作
python run_experiment.py --dataset steam --aspect gameplay --group-size 10

# 結果例:
# - BERTスコア: 0.5419
# - BLEUスコア: 0.0000
# - JSON形式で結果保存
```

✅ **複数アスペクト対応**

- gameplay, visual, story で実験成功
- 設定ファイルからバッチ実行可能

### 実装内容

1. **DatasetManager 修正**

   - データセット固有パスの自動構築
   - Steam データセット: 8,800 レコード正常読み込み

2. **パイプライン機能**

   - 設定ファイル駆動の実験管理
   - エラーハンドリング完備
   - JSON 形式での結果保存
   - 実験サマリー表示

3. **修正した問題**
   - utils モジュールのインポートエラー修正
   - DatasetManager のパス解決問題修正
   - .env 環境変数読み込み追加

### 次のステップ

評価方法の改善はまだ実装していない：

- [ ] 正解ラベルをアスペクト説明文に変更
- [ ] LLM ベースの評価追加
- [ ] SemEval/Amazon データセット対応

現状のパイプラインを使用して、まず Steam データセットで正解ラベル改善の効果を測定できる。
