## gpt はかなりいい線いってる。ただ、、

アスペクトが audio の場合の出力（1-shot）が、
"Specific mentions of audio quality and music"
かなりいい。アスペクトとなってる単語が文章に入ってる時点で百点上げてもいい。
bert スコアは 0.7766。ただ、人目線だと百点上げてもいいと思う。
というか、この回答に対して、0.77 しか出せない bert は評価指標として適切なのか、、、？と思ったりする。

これは他の例でも思うことだ。
gameplay（1-shot）
アスペクト gameplay
GPT 応答 "Group A focuses on gameplay specifics, Group B does not."
BERT スコア 0.6971

アスペクトが GPT 応答に含まれてる。まさに 100 点の回答。なのに 0.69 だ。
文脈を推し量るという本タスクにおいては完璧。しかしスコアは低い。

recommended（0-shot）
アスペクト recommended
Shot 設定 0-shot
BERT スコア 0.5737
BLEU スコア 0.0000
GPT 応答 "Group A reviews express satisfaction and recommendation, Group B reviews do not."

recomennded が特徴で、回答では"express recommendation"つまり推薦してるって、意味的に合致してる。のに、単語の違いというだけで 0.57 しかスコア出てない。
これは良くない。

## bert の限界

bert も bleu もそうだが、単語がどれだけ似ているかで判断する節がある。しかしこの設定は研究にはそぐわない。

いったん、検証としてスコアが出るようにしてあるが、評価の仕方を変えるべきかも知れない。
この実験における低いスコアというのは、グループ A と B の微妙な差異を認識できない、つまり文脈の認識が足りていない例なのだ。
もっとわかりやすく言えば、「gameplay」なのに、「recomemmded」みたいにいってる例が必要。

やはり評価には gpt と人の手作業が必要なのか？
sentenceBERT など活用する方向性も考えた方がいいかもしれない、、、

## ベースラインを再考する

今もとある単語同士の BERT スコアをベースラインとしてるが、これも参考になるかと言われれば微妙なラインだ。本来なら、簡単に思いつく既存手法との比較が妥当（新規研究なので）だが、この比較はあまり意味がない気がする。
だって、アスペクト同士の意味的類似度と、GPT が出した出力の意味的類似度の比較に何の意味がある？
二つの要素の関連性が無さすぎる。アスペクト同士の意味的類似度は、もうほぼランダムな単語の比較と言っていいだろう？なぜこれが参考指標になるのかわからない。もちろん、ドメインは同じな単語同士という意味は多少あるのだろうけれども、だとしてもだ。price と gameplay なんて全く方向性違うだろう。

そういうことで、ベースラインとして、せめて、グループ A と B それぞれで共起頻度を出して、それを元に、単語を抽出する、みたいなことした方がいいと思った。
共起頻度がグループ A で高くて、尚且つグループ B で高くないものを抽出するみたいな。
グループ A 共起頻度高い順から、各単語のグループ B での共起頻度を差し引いて、残ったスコアで勝負するかじで。
