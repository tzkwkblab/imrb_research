# SemEval ABSA GPT 対比因子生成検証実験レポート

**実験日時**: 2025 年 6 月 12 日  
**実験者**: 清野駿（筑波大学大学院修士 2 年）  
**研究テーマ**: ドメイン特化型説明可能 AI のための対比因子ラベル生成手法に関する研究

---

## 📋 実験概要

### 研究目的

SemEval ABSA データセットを用いて、**ドメイン別**（Restaurant、Laptop）で GPT が特徴を含むレビュー群と含まないレビュー群の違いを正確に説明できるかを検証する。従来のランダム分割実験との比較により、実際の特徴に基づく分析での精度向上を定量的に評価する。

### 実験設計

1. **ドメイン別データ分割**: Domain-Aware Feature Splitter を使用して Restaurant/Laptop ドメインで特徴ベース分割
2. **対比因子生成**: GPT に 2 グループ間の特徴的違いを説明させる
3. **精度評価**: 生成された説明の正確性を多角的に評価
4. **ドメイン間比較**: Restaurant vs Laptop での特徴抽出精度の比較
5. **ベースライン比較**: ランダム分割（ハルシネーション実験）との性能差分析

---

## 🔬 実験方法

### データセット

- **データソース**: SemEval ABSA Dataset (Restaurant14/15/16, Laptop14/15/16)
- **対象ドメイン**: Restaurant、Laptop
- **分析対象特徴**:
  - **Restaurant**: food, service, staff, atmosphere, menu, price
  - **Laptop**: battery, screen, keyboard, performance, design, portability

### 実験条件

#### **Group 分割設定**

- **Group A**: 特定特徴を含むレビュー（例：food アスペクトを含む）
- **Group B**: 特定特徴を含まないレビュー（例：food アスペクト以外）
- **サンプルサイズ**: 各グループ最大 50 件（バランス調整）

#### **Few-shot 学習設定**

- **0-shot**: 例題なし
- **1-shot**: 1 つの例題
- **3-shot**: 3 つの例題
- **5-shot**: 5 つの例題

#### **評価軸**

1. **正確性**: 生成説明が実際の特徴差を反映しているか
2. **具体性**: 抽象的でなく具体的な違いを述べているか
3. **ハルシネーション**: 根拠のない特徴を述べていないか
4. **ドメイン適合性**: ドメイン特有の語彙・表現を使用しているか

---

## 📊 実験仮説

### H1: ドメイン特化効果

**仮説**: Restaurant ドメインは日常的な表現が多く、Laptop ドメインは技術的な表現が多いため、技術ドメイン（Laptop）の方が特徴差が明確で説明精度が高い。

### H2: 特徴別精度差

**仮説**: 感覚的特徴（taste, atmosphere）より客観的特徴（battery, screen）の方が説明精度が高い。

### H3: Few-shot 学習効果

**仮説**: ドメイン特化データでは、Few-shot 学習による精度向上がランダムデータより顕著に現れる。

### H4: ハルシネーション抑制効果

**仮説**: 実際の特徴に基づく分析では、ランダム分割と比較してハルシネーションスコアが大幅に低下する（0.433 → 0.2 以下）。

---

## 🎯 詳細実験計画

### Phase 1: ドメイン別基礎分析

#### **1.1 データ分布分析**

- 各ドメインの特徴別サンプル数調査
- センチメント分布の確認
- レビュー長分布の分析

#### **1.2 特徴重複度分析**

- ドメイン内での特徴間の重複度測定
- Cross-domain 特徴の特定（price 等）

### Phase 2: GPT 対比因子生成実験

#### **2.1 Restaurant ドメイン実験**

各特徴（food, service, staff, atmosphere, menu, price）について：

- 0/1/3/5-shot での説明生成（各設定 5 回実施）
- 総実験数：6 特徴 × 4 設定 × 5 回 = **120 実験**

#### **2.2 Laptop ドメイン実験**

各特徴（battery, screen, keyboard, performance, design, portability）について：

- 0/1/3/5-shot での説明生成（各設定 5 回実施）
- 総実験数：6 特徴 × 4 設定 × 5 回 = **120 実験**

#### **2.3 Cross-domain 比較実験**

共通特徴（price）での両ドメイン比較：

- 同一特徴でのドメイン差分析
- 総実験数：**40 実験**

### Phase 3: 評価・分析

#### **3.1 自動評価**

- **正確性スコア**: GPT-4 による 0-1 評価
- **具体性スコア**: 具体的表現の含有率
- **ハルシネーションスコア**: 根拠性の 0-1 評価
- **ドメイン適合性スコア**: ドメイン特有語彙の使用率

#### **3.2 比較分析**

- **ドメイン間比較**: Restaurant vs Laptop 精度比較
- **特徴間比較**: 感覚的 vs 客観的特徴の精度差
- **ベースライン比較**: ランダム分割実験との性能差

---

## 📈 予想される結果パターン

### パターン 1: ドメイン特化効果確認

- **Laptop > Restaurant**: 技術的特徴の明確性により
- **客観的特徴 > 感覚的特徴**: 測定可能な特徴の優位性

### パターン 2: Few-shot 学習の効果

- **5-shot > 3-shot > 1-shot > 0-shot**: 例題数による精度向上
- **実特徴 >> ランダム**: 実際の特徴での学習効果の優位性

### パターン 3: ハルシネーション抑制

- **実特徴ベース**: ハルシネーションスコア 0.1-0.3
- **ランダムベース**: ハルシネーションスコア 0.4-0.5
- **改善率**: 50%以上のハルシネーション抑制

---

## 🔍 評価指標設計

### 主要評価指標

#### **1. 正確性スコア (Accuracy Score)**

- **0.0**: 特徴差を全く反映していない
- **0.5**: 部分的に正確だが不完全
- **1.0**: 実際の特徴差を正確に反映

#### **2. 具体性スコア (Specificity Score)**

- 具体的な単語・表現の含有率
- 抽象的表現（"品質が良い"）vs 具体的表現（"バッテリー持続時間"）

#### **3. ドメイン適合性スコア (Domain Relevance Score)**

- ドメイン特有語彙の使用率
- Restaurant: "taste", "flavor", "ambience"
- Laptop: "CPU", "RAM", "display resolution"

#### **4. ハルシネーション抑制率 (Hallucination Reduction Rate)**

```
抑制率 = (ランダム実験スコア - 実特徴実験スコア) / ランダム実験スコア
```

### 統計分析手法

- **記述統計**: 平均、標準偏差、分布分析
- **比較検定**: t 検定によるドメイン間・設定間の有意差検定
- **効果量分析**: Cohen's d による実用的意義の測定
- **相関分析**: 特徴タイプと精度の関係性分析

---

## 🚀 期待される研究成果

### 1. **学術的貢献**

- ドメイン特化型説明可能 AI の有効性実証
- LLM の特徴抽出能力のドメイン依存性解明
- Few-shot 学習とドメイン特化の相乗効果の定量化

### 2. **技術的成果**

- ドメイン別最適化戦略の確立
- ハルシネーション抑制手法の実証
- 実用的な説明生成システムの設計指針

### 3. **実用的価値**

- 商品レビュー分析システムへの応用
- ドメイン特化チャットボットの改良
- 説明可能 AI の信頼性向上

---

## 📝 実験実施計画

### フェーズ 1: データ準備・分析（1 日）

- Domain-Aware Feature Splitter によるデータ分割
- 分割結果の品質検証
- ベースライン統計の算出

### フェーズ 2: GPT 実験実施（2 日）

- Restaurant/Laptop ドメインでの対比因子生成実験
- 各設定での複数回実施による信頼性確保
- リアルタイム結果モニタリング

### フェーズ 3: 評価・分析（1 日）

- 自動評価システムによる全実験結果の評価
- 統計分析とドメイン間比較
- ハルシネーション抑制効果の定量化

### フェーズ 4: レポート作成（1 日）

- 実験結果の可視化
- 学術的意義の考察
- 実用化への提言まとめ

---

## 📊 実験成功の判定基準

### 最小成功基準

1. **実験完了率**: 95%以上の実験が正常完了
2. **精度向上**: 実特徴ベースがランダムベースより 20%以上高精度
3. **ハルシネーション抑制**: 30%以上のハルシネーション抑制効果

### 理想的成功基準

1. **ドメイン特化効果**: Laptop ドメインで 10%以上の精度向上
2. **Few-shot 効果**: 5-shot で 0-shot より 40%以上精度向上
3. **ハルシネーション抑制**: 50%以上の抑制効果達成

---

## 🔬 実験の新規性・独創性

### 従来研究との差別化ポイント

#### **1. ドメイン特化アプローチ**

- 単一データセットではなく、複数ドメインでの体系的比較
- ドメイン特有の特徴抽出パターンの解明

#### **2. 実用的データセット活用**

- 学術標準データセット（SemEval）での再現性確保
- 実世界での応用可能性を意識した設計

#### **3. 多面的評価システム**

- 精度・具体性・ハルシネーション・ドメイン適合性の 4 軸評価
- 従来の単一指標評価を超えた包括的分析

#### **4. ハルシネーション定量評価**

- 先行研究との直接比較による改善効果の明確化
- 説明可能 AI 実用化への具体的貢献

---

## 💡 予想される課題と対策

### 課題 1: データ不均衡

**対策**: 適応的サンプリングによるグループバランス調整

### 課題 2: ドメイン語彙の評価困難

**対策**: ドメイン専門辞書による自動評価システム構築

### 課題 3: 評価の主観性

**対策**: 複数 GPT インスタンスによる評価の一致度検証

### 課題 4: 計算コスト

**対策**: 並列実行とキャッシュ機能による効率化

---

この SemEval ABSA を用いたドメイン特化型 GPT 対比因子生成検証実験により、説明可能 AI の実用化に向けた重要な知見を得ることを目指します。

## 📚 参考値：過去の対比因子生成実験結果

### 🧪 実験 1: ベースライン特徴比較実験（2025 年 5 月 20 日）

| 実験項目          | 値                                 |
| ----------------- | ---------------------------------- |
| **データセット**  | Amazon Product Reviews             |
| **総特徴数**      | 20 個の手動定義特徴                |
| **Few-shot 設定** | 1-shot（例題付き）                 |
| **データサイズ**  | 各特徴 8-22 レビュー（平均 13-17） |
| **出力形式**      | 5-10 英単語                        |
| **評価指標**      | GPT 応答のみ                       |

**特徴例と生成結果：**

- Feature 2 vs 3: "Detailed product features and personal usage experiences described"
- Feature 3 vs 4: "Mentions of customer service issues and personal usage experiences"
- Feature 4 vs 5: "Detailed product usage and feature descriptions"

### 🧪 実験 2: 包括的 Few-shot 実験（2025 年 6 月 6 日）

| 実験項目          | 値                        |
| ----------------- | ------------------------- |
| **Few-shot 範囲** | 0-shot〜5-shot            |
| **総実験数**      | 30 回以上                 |
| **データサイズ**  | 各特徴 8-21 レビュー      |
| **評価手法**      | GPT 回答 + タイムスタンプ |
| **出力形式**      | 5-10 英単語               |

**0-shot 実験の代表例：**

- Feature 1: "Detailed performance comparisons and user experience"
- Feature 2: "Easy to use, great features, well supported"
- Feature 3: "Detailed performance analysis and user experience"
- Feature 5: 日本語回答（詳細なインストール説明や手順）

### 🧪 実験 3: 類似度評価実験（2025 年 6 月 2 日）

| 実験項目             | 値                        |
| -------------------- | ------------------------- |
| **評価対象**         | 190 ペアの特徴組み合わせ  |
| **評価指標**         | BERT 類似度 + BLEU 類似度 |
| **BERT 平均類似度**  | 0.619 (±0.065)            |
| **BLEU 平均類似度**  | 0.018 (±0.025)            |
| **最高 BERT 類似度** | 0.821                     |
| **最低 BERT 類似度** | 0.471                     |

### 📈 過去実験からの示唆

#### 1. 実験精度の傾向

- **BERT 類似度**: 平均 0.619 で中程度の類似性を維持
- **BLEU 類似度**: 平均 0.018 と低く、語彙的多様性が高い
- **特徴識別**: 明確な特徴ほど一貫した説明を生成

#### 2. Few-shot 学習の効果

- **0-shot**: より抽象的・一般的な説明傾向
- **1-shot**: 具体性が向上、例題の影響が明確
- **多言語混入**: 設定によって日本語回答が発生（制御要検討）

#### 3. データサイズの影響

- **小規模グループ**: 8-9 レビューでも特徴識別可能
- **中規模グループ**: 13-17 レビューで安定した結果
- **不均衡データ**: 21 vs 9 のような不均衡でも分析可能

#### 4. 出力品質の特徴

- **語彙制約**: 5-10 単語制限で簡潔性確保
- **一貫性**: 同一特徴での複数回実験で類似結果
- **解釈可能性**: 人間が理解しやすい説明生成

### 📋 今回実験への示唆

1. **データサイズ**: 50-200 レビューは過去実験の 3-10 倍の規模で、より安定した結果が期待
2. **ハルシネーション**: 過去のランダム分割では明確な基準がなかったが、今回は実際の特徴での検証
3. **評価指標**: BERT/BLEU 類似度に加え、より詳細な正確性・具体性評価を追加
4. **ドメイン特化**: 過去は汎用 Amazon レビューだったが、今回は ABSA 特化データでより精密な分析
