#!/usr/bin/env python3
"""
SemEvalå®Ÿé¨“ã§ã®LLMå…±é€šå‡¦ç†ä½¿ç”¨ä¾‹

æ—¢å­˜ã®semeval_absa_contrast_experiment.pyã®GPTå‡¦ç†éƒ¨åˆ†ã‚’
LLMå…±é€šå‡¦ç†ã«ç½®ãæ›ãˆã‚‹ä¾‹
"""

import sys
import os
from pathlib import Path
from typing import Dict, List, Optional
from dotenv import load_dotenv

# ãƒ‘ã‚¹è¨­å®š
current_dir = Path(__file__).parent
experiments_dir = current_dir.parent.parent.parent
utils_dir = experiments_dir / "utils"
sys.path.append(str(utils_dir))
sys.path.append(str(utils_dir / "LLM"))

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_factory import LLMFactory


class SemEvalLLMIntegration:
    """SemEvalå®Ÿé¨“ã®LLMçµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str = None):
        """
        åˆæœŸåŒ–
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
        """
        self.llm_client = LLMFactory.create_client(model_name)
        print(f"âœ… LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {self.llm_client.get_model_name()}")
    
    def create_contrast_prompt(self, group_a: List[Dict], group_b: List[Dict], 
                             domain: str, feature: str, shot_count: int = 0) -> str:
        """
        å¯¾æ¯”å› å­ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨ï¼‰
        """
        # ã‚°ãƒ«ãƒ¼ãƒ—Aã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€å¤§10ä»¶è¡¨ç¤ºï¼‰
        group_a_texts = [sample['review_text'] for sample in group_a[:10]]
        group_b_texts = [sample['review_text'] for sample in group_b[:10]]
        
        group_a_display = "\n".join([f"- {text[:200]}..." if len(text) > 200 else f"- {text}" 
                                   for text in group_a_texts])
        group_b_display = "\n".join([f"- {text[:200]}..." if len(text) > 200 else f"- {text}" 
                                   for text in group_b_texts])
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³æƒ…å ±
        domain_context = {
            'restaurant': 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒ»é£²é£Ÿåº—',
            'laptop': 'ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ãƒ»ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—'
        }.get(domain, domain)
        
        # Few-shotä¾‹é¡Œéƒ¨åˆ†ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ä»Šå›ã¯0-shotã®ã¿ï¼‰
        few_shot_examples = ""
        if shot_count > 0:
            few_shot_examples = f"\nã€å‚è€ƒä¾‹é¡Œã€‘\nï¼ˆ{shot_count}å€‹ã®ä¾‹é¡ŒãŒã“ã“ã«å…¥ã‚Šã¾ã™ï¼‰\n"
        
        prompt = f"""ã‚ãªãŸã¯{domain_context}ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚

ã€åˆ†æã‚¿ã‚¹ã‚¯ã€‘
ä»¥ä¸‹ã®2ã¤ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¯”è¼ƒã—ã¦ã€ã‚°ãƒ«ãƒ¼ãƒ—Aã«ç‰¹å¾´çš„ã§ã‚°ãƒ«ãƒ¼ãƒ—Bã«ã¯è¦‹ã‚‰ã‚Œãªã„è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„å†…å®¹ã®ç‰¹å¾´ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã€‘
- ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain_context}
- å¯¾è±¡ç‰¹å¾´: {feature}
- ã‚°ãƒ«ãƒ¼ãƒ—Aã‚µã‚¤ã‚º: {len(group_a)}ãƒ¬ãƒ“ãƒ¥ãƒ¼
- ã‚°ãƒ«ãƒ¼ãƒ—Bã‚µã‚¤ã‚º: {len(group_b)}ãƒ¬ãƒ“ãƒ¥ãƒ¼

{few_shot_examples}

ã€ã‚°ãƒ«ãƒ¼ãƒ—A ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘ï¼ˆ{feature}ç‰¹å¾´ã‚’å«ã‚€ï¼‰
{group_a_display}

ã€ã‚°ãƒ«ãƒ¼ãƒ—B ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘ï¼ˆ{feature}ç‰¹å¾´ã‚’å«ã¾ãªã„ï¼‰
{group_b_display}

ã€å›ç­”è¦æ±‚ã€‘
è‹±èªã§5-10å˜èªç¨‹åº¦ã§ã€ã‚°ãƒ«ãƒ¼ãƒ—Aã«ç‰¹å¾´çš„ã§ã‚°ãƒ«ãƒ¼ãƒ—Bã«ã¯è¦‹ã‚‰ã‚Œãªã„ä¸»è¦ãªé•ã„ã‚’ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

å›ç­”ï¼š"""

        return prompt
    
    def query_gpt_with_llm_client(self, prompt: str, **kwargs) -> Optional[str]:
        """
        LLMå…±é€šå‡¦ç†ã‚’ä½¿ã£ã¦GPTã«ã‚¯ã‚¨ãƒªï¼ˆæ—¢å­˜ã®query_gpt()ã®ç½®ãæ›ãˆï¼‰
        """
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»˜ãã§ã‚¯ã‚¨ãƒª
            response = self.llm_client.ask(
                question=prompt,
                system_message="ã‚ãªãŸã¯å„ªç§€ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå°‚é–€å®¶ã§ã™ã€‚",
                temperature=kwargs.get('temperature', 0.3),
                max_tokens=kwargs.get('max_tokens', 100)
            )
            return response
        except Exception as e:
            print(f"âŒ LLM API ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_single_experiment(self, domain: str, feature: str, shot_count: int = 0) -> Dict:
        """
        å˜ä¸€å®Ÿé¨“ã®å®Ÿè¡Œä¾‹
        """
        print(f"ğŸ”¬ å®Ÿé¨“å®Ÿè¡Œ: {domain}-{feature}-{shot_count}shot")
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã«ã¯DomainAwareFeatureSplitterã‹ã‚‰å–å¾—ï¼‰
        if domain == "laptop" and feature == "battery":
            group_a = [
                {"review_text": "Great battery life lasting all day"},
                {"review_text": "Excellent power management features"},
                {"review_text": "Long-lasting battery performance"}
            ]
            group_b = [
                {"review_text": "Poor screen quality and resolution"},
                {"review_text": "Uncomfortable keyboard layout"},
                {"review_text": "Slow system performance issues"}
            ]
        else:
            # ãã®ä»–ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            group_a = [{"review_text": f"Good {feature} quality"}]
            group_b = [{"review_text": "Poor overall experience"}]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = self.create_contrast_prompt(group_a, group_b, domain, feature, shot_count)
        
        # GPTå•ã„åˆã‚ã›ï¼ˆLLMå…±é€šå‡¦ç†ä½¿ç”¨ï¼‰
        gpt_response = self.query_gpt_with_llm_client(prompt)
        
        if gpt_response:
            print(f"âœ… GPTå¿œç­”: {gpt_response}")
            
            # çµæœè¨˜éŒ²
            result = {
                "domain": domain,
                "feature": feature,
                "shot_count": shot_count,
                "group_a_size": len(group_a),
                "group_b_size": len(group_b),
                "gpt_response": gpt_response,
                "prompt_length": len(prompt),
                "model": self.llm_client.get_model_name()
            }
            
            return result
        else:
            print(f"âŒ GPTå¿œç­”ã®å–å¾—ã«å¤±æ•—")
            return None


def demo_integration():
    """çµ±åˆãƒ‡ãƒ¢"""
    print("ğŸš€ SemEvalå®Ÿé¨“ LLMçµ±åˆãƒ‡ãƒ¢")
    print("=" * 50)
    
    # LLMçµ±åˆã‚¯ãƒ©ã‚¹åˆæœŸåŒ–
    semeval_llm = SemEvalLLMIntegration()
    
    # å®Ÿé¨“å®Ÿè¡Œä¾‹
    experiments = [
        ("laptop", "battery", 0),
        ("laptop", "screen", 0),
        ("restaurant", "food", 0),
    ]
    
    results = []
    for domain, feature, shot_count in experiments:
        result = semeval_llm.run_single_experiment(domain, feature, shot_count)
        if result:
            results.append(result)
        print()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("ğŸ“Š å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 50)
    for i, result in enumerate(results, 1):
        print(f"å®Ÿé¨“{i}: {result['domain']}-{result['feature']}")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {result['model']}")
        print(f"  å¿œç­”: {result['gpt_response']}")
        print(f"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {result['prompt_length']}æ–‡å­—")
    
    print(f"\nğŸ‰ {len(results)}ä»¶ã®å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    return results


def migration_guide():
    """æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç§»è¡Œã‚¬ã‚¤ãƒ‰"""
    print("\nğŸ“‹ æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç§»è¡Œã‚¬ã‚¤ãƒ‰")
    print("=" * 50)
    
    print("ã€BEFOREã€‘æ—¢å­˜ã®semeval_absa_contrast_experiment.py")
    print("""
# æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰
def query_gpt(self, prompt: str) -> Optional[str]:
    for attempt in range(MAX_RETRIES):
        try:
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"GPT API ã‚¨ãƒ©ãƒ¼: {e}")
            return None
""")
    
    print("\nã€AFTERã€‘LLMå…±é€šå‡¦ç†ä½¿ç”¨ç‰ˆ")
    print("""
# æ–°ã—ã„ã‚³ãƒ¼ãƒ‰
def __init__(self):
    self.llm_client = LLMFactory.create_client()  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—

def query_gpt(self, prompt: str) -> Optional[str]:
    return self.llm_client.ask(
        question=prompt,
        system_message="ã‚ãªãŸã¯å„ªç§€ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå°‚é–€å®¶ã§ã™ã€‚",
        temperature=0.3,
        max_tokens=100
    )
""")
    
    print("\nâœ… ç§»è¡Œã®ãƒ¡ãƒªãƒƒãƒˆ:")
    print("  - OpenAI APIã®ç›´æ¥å‘¼ã³å‡ºã—ä¸è¦")
    print("  - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®è‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—")
    print("  - çµ±ä¸€ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°")
    print("  - å°†æ¥çš„ãªä»–ã®LLMï¼ˆClaudeã€Geminiç­‰ï¼‰ã¸ã®å¯¾å¿œå®¹æ˜“")
    print("  - ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã®å…±é€šåŒ–")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # çµ±åˆãƒ‡ãƒ¢å®Ÿè¡Œ
        results = demo_integration()
        
        # ç§»è¡Œã‚¬ã‚¤ãƒ‰è¡¨ç¤º
        migration_guide()
        
        return results
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()