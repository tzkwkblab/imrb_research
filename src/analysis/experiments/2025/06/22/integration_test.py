#!/usr/bin/env python3
"""
ä¸€é€£ã®æµã‚Œã®çµ±åˆãƒ†ã‚¹ãƒˆ

1. å¯¾æ¯”å› å­æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ (prompt_contrast_factor.py)
2. GPTã«å•ã„åˆã‚ã› (LLM/example_usage.py)
3. BERTã¨BLEUã‚¹ã‚³ã‚¢è¨ˆç®— (get_score.py)
"""

import sys
import os
from pathlib import Path
from dotenv import load_dotenv

# ãƒ‘ã‚¹è¨­å®š
current_dir = Path(__file__).parent
# 2025/06/22 -> 2025 -> experiments -> utils
experiments_dir = current_dir.parent.parent.parent
utils_dir = experiments_dir / "utils"
sys.path.append(str(utils_dir))
sys.path.append(str(utils_dir / "LLM"))



# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from prompt_contrast_factor import generate_contrast_factor_prompt
from llm_factory import LLMFactory
from get_score import calculate_scores


def run_integration_test():
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ ä¸€é€£ã®æµã‚Œçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™")
    group_a = [
        "Great battery life lasting all day",
        "Excellent power management features", 
        "Long-lasting battery performance"
    ]
    
    group_b = [
        "Poor screen quality and resolution",
        "Uncomfortable keyboard layout",
        "Slow system performance issues"
    ]
    
    # æ­£è§£ä¾‹ï¼ˆè©•ä¾¡ç”¨ï¼‰
    expected_answer = "Battery life and power management"
    
    print(f"ã‚°ãƒ«ãƒ¼ãƒ—Aï¼ˆ{len(group_a)}ä»¶ï¼‰:")
    for text in group_a:
        print(f"  - {text}")
    
    print(f"\nã‚°ãƒ«ãƒ¼ãƒ—Bï¼ˆ{len(group_b)}ä»¶ï¼‰:")
    for text in group_b:
        print(f"  - {text}")
    
    print(f"\næœŸå¾…ã™ã‚‹å›ç­”: {expected_answer}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    print("\nğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: å¯¾æ¯”å› å­æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ")
    prompt, model_config = generate_contrast_factor_prompt(
        group_a=group_a,
        group_b=group_b,
        output_language="è‹±èª"
    )
    
    print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š: {model_config}")
    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—")
    print(f"\nç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
    print("-" * 40)
    print(prompt)
    print("-" * 40)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: GPTå•ã„åˆã‚ã›
    print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—3: GPTå•ã„åˆã‚ã›")
    llm_client = LLMFactory.create_client()
    print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {llm_client.get_model_name()}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰messagesã‚’æ§‹ç¯‰
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    gpt_response = llm_client.query(
        messages=messages,
        temperature=model_config.get('temperature', 0.7),
        max_tokens=model_config.get('max_tokens', 100)
    )
    
    print(f"GPTå¿œç­”: {gpt_response}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¹ã‚³ã‚¢è¨ˆç®—
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: BERTã¨BLEUã‚¹ã‚³ã‚¢è¨ˆç®—")
    bert_score, bleu_score = calculate_scores(expected_answer, gpt_response)
    
    print(f"æœŸå¾…ã™ã‚‹å›ç­”: {expected_answer}")
    print(f"GPTå®Ÿéš›å›ç­”: {gpt_response}")
    print(f"BERTã‚¹ã‚³ã‚¢: {bert_score:.4f}")
    print(f"BLEUã‚¹ã‚³ã‚¢: {bleu_score:.4f}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: çµæœã‚µãƒãƒªãƒ¼
    print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—5: çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ: æˆåŠŸ ({len(prompt)}æ–‡å­—)")
    print(f"âœ… GPTå•ã„åˆã‚ã›: æˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {llm_client.get_model_name()})")
    print(f"âœ… ã‚¹ã‚³ã‚¢è¨ˆç®—: æˆåŠŸ (BERT: {bert_score:.4f}, BLEU: {bleu_score:.4f})")
    
    # çµæœè¾æ›¸
    result = {
        "test_data": {
            "group_a": group_a,
            "group_b": group_b,
            "expected_answer": expected_answer
        },
        "prompt": {
            "content": prompt,
            "length": len(prompt),
            "model_config": model_config
        },
        "gpt_response": gpt_response,
        "evaluation": {
            "bert_score": bert_score,
            "bleu_score": bleu_score
        },
        "model_info": {
            "model_name": llm_client.get_model_name(),
            "temperature": model_config.get('temperature'),
            "max_tokens": model_config.get('max_tokens')
        }
    }
    
    print(f"\nğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
    return result


def test_few_shot():
    """Few-shotãƒ†ã‚¹ãƒˆ"""
    print("\n\nğŸ¯ Few-shotãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # Few-shotä¾‹é¡Œ
    examples = [
        {
            "group_a": ["Fast delivery", "Quick shipping"],
            "group_b": ["Slow response", "Delayed support"],
            "answer": "Delivery and shipping speed"
        }
    ]
    
    group_a = ["High-quality materials", "Durable construction"]
    group_b = ["Cheap plastic", "Fragile design"]
    expected_answer = "Material quality and durability"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆFew-shotä»˜ãï¼‰
    prompt, model_config = generate_contrast_factor_prompt(
        group_a=group_a,
        group_b=group_b,
        output_language="è‹±èª",
        examples=examples
    )
    
    # GPTå•ã„åˆã‚ã›
    llm_client = LLMFactory.create_client()
    messages = [{"role": "user", "content": prompt}]
    gpt_response = llm_client.query(messages=messages, temperature=0.3, max_tokens=50)
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    bert_score, bleu_score = calculate_scores(expected_answer, gpt_response)
    
    print(f"Few-shotä¾‹é¡Œæ•°: {len(examples)}")
    print(f"æœŸå¾…ã™ã‚‹å›ç­”: {expected_answer}")
    print(f"GPTå¿œç­”: {gpt_response}")
    print(f"BERTã‚¹ã‚³ã‚¢: {bert_score:.4f}")
    print(f"BLEUã‚¹ã‚³ã‚¢: {bleu_score:.4f}")
    
    return {
        "few_shot_examples": len(examples),
        "expected_answer": expected_answer,
        "gpt_response": gpt_response,
        "bert_score": bert_score,
        "bleu_score": bleu_score
    }


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # 0-shotãƒ†ã‚¹ãƒˆ
        zero_shot_result = run_integration_test()
        
        # Few-shotãƒ†ã‚¹ãƒˆ
        few_shot_result = test_few_shot()
        
        # æ¯”è¼ƒçµæœ
        print(f"\nğŸ“Š 0-shot vs Few-shot æ¯”è¼ƒ")
        print("=" * 60)
        print(f"0-shot  - BERT: {zero_shot_result['evaluation']['bert_score']:.4f}, BLEU: {zero_shot_result['evaluation']['bleu_score']:.4f}")
        print(f"Few-shot- BERT: {few_shot_result['bert_score']:.4f}, BLEU: {few_shot_result['bleu_score']:.4f}")
        
        return {
            "zero_shot": zero_shot_result,
            "few_shot": few_shot_result
        }
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()